---
title: "A Glossary for Distributional Regression Models"
format:
  html: default 
  pdf: default 
number-sections: false   
number-depth: 3
editor: visual
quarto: add leovan/quarto-pseudocode
author: GAMLSS working party  
bibliography: book2026.bib       
---

## Introduction

This is a glossary of terms and ideas related to statistical models in general but more specifically to [distributional regression] models. The current glossary started based on ideas presented in the talk [Regression Models; how to adapt for climate change challenges](https://mstasinopoulos.github.io/Brazil2025-talk/talk_2.html#/title-slide) given by Mikis Stasinopoulos, University of Greenwich, to the XVII Encontro Mineiro de Statistica on Octomber 2025, in Lavras, Brazil. It was soon realised that there was also a large gap between the [statistical modelling] terminology and the terminology used by other data analysis scientists. For example, the [response] variable a well known expression to describe the variable of interest in a [regression analysis] but it is called the [target] in [machine learning]. Similarly the [explanatory variables] are refer to as [feature]s. In this glossary we are trying to deal with those problrm by describing both definitions. We also decided to include at several places of the glossary, `R` output if we thought it would facilitate the understanding of the ideas and concepts presented here.

The following package are needed for explaining some of the concepts below;

```{r}
#| warning: false
library(gamlss)
library(gamlss2)
```

We shall use the `rent99` as a main source of a [data] set we show it here;

```{r}
#| warning: false
data(rent99)
```

Note that two of the variables in `rent99` are not needed so we they are taking them out.

```{r}
da <- rent99[,-c(2,9) ]
head(da)
```

# Glossary

################################################################################ 

## A

#### accumulated local effects

Accumulated Local Effects, ALE, @apley2020visualizing, is a model \[agnostic\] technique for interpretatng [term]s in a [model]. It is showing how explanatory variable affects the predictions value of the model. It works even with correlated explanatory variables, by analysing differences in predictions across local intervals rather than averaging whole predictions. ALE avoids issues with [multicollinearity] or collinearity by focusing on the change in predictions within local windows, therefore providing more accurate and unbiased global explanations for variable importance.

::: callout-note
example here
:::

#### acf

The term [acf] stands for [autocorrelation] function. The [acf] is a [diagnostics] tool for detecting [autocorrelation] in a [vector] $\textbf{x}$. In a [regression analysis] the [vector] is often the [residuals] of the [model]. It is used together with [pacf], the parcial [autocorrelation] function, to identify autocorrelation in [time series] data.

```{r}
# #| warning: false
# library(gamlss.ggplots)
# y_acf(resid(m1))
```

#### additive smoothers

Additive smoothers occurs when the contribution of one or more continuous [term]s [smoother] is added to the rest of the terms in the equation of a model For example the [formula] $s_1(x_1)+s_2(x_2)+ s_3(x_1, x_2)$, where the $s()$s are different [smooth function]s will [fit] two main effects [smoother]s for $x_1$ and $x_2$ and a first order interaction [smoother] for for $x_1$ and $x_2$.

```{r}
#| label: fig-m0
#| fig-cap: "The fitted smoothing terms for model `m0`."
library(gamlss)
#| warning: false 
library(gamlss2)
m0 <- gamlss2(rent~s(area)+s(yearc)+s(area, yearc), 
              data=rent99, famlily=GA) 
plot(m0)
```

Smooth first order interaction models have relatively easier [interpretation].

#### additive model

An additive model occurs when the contribution of each [term]sin a [model] is added to the rest of the terms in the [formula]. Let $x_1$ abd $x_2$ be two continuous variables and $f_1$ and $f_2$ to be two [factor]s. The [formula] $y \sim b_1 x_1 + s_2(x_2) + f_1*f_2 + s_{12}(x_1, x_2, \texttt{by}=f_2)$, will fit a linear [term] for $x_1$ a [smoother] for $x_2$, a linear first order interaction for the [factor]s $f_1$ and $f_2$ and a varied coefficient [smoother] for the first order interaction for for $x_1$ a and $x_2$ varying according to factor $f_2$.

```{r}
#| warning: false 
library(gamlss)
m1 <- gamlss2(rent~area+s(yearc) + location*cheating +
            s(area, yearc, by=cheating), data=rent99, famlily=GA) 
summary(m1)
```

The fitted smooth curves can be plotted by

```{r}
#| label: fig-m1
#| fig-cap: "The fitted smoothing terms for model `m1`."
plot(m1)
```

Note that an [additive model] is usually easy to [interpretation] but more difficult to reach because one has to decide which [term]s should be included in the [model]. The [additive model]s need more thinking on how to reach them while a [machine learning] [model] let the computer to do the hard work. Which we should trust best depends on the [purpose] of the study. My personal view in this that we should \textbf{not} let the computers decide when the life of people is stake see also [black box].

#### adequate fit

We call a [model] an [adequate fit] if the [final model] in a [model selection] procedure is an [adequate model]. The question is how do we know that we reach an [adequate fit]?. By [adequate model] we mean a [model] close to the [data] and be able to answer the [questions]. How close to the data a model is can be checked by the performance of the [goodness of fit] measure and by [diagnostics] tools. However the [goodness of fit] measure is often a relative measures in the sense that is useful for comparing between models but not to show if any of then is [adequate fit]. This task mainly lies with [diagnostics] tools. More specificity \[diagnostic\] tools based on [residuals]. The reason is because in most \[stochastic models\], if the the [assumptions] are correct or nearly correct, we expect the [residuals] to behave as a [white noise]. Diagnostic plots demonstrating [white noise] [residuals] are a good sign that an [adequate fit] has been achieved. In conclusion a [adequate fit] can be verified using [residuals] [diagnostics] on the [training] data set in compilation with a good performance in a [goodness of fit] measure.

#### adequate model

We call a [model] an [adequate model] if the [model] represents the [data] well. That is, a model is [adequate model] if it has passed its [diagnostics] checks. A set of many [adequate model]'s is oftem called the [Rashomon set]. Within a [distributional regression] framework we have two types of [adequate model]s to consider. The first has to do of whether the [distribution] of the [response] is fitted adequately the second on whether the chosen [term]s are adequate represent the relationships in the [distribution parameters].

#### agnostic method

An [agnostic method] in [statistical modelling] is a technique which could apply to any [model] independently if they are [mathematical model]s, [agent based model]s or [algorithmic model]s. Note, however, that a lot of the techniques claimed to be agnostic may depend on the [assumptions] of the model or on the type of [measure of goodness of fit] used to [fit] the model. The [assumptions] and the [measure of goodness of fit] are interconnected with the [model]. Strictly one should defined an [agnostic method] a techniques which is not associated with any of them.

#### agent

An [agent] is the basic unit of interest in a [agent based model].

#### agent based model

An agent based model is a simulation, bottom up model, where the unit of interest is called an [agent]. The model is build by simulating a lot of times how the agents interact between them within a given environment. The [agent] is the basic unit of interest and its behaviour is studied using simple mathematical or logic rules. The behaviour of the agent is studied after a long simulation exercise. Therefore important part of the model is how to set the parameters determine the [agent] behaviour.

#### AIC

[AIC] stands for the Akaike Information Criterion, @Akaike73b. [AIC] it is a measure of [goodness of fit] used to compare different fitted models. The [AIC] is defined as [deviance] $+2\times df$ where $df$ are the [degrees of freedom] and 2 stands for the **penalty**, see also [GAIC] and [BIC]. The [model] with the minimum [AIC] is the best.

```{r}
AIC(m0,m1)
```

#### ALE

[ALE] is an abbreviation for [accumulated local effects]

#### algorithm

An algorithm is a step-by-step computational procedure designed to perform a specified task. For example in a [input-output model]s when typically we have $Y=g(X)+\epsilon$, the task is mainly to find the unknown function $g()$, which connect the input with the output.

#### algorithmic model

An algorithmic model is a model based on an algorithm. In [regression] typically an algorithmic model trying to model $X \rightarrow Y$, through an unknown function $g()$ i.e. $Y=g(X)$. No explicit assumptions are made for the unknown function $g()$ but a lot of implicit [assumptions] depending on the type of algorithm used. Note that an [algorithmic model], like a [mathematical model], can be a deterministic or an [stochastic model]. The [stochastic model] can be written as $g()$ i.e. $Y=g(X)+ \boldsymbol{\epsilon}$ where the last term in the equation, the [error] is a [random] variable and the stochastic part of the [model].

#### Archimedean copulas

The [Archimedean copulas] are a wide class of families of copula with subclass;

-   the [Clayton copula] which cal model strong lower tail dependence, that is, useful when joint small values matter.

-   the [Gumbel copula] supporting strong upper tails and its useful for modelling extreme values.

-   the [Frank copula] with symmetric dependence and not fat tail dependence.

-   the \[Joe copula\] with strong upper tail dependence and more flexible than [Gumbel copula]

-   the [extreme value copulas] suitable for modelling jointly extreme values. (Gumbel–Hougaard, Galambos, Hüsler–Reiss) Popular in hydrology, climate extremes and Risk analysis.

-   the \[vine copulas\] (or pair copula constructions) whicjh are highly flexible, built from bivariate copulas with types: i) C-vines copula ii) D-vines copula and iii) R-vines copula. All have the advantages that handle high dimensions couls mix different copula families and have very flexible dependence structures. They are widely in Finance, Insurance and Machine learning.

-   the [hierarchical Archimedean copulas] (HAC) which allow nested Archimedean copulas, clustering of dependence and in genaral are more flexible than single [Archimedean copulas]

-   the \[empirical nonparametric copulas\] Empirical copula and Bernstein copula with advantages of fewer assumptions and [data]-driven.

-   the \[special purpose copulas\] like the \[Plackett copula\] the \[FGM copula\] (Farlie–Gumbel–Morgenstern) whicj allow limited dependence and the Marshall–Olkin copula with asymmetric dependence.

::: callout-note
More work needed here
:::

#### assumptions

Assumptions are axiomatic statements needed to be accepted for the model to work. Models need assumptions because of their simplified nature. The reasoning behind assumption is that if the assumptions are correct then the model can be useful. There are **explicit** and **implicit** assumptions. The **explicit** assumptions are usually mathematical ones and could be easily checked using [diagnostics] tools. The **implicit** assumptions common in [algorithmic model]s are more difficult to check. **Incorrect** assumptions could lead to questionable scientific discoveries. For an [agent based model] the assumptions are the different ways the agent are behaving.

::: callout-note
maybe assumptions of the linear model here
:::

#### autocorrelation

The term [autocorrelation] in a vector $\textbf{x}$ implies that sequential values of $\textbf{x}$ depend on previous values. Autocorrelation is very common on [time series] data and can be detected using [acf] and [pacf] functions.

In [distributional regression] one should check whether the [z-scores] are autocorrelated. This can be done using the `resid_plots(., theme="ts")`:

```{r}
#| warning: false 
library(gamlss.ggplots)
resid_plots(m1, theme="ts")
```

#### averaging models

In [averaging models] the idea is is to select a [final model] from a [stack of models] by averaging the [model]s. There are a lot of ways to average [model]s see also [model average] and [stacking].

::: callout-note
example?
:::

################################################################################ 

## B

#### backfitting

The [backfitting] algorithm is an algorithm discussed by @HastieTibshirani90 for fitting more that one smoother in a [formula] for a [model] see also Chapter 3 of @Stasinopoulosetal2017.

#### bagging

Bagging refers to [bootstrapping] followed by an [averaging models] procedure. That is. when $B$ models are fitted to $B$ bootstrap [data partition]s and the resulting models are then averaged.

#### Bernoulli distribution

The [Bernoulli distribution] is special case of the [binomial distribution] when $n=1$. A Bernoulli [random] variables has range of values $0$ or $1$, expected value $\mathbb{E}(Y)=p$ and variance $p(1-p)$. @fig-bernoulli show three realization of the [Bernoulli distribution] at different values of $p=(0.1,0.3,0.5)$ SOMETHING IS WRoNG WITH IT the p should be in the first line not the second

```{r}
#| fig-width: 4
#| fig-height: 4
#| label: fig-bernoulli
#| warning: false
#| fig-cap: "The Bermoulli distribution with differenrt values of p=(0.1,0.3,0.5) and n=1"
library(gamlss.ggplots)
family_pdf(BI, mu=c(0.1,0.3,0.5), to=1, title="Bermoulli p=c(0.1,0.3,0.5) an n=1")
```

::: callout-note
Is there a bug in the function? The probabilities should be the other way around.
:::

#### BIC

BIC is the Bayesian information criterion of @Schwarz78. Within a [distributional regression] model the BIC can be used to compare different fitted models. The BIC is also refer to also as [SBC], Schwarz Bayesian Criterion. The BIC is defined as [deviance] $+ \log(n) \times DF$, when $n$ is the number of observations in the [data], and $DF$ are the [degrees of freedom]. See also [GAIC].

#### binomial distribution

The pdf of a binomial distributed [random] variable is: $$f(Y|p)= \binom{n}{p} p^{p} (1-p)^{n-p}$$ where $Y$ takes values in the [range] $0,1,2,\ldots,n$, $n$ is called here the **binomial denominator** and the parameter $p$ is a probability parameter taking values $0<p<1$. Note that $\binom{n}{p}$ can be written also as;\
$$ \binom{n}{p} =\frac{\Gamma(n+1)}{\Gamma(y+1) \Gamma{(n-y+1)}}$$ For more details see pp. 521-522 of @Rigbyetal2019. The expected value of the [distribution] is $\mathbb{E}(Y)=np$ and the variance is $\mathbb{V}ar(Y)=np(1-p)$. @fig-binomial show the three different binomial distributions with binomial denominator equal to 10 and different probabilities at

```{r}
#| label: fig-binomial
#| fig-cap: "The binomial distribution with different values of p=(0.1,0.3,0.5) and n=10"
#| warning: false
library(gamlss.ggplots)
family_pdf(BI, mu=c(0.1,0.3,0.5), title="binomial p=c(0.1,0.3,0.5) and n=10")
```

Note that in the `gamlss.ggplots` documentation the parameter of the [binomial distribution] called here as $p$ is denote as `mu`. The binomial denominator in the function `family_pdf` can be specified in by the argument `to`. In @fig-binimial it take the default value which is 10.

#### black box

A black box is a [model] with difficult interpretation. A lot of machine leaning models are black boxes. There are two reason for black box model i) the function $g()$ which the black box tries to estimate is too complex to explain. ii) there are proprietary reasons. The [questions] of the problem should determine whether a [black box] [model] is appropriate. @rudin2019stop argued "stop explaining black box machine learning models for **high stakes decisions** and use [interpretable model]s instead". The argument came from the fact that among all [adequate fit]ted models in the [Rashomon set] few are interpretable [@rudin2024amazing].

#### bootstrapping

The method of [bootstrapping] is a way to fit multiple models to a single [data] set by repeatedly re-sampling with replacement from the original [data] set. The multiple [fit]s can be used to obtain variability of the [parameters] in the model. In this sense bootstrapping complement Bayesian fits where the information about the variability comes from prior assumptions rather than the bootstrap replications. Bootstrapping is based on [data partition] with replacement for $B$ times so the different fits could produce $B$ [estimates] of [fitted values] and [residuals]. Averaging those values sometimes refer to as [bagging]. A good book about [bootstrapping] is @EfronTibshirani93.

#### boosting

The method od [boosting] is a way of [model fitting] using many **complementary** sequential simple models and average them in order to build a unique [fit]ted [final model]. The individual models should be simple and easy to fit and should complement each other by modelling different characteristic of the [data]. Finally the individual \[fitted\] simple [model]s contribute when averaging only to a small part say 10 or 20 percent of their original contribution. This percentage is one of the [hyper parameter]s of the boosting method. The other [hyper parameter] is how many time we should repeat the fitting procedure in order to reach an [adequate fit]ted model. The latest is the main smoothing parameters of the [boosting] procedure. For determine the smoothing parameters [cross validation] can be used which requiring a good [measure of goodness of fit].

#### bottom up model

A [bottom up model] is a [model] starting at the micro level behaviour of the [subjects] and how they interact with each other in order to check the overall global behaviour on the [variables] of interest. A typical example of this type of model is the [agent based model] where the individual behaviour of the [agent]s affect the overall behaviour variables. A agent based model is a [simulation model] where the behaviour of variables is examine after the simulation of the model several times.

#### bucket plot

A [bucket plot], @de2022bucket, is a [diagnostics] toot which can detect [skewness] and [kurtosis] in any [vector] in the [data] but more important in the [residuals] ([z-scores]) of a [distributional regression] [model]. Detection of [skewness] and [kurtosis] in the residuals of a [distributional regression] [model] amounts to detection of [skewness] and [kurtosis] in the [response] variable. There two types of [bucket plot]s: i) the first is based on [moment] [summary statistic]s; the second on [centile] based [summary statistic]s. The functions `moment_bucket()` and `centile_bucket()` of the package `gamlss.ggplots` can be used respectively to produce a [bucket plot], in `R`.

@fig-mom_bucket shows a [moment] [bucket plot] of the model `m0` fitted earlier in the section [additive smoothers] where two [smooth function]s [main effect]s `s(area)+s(yearc)` and one [first order interaction], `s(area, yearc)`, were fitted to the [response] Munich `rent` taken from the [data.frame] `rent99` using a Gamma (`GA`) distribution.

The moment based [bucket plot] in @fig-mom_bucket plots the transformed [moment] [skewness] of the residuals of the model `mo` against the transformed [moment] excess [kurtosis].

```{r}
#| label: fig-mom_bucket
#| fig-cap: "The moment bucket plot of the residuals of model `m0` "
#| warning: false
library(gamlss.ggplots)
moment_bucket(m0)
```

```{r}
#| label: fig-cen_bucket
#| fig-cap: "The centile bucket plot of the residuals of model `m1` "
#| warning: false
centile_bucket(m1)
```

Here we up data the [distribution] of the response from the two parameter `GA` to the four parameter `BCTo`.

```{r}
#| label: fig-model_mom_bucket
#| fig-cap: "The moment bucket plot of the reiduals from models `m0`, `m1` and `m2`.  "
#| warning: false
m2 <- update(m1, family=BCTo)
model_mom_bucket(m0, m1, m2)
```

::: callout-note
This needs more work
:::

################################################################################ 

## C

#### candidate model

The [candidate model] is next in line model for [fit] in the range of all possible models to be selected in a [step wise] procedure and it is compared to the [current model].

#### centile

A [centile] value is defined as $$100 \times \text{quantile}.$$ Since a [quantile] is taking values from 0 to 1 a [centile] is defined in the interval 0 to 100 for example the [quantile] at $0.50$ or 50% [centile] is the [median]. In practice the term [centile] is used as synonymous to [quantile]. A [centile]/[quantile] is a characteristic of both of the [distribution] of a [random] variable and of a [vector] in the [data]/[sample]. Centiles in the leter case can be defined through the [empirical cdf].

#### centile estimation

Centile estimationis another term for [reference curves] fitting.

#### censored response

A [censored response] is a [response] variable which could takes [interval values],

#### censored values

A [random] variable which we do not know its exact value but that only that it have been fallen within a known interval it said to have [censored values] or [interval values].

#### cdf

A cdf is the **commutative distribution function** of a random variable $Y$, usually denoted as $F(Y)$. The first derivative of the cdf define the probability distribution function, pdf of $Y$ i.e. $f(Y)=F'(Y)$ For a [distributional regression] where the [response] is assumed to have a proper [distribution] [pdf] he [cdf] is usually refer to as the \[commutative distribution\] function of the response.

#### Clayton copula

The [Clayton copula] for two random variable $u_1$ and $u_2$ both taking values in the [range] $[0,1]$ is defined as; $$C_\theta(u_1,u_2)
=
\left(u_1^{-\theta} + u_2^{-\theta} - 1\right)^{-1/\theta}$$ where $\theta$ is the parameter of the copula. The [Clayton copula] belongs to to the [Archimedean copulas] family. It models only positive dependence and only lower tail and its asymmetric with stronger lower tail. The interpretation of the parameter $\theta$ is when $\theta = 0$ implies independence, larger $\theta$ stronger dependence. It emphasises joint small values and it is related with Kendall’s $\tau$: $\tau = \frac{\theta}{\theta + 2}
\quad \Rightarrow \quad
\theta = \frac{2\tau}{1-\tau}.$

#### coefficients

The [coefficients] are [parameters] in a [model]. In [distributional regression] we refer to as [coefficients] when they are the [parameters] describing the relationship between the [distribution parameters] and the [explanatory variables] to distinguish them from the [distribution parameters].

#### continuous distribution

A [continuous distribution] is a [distribution] function, [pdf], of a [random] variable $Y$ taking values in the real line. The only property a [continuous distribution] has to have is that if is integrated with respect to the random variable it should add up tp one i.e. $\int_{-\infty}^{\infty} f(Y)dy=1$. There are three types of continuous distributions depending on the [range] of the continuous random variable $Y$;

i)  $(-\infty , \infty)$, continuous in the real line;
ii) $(0, \infty)$, continuous in the positive real line;
iii) $(0 , 1)$ continuous on a finite interval in the real line;

Next we show plots the different types of continuous distributions. We start with the normal [distribution] in which has two parameters $\mu$ the mean and $\sigma$ the standard deviation and which is defined on $(-\infty , \infty)$

```{r}
#| label: fig-normal
#| fig-cap: "The normal distribution with differenrt values of sigma=(1,1.5,2) and mu=0"
#| warning: false 
library(gamlss.ggplots)
family_pdf("NO", mu=c(0, 0, 0), sigma =c(1,1.5, 2), from=-5, to=5,title="Normal with mu=(0,0,0) and sigma=(1,1.5,2)")
```

Similarly the gamma [distribution] in @fig-beta, has two parameters $\mu$ the mean and $\sigma$a scale parameter but its defined on the positive real line $(0, \infty)$:

```{r}
#| label: fig-gamma
#| fig-cap: "The gamma distribution with differenrt values of mu ar 1 and sigma=(1 ,0.8, 1.5)."
#| warning: false 
library(gamlss.ggplots)
family_pdf("GA", mu=c(1, 1, 1), sigma =c(1,0.8, 1.5), from=0.01, to=5,
        title="Gamma with mu=(1,1,1) and sigma=(1,0.8, 1.5)")   
```

The beta [distribution] in @fig-beta, is defined on the real line $(0, \infty)$ and two parameters $\mu$ the mean and $\sigma$ a scale parameter:

```{r}
#| label: fig-beta
#| fig-cap: "The beta distribution with differenrt values of mu=(0.2, 0.5, 0.8) and sigma=(0.2 ,0.8, 0.3)."
#| warning: false 
library(gamlss.ggplots)
family_pdf("BE", mu=c(.2, .5, .8), sigma =c(0.2 ,0.8, 0.3), from=0.01, to=0.99,
           title="Beta with mu=(.2, .5, .8) and sigma=(0.2,0.8, 0.3)") 
```

As rule the more parameters exist in a [distribution] the more flexible the [distribution] is.

Note that any random variable in the interval $(\alpha, \beta)$ can be transformed to a [distribution] in the interval $(0 , 1)$. Also note that any [distribution] with [range] $(-\infty , \infty)$ can be **log** or **logit** transformed to a [distribution] in the positive real line $(0, \infty)$ or to the fine interval $(0 , 1)$, respectively.

#### characteristics of the distribution

Each [distribution] has its own properties. We called those properties the [characteristics of the distribution]. Within a [distributional regression] framework and for better [interpretation] we should be ware with both the properties of the [distribution parameters] but also other [characteristics of the distribution] itself. The [characteristics of the distribution] can be defined i) using [moment]s or ii) using [quantile based measures].

#### quantile based measures

A \[quantile based measure\] in a [distribution] is defined as a [characteristics of the distribution] based on the [quantile]s rather [moment]s. For example the measure are mbased no the \[mediam\] the [IQR] e.t.c.

#### IQR

Please see [inter quantile range]

#### inter quantile range

#### classification model

A [classification model] is an [unsupervised learning] [model] in which the output that is the [response] is a [factor] with unknown categories. The model tries to guess what is the best possible value for the response given the [explanatory variables]. The models for [distributional regression] are all [supervised learning model]s in which a [response] exist in the [data].

#### copula

The [copula] methodology is a way of constructing a [multivariate distribution] binding its [marginal distribution]s with a [copula]. The methodology is based on \[Sklar’s theorem\]. It is a convenient way of introducing multiple responses in a \[distributional. regression\] [model].

While [copula] can be defined in higher dimesions here for clarity we are concentrate in 2-dinemseions. Let $X_1$ and $X_2$ two random variables with marginal [pdf] distributions $f_1()$ and $f_2()$ and marginal [cdf]s $F_1()$ and $F_2()$, respectively. Using the [Sklar's theorem] the joint [pdf] of the variables $X_1$ and $X_2$ can be written as $$f(X_1, X_2)= C[F_1(X_1, F_2(X_2)] f_1(X_1) f_2(X_2),$$ where the function C() is a [copula], that is, a [cdf] function of two [random] variables $u_1=F_1(X1)$ and $u_2=F_1(X2)$ defined in the [range] of the two dimensional cube $[0,1]^2$. Note that for the construction of the [likelihood] function of the joint [distribution] of two random [random] variables $X_1$ and $X_2$, assuming that the [pdf]'s and the [cdf]'s are easy to evaluate, we only need the tyoe of the copula function $C()$ to use.

::: callout-note
We need to say someining about the types, the generalazation to motre than two dimensions , [vine copula] and minimal and maximal copula
:::

There are different types of copula; i ) [elliptical copulas] ii) [t copula] iii) Archimedean copulas

In general when someone tries to construct a [multivariate distribution] using copula she/he should look the tail behaviour, symmetry, dimensionality and interpretability of the copula.

#### correlation coefficient

A [correlation coefficient] is a pairwise [measure of association] between two \[continuous variables\] in the [data].\
@brito2025exploring

#### association measures

see [measure of association].

#### measure of association

A [measure of association] is a [statistic] describing the pairwise connection between variables in a the [data]. Note that there are differnt types of [measure of association] dependenting on the type of the \[vecror\]s involve. For exaple association between

-   **continuous** against **continuous** variables

-   **continuous** variable against **categorical** variable

-   **categorical** variable against **categorical** variable

```{r}
#| fig-width: 5
#| label: fig-association
#| fig-cap: "Poirwise measures of association in the Munich `rent99` data set." 
library(gamlss.prepdata)
data_association(da, method = "circle")
```

#### count data distribution

See [discrete distribution]

#### continuous rank probability scores

::: callout-note
We need to define CRPS
:::

#### cross validation

The idea behind [cross validation] is that the [model] performance is validated using a measure of [goodness of fit] on [out of bag] [data] that were not be used in the [fit] the [model].

#### CRPS

The expression [CRPS] is an abbreviation for [continuous rank probability scores] which are a [measure of goodness of fit] appropriate for [distributional regression] [model]s.

#### current model

The [current model] is the model which is compared to the the \[canditate model\] in a [step wise] selection procedure.

################################################################################ 

## D

#### data

The term [data] is a generic term to describe object which contain some kind of [information]. It used to mean a file with many numbers, but [data] today could have different forms. Could be **texts**, **pixels**, or any other form containing some [information] to be extracted . The [model] tries to extract the [information] from the [data]. Note that at a pre-[statistical modelling] stage we can still extract useful information from the [data] to actually help us with the statistical modelling. The package `gamlss.prepdata` is design to help in this direction. Within [distributional regression] [model]s most of the data used are [tabular data]. Note that [data] sets in the statistical programming language `R` are refereed to as [data.frame]s.

#### data.frame

A `data.frame` is the way a [data] set is refer to in the `R` statistical programming language.

#### data partition

The term [data partition] has two different meanings. The fist has to do with [statistical modelling] the second on looking at whether the variables involved behave differently at different parts of the [data].

Data partition in the process of [statistical modelling] helps the model building process, [model] [interpretation], the checking for [over fit] and also helps to improve the [statistical inference] by providing extra [information] about variations in the parameters. There are different types of data partition; a **single** partition of a data set provides holdout samples for prediction and validation purposes. A **multiple** partitions as for example [bootstrapping] and [K-fold cross validation] to help inference.

```{mermaid}
%%| label: fig-data_split
%%| fig-cap: Different ways of partitioning data in order to get more information aboout the model."
flowchart TB
  A[Data] --> B{Holdout} 
  A --> C{K-fold-CV}
  A --> D{Bootstrap}
  B --> E[Training]
  E --> F[Validate]
  F --> G[Test]
  D --> H[Non-parametric]
  D --> K[Bayesian]
  C -->  L[LOO]
```

The second meaning of [data partition] tries to answer the following question. Is there a way to partition the data in such a way that the relationships between the variables in the [data] behave differently?; or more generally can I divide the [data] in such way that different models can apply to different parts of the [data]?. Models like [regression tree]s try to separate the [data] in such a way that the resulted subclass are homogeneous by fitting different constant models withing each part.. $\ldots$

#### data partition based model

A [data partition based model] is a [model] which is based on the assumption that different [variables] in the data affect the [subjects] differently at different parts of the [data]. A [regression tree] is a typical example of a [data partition based model] where the effect of the [response] variable is changing according different values of the explanatory variables. The effect of the explanatory variables on the response is piece wise constant. More complicated models could assume piece-wise linear or more complicated relationships. Th ebasic idea remain the same the [variables] affect the [subjects] differently at diffrent parts of the [data].

#### data analysis

Data analysis is the art of extraction information from the [data]. In any [data analysis] the first question any researcher has to ask is whether the [data] or the [model] could answer the [questions].

#### data generating mechanism

The [data generating mechanism] is a set of assumptions on how the [data] are generated. It is often difficult to check those [assumptions] because different [data generating mechanism]s could possible lead to similar results. Thet is a have a [Rashomon effect] on the bdata rather than the. model. In [distributional regression] often the [data generating mechanism] is an assumption abouth the [distribution] of the [response] given the [explanatory variables].

#### degrees of freedom

The [degrees of freedom] of a model measure the complexity of the model. For mathematical parametric models the degrees of freedom are the number of independent parameters used in the model. For mathematical [smooth model]s the degrees are defined as the diagonal elements of the smoothing matrix i.e. $\hat{\textbf{y}}= S(\textbf{x})$ where $\hat{\textbf{y}}$ are the [fitted values] of the smooth function and $\textbf{x}$ is the [explanatory variables]. For algorithmic models the degrees of freedom often are difficult to calculate.

#### design matrix

By [design matrix] we usually refer to the matrix $\textbf{X}$ containing all relevant explanatory variables. Note that for distributional regression model each paramerter of the [distribution] could have its own design matrix i.e. $\textbf{X}_{\theta_k}$

#### deviance

The [deviance] is a measure of goodness of fit and defined as $-2 \ell(\boldsymbol{\theta})$, that is, minus twice the [log likelihood]. The [deviance] can be used for model comparison.The [deviance] can be defined in both the [training] or the [test] [data] set so we can have a [training] or [test] [deviance]. In [GAMLSS] modelling the [training] [deviance], is used with its penalised form the Generalise Akaike Information Criterion, [GAIC], for model comparison. The deviance can be seeing as an [empirical risk] measure based on [information criterion] concepts. It is a [summary statistic], because it is a sum of the individual deviance increments and therefore an overall measure of goodness of fit. It tell us how well the conditional [distribution] of $D(Y|\boldsymbol{\theta})$ fits the [data] overall. It does not tells whether the tail or the middle of the [distribution] is fitting well. The [deviance increment]s may help to obtain such information.

#### deviance increment

::: callout-note
Definition
:::

```{r}
library(gamlss.ggplots)
fitted_devianceIncr(m1)
```

#### density function

At a theoretical level a [density function] is the [pdf] of a [random] variable. At the [sample] level a density function of a [vector] in the [data] is an empirical [distribution] function of a variable in the [data] obtain using smoothing techniques. In thios case the [density function] is a smooth [histogram].

```{r}
#| label: fig-hist-density
#| fig-cap: "An estimated density funtion on top of a histogram for the variable the price of `rent` in the Munich `rent99` data set." 
#| warning: false 
library(gamlss.data)
library(gamlss.ggplots)
y_hist(rent99$rent)
```

#### diagnostics

The diagnostics are statistical or graphical tools for helping to check the [assumptions] of a fitted [model]. Within [distributional regression] [diagnostics] can help to identifying whether the [distribution] of the [response] is fitted well or whether the [term]s in the different [distribution parameters] models are fitted appropriately.

#### discrete distribution

A [discrete distribution] is [distribution] function [pdf] of a [random] variable which takes only integer values i.e. $0,1,2,\ldots, 10$. Another name for a [discrete distribution] is [count data distribution]. The only property that any [discrete distribution] must have is that it should sums up to one. $\sum_{i=1}^{\infty}  f(y_i)=1$. There are mainly two types of [discrete distribution]s depending on the [range] of the discrete [random] variable involve i) infinite count ii) finite count [distribution]s. The most common example of an infinite count [distribution] is the [Poisson distribution] and the most common example of the finite count is the [binomial distribution].

#### distribution

The term [distribution] refer to a probability [distribution] function [pdf], $f(Y)$. The only property for $f(Y)$ is required is that it should sum up to one. In distributional regression we write the [pdf] of the [response] as $D(Y | \boldsymbol{\theta})$ not $f()$ a notation which could be used for describing the relationships between the [explanatory variables] and the [response]. We are also emphasising that the [pdf] of the response is a function the different [distribution parameters], $\boldsymbol{\theta}$. The function $D(Y | \boldsymbol{\theta})$ describes how the behaviour of the [response] variable is affected by the [explanatory variables] and it is the main [stochastic] component of a [distributional regression].

#### distribution parameters

Within a [distributional regression] we use the term [distribution parameters] as almost synonymous to [distributional parameters] to refer to the parameters $\boldsymbol{\theta}$ in the [distribution] of the [response] $D(y|\boldsymbol{\theta})$, see also [distributional parameters].

#### distributional parameters

Theoretical distributions depend on [distribution parameters] $\boldsymbol{\theta}$ where $\boldsymbol{\theta}=(\theta_1, \theta_2, \ldots, \theta_K)$ is a [vector] of length $K$. The more (independent) [distribution parameters] exist in a [distribution] family the more flexible the [distribution] is. For example one parameter distributions can only model one characteristic of the [distribution] of the [response] mainly the [location parameter].

#### distributional regression

A distributional regression model is an [input-output model] model in which the response $Y$ is assumed to have a [distribution] in which all the parameters of the [distribution] could depend on explanatory variables. This can be represented as $X \rightarrow D(Y|\boldsymbol{\theta}(X))$ where $\boldsymbol{\theta}$ are the k parameters of the distribution.

#### dummy variables

A dummy variables is a set of binary (0 ,1) vectors indication whether certain conditions are present or not. Any `factor` is represented as a set of dummy variables in a `design matrix` of a model. The number of columns in the dummy variable set representation of a factor are the number of `levels` of factor minus one to avoid collinearity with the constant a vector of ones in the design matrix.

################################################################################ 

## E

#### empirical cdf

The [empirical cdf], [ECDF] is the cumulative function function of any continuous or ordered categorical variable in the [sample], ([data]). The [empirical cdf] is a step function that shows the proportion of observed [data] points less than or equal to a specific value in the [data] . It creates a cumulative probability [distribution] directly from a vector say $\textbf{x}$ in the [data], without assuming a theoretical model therefore is the source of a many non-parametric procedures in statistics. Any [vector] $\textbf{x}$ in a [tabular data] set can have an [empirical cdf] but it is more difficult to justify it for unordered categorical variables since it implies an explicit order in $\textbf{x}$. For a given value $x_o$ in $\textbf{x}$ for any continuous or ordered categorical variable in the [data] the [empirical cdf], [ECDF] gives the percentage of [data] points in $\textbf{x}$ less than or equal $x_o$ for example, \#$[\textbf{x} \le x_o] \times \frac{1}{n}$. @fig-ECDF-continuous shows the ECDF curve of the continuous variable `rent` from the `rent99` data set.

```{r}
#| warning: false
#| label:  fig-ECDF-continuous
#| fig.cap: "The ECDF of a continuous variables. "
library(gamlss)
plot(ecdf(rent99$rent), main="ECDF", xlab="rent")
```

@fig-ECDF-ordered shows the ECDF curve of the oerdered categorical variable `location` from the `rent99` data set.

```{r}
#| fig-width: 5
#| label:  fig-ECDF-ordered
#| fig.cap: "The ECDF of an ordered categorical variables."
plot(ecdf(rent99$location), main="ECDF", xlab="rent")
```

The [empirical cdf] is also defined on the [residuals] of a fitted [model]. In [distributional regression] if the model represent correctly the [data generating mechanism] then the [residuals] of a [distributional regression] [model] should be behave as a [white noise]. In this case the empirical \[cumulative distribution\] function of the residuals should be very close to the \[cumulative distribution\] function of the Normal distribution. Next we fit a GAMLSS model using the `gamlss2()` function to the `rent99` [data]. We use the [formula] `rent~s(area)+s(yearc)+location+kitchen+kitchen` for the $\mu$ model and the [formula] `s(area)+s(yearc)` for $\sigma$ and we fit a \[BCT\]o distribution. The [ECDF] of the residual of the fitted [model] should behave, if the model is a [adequate fit] as normally distributed [random] variables. @fig-ECDF-resid_normal show the cdf of the [residuals] with a normal [cdf] superimposed in red. From the plot there is no reason he doubt the adequacy of the model. Note that the same information could be extracted by plotting the [QQ-plot] or the [worm plot] of the residuals.

```{r}
#| fig-width: 5
#| label:  fig-ECDF-resid_normal
#| fig.cap: "The ECDF of the residuals from a GAMLSS fitted model with the normal cdf superimposed."
#| warning: false
library(ggplot2)
library(gamlss.data)
library(gamlss2)
m1 <- gamlss2(rent~s(area)+s(yearc)+location+kitchen+kitchen|
                s(area)+s(yearc), family=BCTo, data=rent99 )
library(gamlss.ggplots)
gg <- resid_ecdf(m1)
gg+ggplot2::stat_function(fun = pNO, args=list(mu=0, sigma=1), col="red")
```

#### ECDF

See [Empirical cdf]

#### elliptical copulas

The [elliptical copulas] are based on the multivariate normal [distribution] and they can capture linear dependencies but no extreme tails

#### empirical copulas

::: callout-note
Definition
:::

#### empirical risk

The [empirical risk] function is defined by dropping the expecations from the definition of the [risk] function, The definition of the [empirical risk] is given in [risk] as; $$\mathbb{ER}(g) = \frac{1}{n}\sum_{i=1}^n [ \ell oss(\hat{g}(x_i), y_i).$$ where $\ell oss()$ is the [loss] function.

#### entropy

The [entropy] for a discrete [random] variable $X$ is defined as: $$
\begin{split}
H(X) & = -\sum_{x \in X} p(x) \log p(x), \\
     &=  - E_p \log p(x) ,\\
     &= E_p \log \frac{1}{p(x)}
\end{split} $$\
That is, the [entropy] for a discrete random variables $X$ is equal to its expected value of minus the its log [probability]. Note that this quantity is identical to the [log likelihood] (but the [log likelihood] it is a function of the parameters and not the [random] variables $X$). The [entropy] of a continuous random variable $X$ is given as: $$\begin{split}
H(X) &=-\int_{X}f(x) \log f(x)  dx \\
      &= - E_f \log f(x)  \nonumber \\
     &= E_f \log \frac{1}{f(x)}  \nonumber 
\end{split} $$ The [entropy] is a [summary statistic] describing the randomness of the [distribution] of a [random] variables and it is a function of probabilities **not** the values or the [range] of the [random] variable $X$. The highest value of entropy is when we have equal probabilities for all values of $X$'s. That is when the $X$ has a [uniform distribution]. Note that the minus the log likelihood of a fitted [distributional regression] [model] which is defined as $- \ell(\boldsymbol{\theta})=\sum_{i=1}^{n} - \log f(y_i|\boldsymbol{\theta})$ is proportional to the empirical estimated of the entropy of the [response] variable with the constant of proportionately equal to $\frac{1}{n}$. This shows the close relationship between the concept of [entropy] and the concept of the [log likelihood] important for statistical [inference].

#### error

The term [error] of a [model] is the component which makes the model [stochastic], that is, it the model contains a set of [probability] elements. In [regression analysis] the [error] is often a terms used to describe the residual variation of a model.

#### errors

The term [errors] refers to the statistical part of a [stochastic] model which accounts for the natural variability of the data. The [errors] often refer to the [residuals] part of the [model] that is what is left after the model has been applied.

#### estimates

Let us assume that the [model] is specified by the [parameters] $\boldsymbol{\theta}$, After a [fit] we have [estimates] for the $\boldsymbol{\theta}$ which often present them with a hat $\hat{\boldsymbol{\theta}}$.

#### exceedance probability

By [exceedance probability] we refer to the probably that a values of a [random] variable, $Y$, will exceed a certain pre specified value $c$, that is $Pr[Y \ge c]$ If we know the [cdf], $F(.)$ of the random variable $Y$ its [exceedance probability] is defined as $S(Y)=1-F(Y)$ where the function $S()$ often called in statistics the [survival function].

#### explanatory variables

Explanatory variables in an [input-output model] are the input variables $X$. In a [regression analysis] are the variables in which affect the response. In a [distributional regression] are the variables on which the [distribution] of the response is conditioned on.Note that not all variables in the data need to be [explanatory variables] in a [model] only a relevant subset and [feature]s which are functions of the [explanatory variables].

#### exponential family

The [exponential family] is a family of theoretical [distribution]s with the propety of allowing [sufficient statistic]s

::: callout-note
Definition
:::

#### expected value

The [expected value] (the [mean] or first [moment]) of a [distribution] is defined as $\mathbb{E}(Y)=\int Y f(Y)dY$ for [continuous distribution]s and $\mathbb{E}(Y)=\sum Y f(Y=y)$ for [discrete distribution]s where the ingration of summetion, repsectively, is over the [range] of the [random] variables $Y$.

#### extreme value copulas

::: callout-note
Definition
:::

################################################################################ 

## F

#### factor

A [factor] is a \[categorical variable\] in the [data]/[sample]. A [factor] is a [vector] column in the [data] set which takes only limited values which rae called the [level]s. The [level]s can be **unordered** or **orderer**.

The function `str()` in `R` show the tyoe of [vector]s in the [data].

```{r}

```

#### fit

By [fit] we mean fitting a [model] to the [data] by minimise a [measure of goodness of fit] to select appropriate values for the [parameters] of the model. As a final results of the [fit] we have [estimates] for the [parameters]. A single [fit] could provides unique [fitted values] and [residuals] for both [training] and [test] data sets.

#### fitted values

In [distributional regression] we use the expression [fitted values] to indicate estimates for the [distribution parameters] $\boldsymbol{\theta}$. In each fitted model there are as many [vector]s (of length $n$) of \[fitted parameters\] as the number of the assumed [distribution parameters].

To extract in `R` the fitted values of a fitted `gamlss2` model use;

```{r}
head(fitted(m1))
```

By default `head()` give only the first 6 values of the $n$ length vectors. Note that the resulted vectors are in the scale of the 'predictors' not the original \[paramatrers\]. Similar result can be onbtain using function `predic(., type="link)`

```{r}
head(predict(m1,  type="link"))
```

#### feature

A [feature] in [data] analysis is an one of the [explanatory variables] used for the model possible after a [transformation]. In statistical modelling a [feature] is a function of the [explanatory variables] in the [data].

#### function

There are several occasions we use the terms [function] in [statistical modelling]. In an [input-output model] we refer to the function $g()$ as an true unknown relationship between $X$ and $Y$ which the [model] tries to approximate. In the definition of distributions we use the functions $f()$ and $F()$ to describe [pdf]'s and [cdf]'s respectively. In [distributional regression] we use $D()$ for the [pdf] of the response. In [GLM]'s [GAM]'s and [GAMLSS]'s we use $g()$ to describe a [link function] a function which relates of the [distribution parameters] i.e. $\mu$ to the [predictor] of the parameter i.e. $\eta_{\mu}=g(\mu)$.

#### predictor

A [predictor] in a [regression analysis] helps to model one of the [distribution parameters] i.e. $\boldsymbol{\theta}_i$. It help the [statistical modelling] process by ensuring that the \[distribution parameter\] $\boldsymbol{\theta}_i$ is always in the correct range. Consider the following example, let us assume that $0 < \boldsymbol{\theta}_i < \infty$ that is, $\boldsymbol{\theta}_i$ is always positive. If we modelled $\boldsymbol{\theta}_i$ as a function of explanatory variables i.e. $\boldsymbol{\theta}_i= c+s(x_1)+ s(x_2)$ occationally maybe $\boldsymbol{\theta}_i$ could go to negative values. But if we model the log of $\boldsymbol{\theta}_i$ using the [link function] $\eta_{\boldsymbol{\theta}_i}=\log(\boldsymbol{\theta}_i)=c+s(x_1)+ s(x_2)$ the parameter $\boldsymbol{\theta}_i= \exp(\eta_{\boldsymbol{\theta}_i})$ is always positive.

::: callout-definition
A [predictor] is a one to one function of the [distribution parameters] which when modelling the parameter as a function of [explanatory variables] insures that that the \[distribution parameter\] will be in the right range.
:::

#### final model

Is the [model] chosen after a [model selection] is applied. In general we would like the [final model] to be an [adequate model]. That is, a [model] represending the [data] well and cable to answering the [questions] in hand.

#### finite mixture

A finite mixture is a [distribution] [pdf] make up as a sum of $K$ different distributions functions $f_k(x)$.

$$
f(x) = \sum_{k=1}^{K} \pi_k f_k(x),
$$ where: $f_k(x)$ are probability density [pdf]s functions, $\pi_k$ are mixing weights which should sum up to one, $\sum_{k=1}^{K} \pi_k = 1$. A [finite mixture] [distribution] is useful in [distributional regression] when the [response] show multi-modality. That is, when there are different [mode]s peaks in the response.

#### first order interaction

#### Frank copula

#### formula

The term [formula] and formulae refer to the `R` [formula] object which is used in the definition of a model. A formula in `R` stars with $\sim$, for example, `rent~area+yearc+location` wuold indicate that the response is `rent` and the variables `area`, `yearc` ans `location`. Note that because within [distributional regression] there could be one formula for each parameter

################################################################################ 

## G

#### GAIC

GAIC stand for the Generalised Akaike Criterion @Akaike83 defined as `deviance` $+ K \times df$ where $df$ are the `degrees of freedom` and $K$ stands for the `penalty`.

#### GLM

GLM stands for Generalized Linear Model, @NelderWeddeburn72, a [mathematical model] which dominated the 1980's. As [input-output model] it can be written as $$
    X  {\longrightarrow} \fbox{f()}  {\longrightarrow} \mathbb{E}(Y) 
    $$ where $\mathbb{E}(Y)$ is the expected values of the output $Y$ and $f()$ is a general function connecting the $X$'s with the expected value of $Y$. As a [statistical model] a GLM can be written as; $$
\begin{split}
\textbf{y}    &  \stackrel{\small{ind}}{\sim }  D( \boldsymbol{\mu},  \phi) \nonumber \\
\eta &= g(\boldsymbol{\mu}) &= \textbf{X}\boldsymbol{\beta} \nonumber \\
\end{split} $$ where $\stackrel{\small{ind}}{\sim}$ is read as the vector of the response is "independently distributed [random] [vector] having a [distribution] $D()$." The [distribution] $D()$ belongs to the [exponential family] which has as sub-models the \[normal distribution\], the \[gamma distribution\], the \[inverse Gaussian\], the [Poisson distribution] ans the [binomial distribution]. The fact that the elements of the vector $\textbf{y}$ are independent from each has the consequence that the log-likelihood is simply the sum of the individual log-[likelihood]s.

other with different mean $\mu$ depending linearly on the $X$'s but with a common second parameter $\phi$. which make sure that values of any \[distribution parameter\] $\theta$ are in the right [range].

There were two major problems with the assumption for $g(\boldsymbol{\eta})$

#### GAM

GAM stands for Generalized Additive Model, @HastieTibshirani90, ...

#### GAMLSS

GAMLSS stand for Genaralized Additive Model for Location, Scaler and Shape, @RigbyStasinopoulos05. The classical GAMLSS model as first defined by @RigbyStasinopoulos05; $$
\begin{split}
y| \boldsymbol{\gamma}     &  \stackrel{\small{ind}}{\sim } \mathcal{D}( \boldsymbol{\theta}_1, \ldots, \boldsymbol{\theta}_k) \nonumber \\
g_1(\boldsymbol{\theta}_1) &= \textbf{X}_1 \boldsymbol{\beta}_1+ s_{\theta_1,1}(\textbf{x}_{\theta_1,1})+\ldots+ s_{\theta_1, J_k} (\textbf{x}_{\theta_1,J_{\theta_1}}) \nonumber \\
 \cdots &= \cdots \nonumber\\
g_k(\boldsymbol{\theta}_K) &= \textbf{X}_K \boldsymbol{\beta}_K + 
s_{\theta_K,1}(\textbf{x}_{\theta_K,1})+\ldots+ s_{\theta_K, J_K} (\textbf{x}_{\theta_K,J_{\theta_K}})
\nonumber 
 \end{split}
$$ {#eq-gamlssclassical} where we assume that the response variable $y_i$ for $i=1,\ldots, n$, is independently distributed having a [distribution] $\mathcal{D}( \theta_1, \ldots, \theta_k)$ with $k$ parameters and where $s_{\theta_k, j} (.)$ for $k=1,\ldots,K$ and $j=1,\ldots, J_{\theta_k}$ are smoothers for different explanatory variables. Note that smoother are depending on smoother parameters $\lambda$. Because of the duality of smoothers and random effects it was soon realise that the model can be a written in its **mixed** random effect model form as:

$$
\begin{split}
y| \boldsymbol{\gamma}   &  \stackrel{\small{ind}}{\sim }  \mathcal{D}( \boldsymbol{\theta}_1, \ldots, \boldsymbol{\theta}_k) \nonumber \\
g_1(\boldsymbol{\theta}_1) &= \textbf{X}_1 \boldsymbol{\beta}_1+ \textbf{Z}_{1,1} \boldsymbol{\gamma}_{1,1}+ \dots +\textbf{Z}_{K, J_1} \boldsymbol{\gamma}_{K, J_1} \nonumber \\
 \cdots  &= \cdots \nonumber\\
g_k(\boldsymbol{\theta}_K) &= \textbf{X}_K \boldsymbol{\beta}_K + \textbf{Z}_{K,1} \boldsymbol{\gamma}_{K,1}+\dots+\textbf{Z}_{K,1} \boldsymbol{\gamma}_{K,J_K}  \nonumber
 \end{split}
$$ {#eq-gamlssmixed} where $\boldsymbol{\gamma}_{k,j}$ are random effect, for $k=1, \ldots,K$ and $j=1,\ldots,J_K$ and where each $\boldsymbol{\gamma}_{k,j}$ is distributed as a Normally distributed variable with zero mean and standard deviation given by $\sigma_{b_{k,j}}$. In fact there is set of $\sigma_{b_{k,j}}$'s which length is the length of all smoothers in the model. Note that smoothing parameters $\lambda_{\theta_k, j}$ and standard errors $\sigma_{b_{k,j}}$ are inverse related $\lambda_{k, j}=\frac{1}{\sigma_{b_{k,j}}}.$

#### `gamlss` {#gamlss-1}

`gamlss` is the original package in `R` for fitting a GAMLSS model @StasinopoulosRigby07.

#### `gamlss2`

`gamlss2` the new package in R for fitting a GAMLSS model.

#### `gamlss.data`

`gamlss.data` is a package in `R` containing data sets use to demonstrate GAMLSS models.

#### `gamlss.prepdata`

`gamlss.prepdata` is a package in R to help users to preper data for analysis using a [distributional regression] model.

#### `gamlss.ggplots`

`gamlss.ggplots` is a package in `R` to help with diagnostics and other graphics using the `ggplot2` package. The functions in the package can be applied to fitted GAMLSS model independently whether were fitted using the [`gamlss`](#gamlss-1) or [`gamlss2`].

#### `gamlss.dist`

`gamlss.dist` a package containing all the theoretical [distribution] which can be assumed for the response when fitting a GAMLSS model.

#### `gamlss.tr`

`gamlss.tr` can take any [distribution] family in `gamlss.dist` and truncated `left` `right` of in `both` directions.

#### `gamlss.cens`

`gamlss.cens` can take any [distribution] family in `gamlss.dist` and apply `left` `right` or `interval` censoring.

#### Gaussian white noise

An identical distributed standardised normal [random] variable with no pattern, see [white noise] see [white noise].

```{r}
#| fig-width: 5
#| label: fig-gaussianwhitenoise
#| fig-cap: "Gaussian white noise of 1000 observations"
y <- rnorm(1000)
plot(y)
```

A [worm plot] of a [Gaussian white noise] should show no pattern:

```{r}
#| fig-width: 5
#| label: fig-wpgaussianwhitenoise
#| fig-cap: "A worm Gaussian white noise of 1000 observations"
library(gamlss)
wp(resid=y)
```

#### generalized Tobit model

#### generalized Tobit distribution

#### goodness of fit

A goodness of fit is a measure (a quantity) design to evaluate how close the model is to the data.

#### GAIC

GAIC is the Generalise Akaike Information Criterion defined as `deviance` $+k \times df$ where $k$ is the penalty applied to the [degrees of freedom] $df$.

#### graphic

A [graphic] is a plot which help the analysis of [data]. A post-[fit] graphic that is a plot depending on the [fitted values] of a [model] usually is either a \[diagnostic\] to help checking assumptions or it is there to help the [interpretation] of the model.

#### Gumbel copula

For [random] variables $U_1$ and $U_2$ with [range] in $(0,1)$ and parameter $\theta \ge 1$ the [Gumbel copula] is defined as

$$C_\theta(U_1,U_2)
=
\exp\!\left(
-\Big[(-\log U_1)^\theta + (-\log U_2)^\theta\Big]^{1/\theta}
\right)$$ The [Gumbel copula] belong to the \[Archimedean copula\] family. It can model only positive dependence only and only the upper tail. It can not model the lower tail dependence. It is asymmetric stronger in upper tail. If the parameter $\theta = 1$ implies independence. Large $\theta$ implies stronger dependence and $\theta \to \infty$ implies comonotonicity. The relationship of $\theta$ with Kendall’s $\tau$ is:

$\tau = 1 - \frac{1}{\theta}
\quad \Rightarrow \quad
\theta = \frac{1}{1-\tau}$ The [Gumbel copula] supports strong upper tails and its useful for modelling extremes high values cluster. The upper tail dependence coefficient is: $\lambda_U = 2 - 2^{1/\theta}$ ans the ;ower tail dependence is: $\lambda_L = 0$. The [Gumbel copula] should be use if: i) joint large outcomes are important, ii) dependence increases in the upper tail, or the work involves extreme rainfall / floods, financial booms, large insurance claims.

################################################################################ 

## H

#### hazard function

The [hazard function] of a [random] variable $Y$, $h()$, shows the risk of the instantaneous failure of $Y$. It is defined as the [pdf] divided by the [survival function] i.e. $h(Y)= \frac{f(y)}{S(Y)}=\frac{f(y)}{1-F(Y)}.$

```{r}
library(gamlss.dist)
gen.hazard("BCTo", mu=1, sigma=0.2, nu=1, tau=10)
curve(hBCTo, from=0.001, to=10)
```

Or using `ggplot2`;

```{r}
 library(ggplot2)
 ggplot(data.frame(x = c(0.001, 10)), aes(x)) +
   stat_function(fun = hBCTo) +
   labs(x = "x", y = "S(x)")
```

#### histogram

A [histogram] is a estimated density probability function function for a [sample] [vector]. See [density function] for a [smooth function] of a [histogram].

```{r}
library(MASS)
truehist(rent99$rent)
```

#### hierarchical Archimedean copulas

#### hypothesis

The hypothesis is what the researcher tries to understand and answer see also the `purpose` of the study

#### hyper parameter

################################################################################ 

## I

#### independence

The word [independence] is refereeing to the state of two or more [random] variables. For example two random variance $Y_1$ and $Y_2$ are independent if their joint density [probability] can be written as the product of their marginal; $f(Y_1, Y_2)= f(Y_1) \times f(Y_2)$. In the same way if the probability of more than two random variables can be written as the product of their marginals then then the [random] variables are independent.

#### information

The word [information] represent the uncertainty about an unknown quantity, in our case the [data]. In [statistical modelling] is refereed to the way [information] is extracted from [data] using a [model].

#### information criterion

By [information criterion] we refer to a [measure of goodness of fit] which it based on [information theory]

#### information theory

By [information theory] we mean the theory of of communication developed by @shannon1948mathematical which is based on the idea of [entropy].

#### interpretable model

A model is an [interpretable model] if its is easy to explain to others. Most [mathematical model]s but not all [algorithmic model]s are [interpretable model]s. The following words have the same meaning as [interpretable model], **transparent** model, **explainable** model or **comprehensive** model.

#### interval values

A [random] variable is taking [interval values] if its exact value observed value is not known but we know that it occurred in a specific interval [range] i.e $10< Y <15$. Another expression of this behaviour is [censored values].

#### input-output model

An input-output model is a model where the variables $X$, the input, affect the variables $Y$, the output i.e. $X \rightarrow Y$. Input-output model are [supervised learning model]s since a response variable, $Y$ always exist. @breiman2003statistical use the diagram $$
X  {\longrightarrow}  \fbox{NATURE} {\longrightarrow} Y, \ 
$$ but nature is too complex so we use a `model` to help us. $$
X  {\longrightarrow}  \fbox{Model} {\longrightarrow} Y \
$$ In practice we use a mathematical function $g(X)$ to describe the relationship. $$
X  {\longrightarrow} \fbox{g()}  {\longrightarrow} Y \ 
$$ The function $g(.)$ is unknown so the task of the modeller is to find such a function. In a distributional regression framework the represanation is more complex. $$
X  {\longrightarrow} \fbox{g()}  {\longrightarrow} D(Y|\theta(X)) \ 
$$

#### independent random variable

A [vector] of [random variable]s say $\textbf{y}$ is said to be independent (or independently distributed) if the [distribution] of the component of the vector can be written as the product of the individual probabilities i.e. $Pr(\textbf{y}) =  \prod_{i=1}^n Pr({y}_i)$

#### independent variables

The [explanatory variables] in a [regression analysis]

#### interaction

#### inference

In statistics [inference] refers to the idea that under certain circumstances we can go from the [sample] [data] to draw conclusions about the [population] of interest, That is we can can go from the **small** to the the **big**. The circumstances depend on [assumptions] about the \[data generation mechanism\].

#### interpretation

The interpretation of a model is the story behind the fitted model, what it is telling you.

#### input variables

The [explanatory variables] in a [regression analysis] same as the input in an [input-output model].

################################################################################ 

## J

################################################################################ 

## K

#### K-fold cross validation

A K-fold Cross Validation provides [training] and [test] data for all the observations by going through all the K-folds, It provides unique `fitted values` and `residualas` and in addition provides multiple $K-1$ values for fitted values and residualas for the training $K-1$ folds.

#### kurtosis

#### kurtosis parameter

################################################################################ 

## L

#### LASSO

#### least squares

#### level

By [level]s we refent to the distict values of a [factor].

#### linear model

By linear model we refer to models described by the [assumptions] $\textbf{y}=\textbf{X}\boldsymbol{\beta}+e$ where $e_i \sim N(\boldsymbol{0}, \sigma^2)$. That is the relationship between the response and the explanatory variables is linear determined by the coefficients $\boldsymbol{\beta}$ and the error term has a normal [distribution] with constant variance $\sigma^2$. The unknwon parameters in this case are the $\boldsymbol{\beta}$ and $\sigma^2$. I we know those parameters we can predict the behaviour of the response if the model is correct.

#### link function

A function connecting the model predictor with the \[distribution parameter\] for example $\eta_{\sigma}= g(\sigma)$ The link function ensure that the \[distribution parameter\] is on the right [range]. For example, since $\sigma$ take values in the positive real line $0< \sigma <- \infty$ modelling sigma as $\eta_{\sigma}=log(\sigma)$ will make sure that $\exp(\eta_{\sigma})$ is always positive. For modelling purposes the the link function $g(\theta)$ its inverse $g^{-1}(\eta_{\theta})$ and the first derivative $d \eta_{\theta}/d\theta$ must be defined.

#### likelihood

The likelihood function is define as the [probability] of observing the [sample] seeing not as a function of the random variable involed but as a function of the parameters of its distribution. For example, let the random vector variable $Y$ to come from an assumed [distribution] $f(\textbf{Y}|\boldsymbol{\theta})$. We obsrved the $n$ dimensional vector $\textbf{y}$. If in addition we assume that the elements of $\textbf{y}$ are independed fraom each other, then the likelihood funtion for $\boldsymbol{\theta}$ is defined as $$L(\boldsymbol{\theta})=\prod_{i=1}^{n} f(y_i|\boldsymbol{\theta})$$ and its log-likelihood as

$$\ell(\boldsymbol{\theta})=\sum_{i=1}^{n} \log f(y_i|\boldsymbol{\theta})$$ In both cases the functions are seen as functions of the parameters $\boldsymbol{\theta}$ rather than of the ranom variable $\textbf{y}$. Note also that the assumed [data generating mechanism] plays a role in the construction of the likelihood function. If we suspect that the data are not independent from each other the definition of the likelihood sgould be different.

#### log likelihood

The [log likelihood] is a [measure of goodness of fit] for a [model]. It is denoted as $\ell(\boldsymbol{\theta})$ see [likelihood] for its definition. Note that in general most of [measure of goodness of fit] depend on the [data generating mechanism].

#### location parameter

A parameter of the [distribution] describing the centre of the [distribution]. The most common [location parameter]s are the [mean] and the [median].

#### loss

A [loss] function is a [measure of goodness of fit]. The [loss] function $\ell oss()$ is a function of both the [data] and the fitted [model] and measures how far we are prepared to accept that the model is correct. In gambling the [loss] function has a monetary value but in [statistical modelling] is a measure of difference between the [model] and the [data], (see also [risk]). Typical loss functions are; i) \[squared errors\] i.e. $\sum_{i}^n (y_i - \hat{y_i})^2$ and \[absolute errors\] i.e. $\sum_{i}^n \left|y_i - \hat{y_i} \right|$ determine how far the actual value $y$ is from its estimating value $\hat{y}$. Note that in [distributional regression] $\hat{y}$ is an estimate of the [location parameter] of the the model therefore is not an appropriate measure we are interest in other parts of the [distribution] fo the [response] for example the [tail]. a more appropriate measure in this case could be minus the the [log likelihood] $-\ell(\hat{\boldsymbol{\theta}}_i)$. Notice that [loss] functions could include penalty terms penalising the complexity of the model.

#### lower model

The [lower model] is the simplest possible model in the range of all possible models to be selected in a [step wise] procedure. Often it is the [null model].

################################################################################ 

## M

#### main effect

#### marginal distribution

#### mathematical model

A [mathematical model] is a [model] which is build using mathematical equations.

#### MLE

see \[aximum likelihood estimation\]

#### maximum likelihood estimation

The Maximum Likelihood Estimation, MLE, is method of fiting a [distribution] to a vector $\textbf{y}$ using minus the [log likelihood] as [measure of goodness of fit]. Note the minimise $-\ell(\boldsymbol{\theta})$ is equivalent of ninimise the [deviance] $-2\ell(\boldsymbol{\theta})$ or maximising the [likelihood] $L(\boldsymbol{\theta})$ or the [log likelihood], $\ell(\boldsymbol{\theta})$.

#### measure of association

#### measure of goodness of fit

A measure of goodness of fit is a way to evaluate the fidelity of the data for a given. model. That is, how close, is the model to the data using an objective measure. of goodness of fit.

#### mean

A [mean] is defined either as one of the [characteristics of the distribution] of a [random] variable or as a characteristic of a [vector] in the [data] set. The mean of a [vector] in the [data] is the average value of the [vector] i.e. $\bar{x}= \frac{1}{n}\sum_{i=1}^n x_i.$ The mean or [expected value] of a [distribution] sometimes called the first [moment] of the [distribution] is defined as $\mathbb{E}(Y)=\int_{-\infty}^{\infty} Y f(Y)dY$ for a [continuous distribution] and as $\mathbb{E}(Y) = \sum_{i=1}^{\infty} Y_i f(Y_i)$ for a [discrete distribution].

#### median

A [median] is defined either as one of the [characteristics of the distribution] of a [random] variable or as a characteristic of a [vector] in the [data] set. Let $F(Y)$ be the [cdf] of the random variable $Y$ then the [median] of $Y$ is defined as the value of $Y$ satisfying the equation $0.50=F(Y)$ or $\text{median}(Y)=F^{-1}(0.50)$ where $F^{-1}()$ is the inverse of the [cdf] also known as the [quantile function]. For [vector] in the [data]/[sample] the median can be defined similarly using the [empirical cdf] of the [vector].

#### mode

A [mode] is defined either as one of the [characteristics of the distribution] of a [random] variable or as a characteristic of a [vector] in the [data] set. The [mode] as one of the [characteristics of the distribution] indicates which the value of a [random] variable $X$ in which it has its higher [pdf] value. In a [vector]s in the [data], $\textbf{x}$, the [mode] indicates the value of $\textbf{x}$ with the highest frequency of occurring.

#### moment

#### mixed distribution

#### questions

The [questions] are the [purpose] of the study and what the researcher is interested to investigate by collecting the [data]. One should ways ask the question on whether the [data] could answer the [questions].

#### mathematical model

A mathematical model is a model which is using mathematical equations to describe the variables involved. A popular mathematical model partial or ordinary differential equations model

#### machine learning

Machine learning refers to a selection of algorithmic models. Some of the machine learning are [model]s are \[supevised learnig\] models i.e [regression] type and some \[classification\] type. A typical [regression] [machine learning] [model] has the form $\textbf{y}=g(\textbf{x})+\boldsymbol{\epsilon}$ where the error $\boldsymbol{\epsilon}$, a [vector], is assumed to be an independently distributed [random] variable and $g()$ an unkown function to be estimated form the data. Note that one of the implicit [assumptions] of the model is that the [error], $\boldsymbol{\epsilon}$, has a symmetric distribution. This explicit assumption can be recrtified by using a [transformation] on $\textbf{y}$ which would possible make the transformed response $\textbf{y}'$ symmetric before fitting. Note however that this transformation is not always exist therefore [distributional regression] models do not rely on it but relying on the fact that the a theoerical [distribution] of the [response] exist.

#### MSE

The term stands for Mean Square Error. Its defined as $$\text{MSE} = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2$$ and it is an [empirical risk] and a [measure of goodness of fit] for the standard [regression analysis] [model]s. It can be calculated in both [training] or [test] data sets for a [measure of goodness of fit] or a measure of [prediction] power, respectively. MSE is **NOT** appropriate for [distributional regression] models unless the [purpose] of the study is on the centre of he assumed distributions.

#### MAE

The term {MAE} stands for Mean Absolute Error. It is defined as $$\text{MAE} = \frac{1}{n}\sum_{i=1}^n |y_i - \hat{y}_i|$$ and it is an [empirical risk] and a [measure of goodness of fit] for the standard [regression analysis] [model]s. It can be calculated in both [training] or [test] data sets for a [measure of goodness of fit] or measure of [prediction] power, respectively. MAE is **NOT** appropriate for [distributional regression] models unless the [purpose] of the study is on the centre of he assumed distributions.

#### measure of goodness of fit

A measure of goodness of fit is usually an [empirical Risk] measure. For exampe the mean square error, MSE use in LS model, or the log-likelihood used in GLM and GAMLSS.

#### model

A [model] is a simplification of reality to provide simple way to understand the structure of a problem. There are different types of [model]s according to [purpose] of the study. The [purpose] of the study is defined by the [questions] the investigator who contact the study need to answer. In [data analysis] we can think of a [model] as a filter which tries to extract the important information from the data while disregard the non-relevant see @fig-filter.

```{mermaid}
%%| fig-width: 5
%%| label: fig-filter
%%| fig-cap: "The model acts as filter to extract  important information  from the data."
flowchart LR
    direction LR
    D[data]
    F[model ]
    I(information)
  D --> F
  F --> I
```

In [data analysis], often, there are basic [subjects] of interest and there [variables] which affect the subject behaviour. Any study would trying to understand the behaviour of the [subjects] at different values of the [variables].

-   A [mathematical model] tries to answer the [questions] of the [study] using mathematical equations.

-   An [algorithmic model] is using algorithms that is a set of programmable rules to answer the [questions].

Another important distinction between models is;

-   A [top down model] in which the [variables] influence is applied to all the subjects. That is, there isa **macro** influence of the [variables] applying to all [subjects]. Typically a [regression analysis] model is a [top down model].

-   A [bottom up model] in which the **micro** behaviour of the objects affect the overall global behaviour of the variables. A typical example of this type of [model] is the [agent based model] whete the individual behaviour of the [agent] affect the overall behaviour. A [agent based model] is a [simulation model] where the behaviout of variables is examing after the simulation of the model several times.

A [data partition based model] is a [model] which assumes that different [variables] in the data affect the [subjects] differently at different parts of the [data].

An [adequate model] is a [model] which fits the [training] data well after [diagnostics] checks and its [interpretation] will help to answer the [questions], but remember that;

> `all models are wrong but some are useful`.
>
> -- George Box (@box1979robustness)

Whether a model is useful depends on the [purpose] and whether could answer the [questions] of the study.

#### model average

The term [model average] or [stacking] is when the [final model] is chosen by averaging different [fit]ted [model]s. This prevents us from choosing a single model by summarizing the results from fits from multiple models. [model average] is advantageous if the fitted models complement each other. For example, in a [distributional regression] framework, this could mean the one model fit the [tail] of the [distribution] better while an other fits the middle better. By averaging the [model]s well we could possible fit both the middle and the [tail] well. There are a lot of way to average [model]s. In [distributional regression] a possibility is a [finite mixture] [distribution] with elements all fitted distributions. For example $$ f_F(y_i)= \sum_{i=k}^{K} f_k(y_i| \hat{\boldsymbol{\theta}}^{k}_{i}) $$ where $i=1,2,\ldots, n$ and we assumed K different [model]s with assumed distributions $f(|\boldsymbol{\theta}^{k})$.

#### model fitting

A [model fitting] process involves the [data], the [model] (which depends on [assumptions]) and the [purpose] of the study. The assumed [model] is fitted to the [data] by minimise a [measure of goodness of fit]. @fig-fit shows a single [fit] to the [data] which should reflect the [purpose] of the study. Note that we will need a [model selection] procedure to make sure that we end up with an [adequate fit]. A [adequate fit] provides evidence that we have not [under fit] but we could have [over fit]. Over fit could be checked with [test] data. A single [fit] provides unique [fitted values] and [residuals] for both [training] and [test] data sets.

```{mermaid}
%%| fig-width: 5
%%| label: fig-fit
%%| fig-cap: "The model fitting process needs both data and model which to conform with the purpose of the study  "
flowchart TB
  subgraph model-fitting-process
   style model-fitting-process fill:#e3f2fd,stroke:#1e88e5,stroke-width:2px
    direction LR
    D[data]
    M[model]
    P[purpose]
  end
  M --> F(fit)
  
  M <--> D
  D <--> P
  M <--> P
  F --> P
```

#### model formula

A model formula in R takes a

#### model selection

A [model selection] is a procedure to select a [final model] given the [data] and a set of different [candidate model]s. The [model selection] in a GAMLSS framework has two meanings: i) how to choose the [distribution] of the response and ii) how to find which explanatory variables and how affects the [distribution] of the response. In a GAMLSS model the distribution is affected through the [distributional parameters].

#### model interpretation

The [model interpretation] is the state, of a [statistical modelling] process in which after an [adequate model] is found, we are trying to answer the [questions] of the study using the [fitted model].

#### model prediction

The [model prediction] is the state, of a [statistical modelling] process in which after an [adequate model] is found, the model is checked on [out of bag], new data. Thet is, [data] which have been not used for the [training] of the model.

#### multicollinearity

#### multivariate distribution

A [distribution] function [pdf] for more that one [random] variable.

################################################################################ 

## N

#### $n$ {#n-1}

$n$ is number of observations in a [tabular data] set

#### natural variability

#### normalised randomosed quantile ressiduals

#### neural network

Neural networks were not invented by a single person; they developed over time through contributions from several key researchers. The origins are usually traced to the following milestones:

-   Warren McCulloch (neuroscientist) and Walter Pitts (logician) proposed the first mathematical model of a neuron. Paper: “A Logical Calculus of the Ideas Immanent in Nervous Activity”Introduced binary neurons with thresholds. Laid the theoretical foundation of neural networks

-   Frank Rosenblatt (1957–1958 Frank Rosenblatt invented the Perceptron, the first learning neural network. Could learn weights from data , Implemented both in theory and hardware Major step toward modern machine learning Commonly credited with inventing the first trainable neural network

-   Minsky & Papert (1969). Marvin Minsky and Seymour Papert showed theoretical limitations of single-layer perceptrons. Temporarily slowed neural network research Their work motivated multi-layer networks

-   Backpropagation (1980s). Modern neural networks became practical after the rediscovery of backpropagation: Paul Werbos (1974): first formulation, Rumelhart, Hinton & Williams (1986): popularised it. Backpropagation made training multi-layer networks feasible.

-   1943 Artificial neuron model McCulloch & Pitts

-   1958 Learning neural network Rosenblatt 974–86

-   Backpropagation

-   Werbos; Rumelhart, Hinton, Williams 2010s ?? Deep learning boom Many contributors

#### normalized quantile residuals

The [normalized quantile residuals] or [z-scores] are the residuals of a [distributional  regression] [model]. Residuals in a DR model are defined differently from the classical regression models. In classical models the residuals are defined as the difference between the response and its estimated mean value $(y_i-E(y_i)$. [Residuals] in a [distributional regression] model are the [normalised quantile residuals], (NQR) @DunnSmyth96 or [z-scores]. Two transformations are needed to get the z-scores from a fitted distributional model. Let $y_i$ and $F(y_i, \hat{\boldsymbol {\theta}}_i)$ be the $ith$ observation of the response and its fitted cumulative distribution function [cdf], respectively. Then first transformation is to get the probability integral transformed (**PIT**) residuals which are defined as, $$\hat{u}_i = F(y_i, \hat{\boldsymbol {\theta}}_i).$$ {#eq-PITresid} If the distribution of $y_i$ is specified "correctly" then the PIT residuals are expected to behave as identical and independently distributed (i.i.d) random variable from a uniform distribution i.e $u_i \sim U(0,1)$. The problem is, that is rather hard to check deviation from the uniform distribution, so we take the PIT residuals and transform them to z-scores using $$\hat{z}_i = \Phi^{-1}(\hat{u}_i).$$ If the assumed distribution of the response is approximate "correct" the z-scores are i.i.d. standardised normally distributed random variables i.e. $z_i \sim N(0,1).$

The two transformations needed to create the z-score residuals are shown in @fig-resid_types. In @fig-resid_types (a) the response is transform to a PIT using the fitted cdf function $F(y_i, \hat{\boldsymbol {\theta}}_i)$ while In @fig-resid_types (b) the PIT are transformed to z-score using the inverse cdf of the normal distribution $\Phi^{-1}(0,1)$ (otherwise known as the q-function of the normal distribution). While the above residuals works perfectly for continuous distributions are not ideal for discrete counts especially if the range of the response is limited to small range i.e. $0,1,\ldots,10$. A randomisation of the PIT residuals between the values $y_i$ and $y_i+1$ usually solves the problem see for example Chapter 10 of @Stasinopoulosetal2017.

```{r}
#| echo: false 
#| warning: false
#| message: false
#| fig-cap: "The two transformation needed to get the residuals"
#| fig-subcap: 
#|     - "The PIT transormation (from cdf to uniform)"
#|     - "The z-scores transormation (from iniform to normal)"
#| layout-ncol: 2
#| label: fig-resid_types 
library(gamlss.ggplots)
library(ggplot2)
library(gamlss)
library(gamlss2)
op <- par(mfrow=c(1,2), mar=par("mar")+c(0,1,0,0) )
da <- dbbmi[db$age>10&db$age<20,]
m6 <- gamlss(bmi~age, data=da, trace=FALSE, family=BCTo)
y100 <- da[100,]$bmi
u100 <- pBCTo(y100, mu=fitted(m6, "mu")[100], sigma=fitted(m6, "sigma")[100], nu=fitted(m6, "nu")[100], tau=fitted(m6, "tau")[100])  
fitted_cdf_data(m6, 100, from=10, to=30, title="")+
  ylab("PIT")+
 ggplot2::geom_vline(xintercept = y100, colour="pink")+  
 ggplot2::geom_hline(yintercept = u100, colour="pink")+
  geom_text(x=y100+0.15, y=.01, label="Y")+
  geom_text(x=10-0.15, y=u100, label="U")+
  geom_segment(aes(x=y100, y=0, xend=y100, yend=u100), arrow = arrow(length=unit(0.4, 'cm')))+
  geom_segment(aes(x=y100, y=u100, xend=10, yend=u100), arrow = arrow(length=unit(0.4, 'cm')))+
   theme_bw(base_size = 20)
###################################
z100 <- qNO(u100)
p9 <- ggplot(data.frame(u = c(0, 1)), aes(x = u)) +
        stat_function(fun = qnorm, lwd=1.5)+
        ylab("z-score")+
   ggplot2::geom_vline(xintercept = u100, colour="pink")+
   ggplot2::geom_hline(yintercept = z100, colour="pink")+
   geom_text(x=u100+0.08, y=-2.3, label="U")+
   geom_text(x=-0.01, y=u100, label="Z")+
   geom_segment(aes(x=u100, y=-2.3, xend=u100, yend=z100), arrow = arrow(length=unit(0.4, 'cm')))+
  geom_segment(aes(x=u100, y=z100, xend=0, yend=z100), arrow = arrow(length=unit(0.4, 'cm')))+
   theme_bw(base_size = 20)
p9
par(op)
```

In conclusion, if the assumption of the distribution for the response is adequate for the given data then we expect $$\hat{z}_i=\Phi^{-1}\left(F_o(y_i,\hat{\boldsymbol {\theta}}_i)\right)$$ {#eq-z-scores} for $i=1,2,\ldots,n$ to behave like a i.i.d normally distributed variables, that is a [white noise].



#### null model

As null model we refer to a model with no \[terms\] in it. In R notation this is the model with a [model formula] equal to `~1`. For distributional regression a null model is model which has [model formula] `~1` for all the [distribution parameters] models.

################################################################################ 

## O

#### observations

We usually refer to the rows of a [tabular data] as the [observations]. Note that the number of obsrvations in the [data] is denoted as [$n$](#n-1).

#### ordinary differential equations

#### Occam’s principle {#occams-principle}

The [Occam’s principle](#occams-principle) from the Latin "Entia non sunt multiplicanda praeter necessitatem" (“Entities should not be multiplied beyond necessity.”) it says that "among competing explanations, the one with the fewest assumptions should be preferred". The principle does not say the simplest explanation is always true but a simpler models is preferable if explain the data equally well and complexity should not be introduce unnecessary. In [statistical modelling] for [mathematical model] it means [model] with fewer parameters are preferable if they explain the model well or more generally if two models fit the data similarly, choose the one with fewer predictors. Note that if models fit the data similarly they belong the [Rashomon set] of relevant models.

#### over fitting

Over fitting happens when the [fit] is too close to the [data] and therefore do not generalised well when try to predict, see also [over fit].

#### over fit

By [over fit] we mean situations where the model is too close to the actual [data] but not close to the [population]. An \[overfitted model\] is not good for predilection purpose, see also [over fitting].

#### out of bag

By [out of bag] [data] we refer to [data] that they have not be used to [fit] the [model]. That is, [test] or \[validation\] [data] sets.

################################################################################ 

## P

#### pacf

The term [pacf] stands for partial [autocorrelation] function. The [acf] and [pacf] are \[diagnostic\] tools for detecting [autocorrelation] in a [vector], $\text{x}$.

```{r}
#| warning: false
# library(gamlss.ggplots)
# y_pacf(resid(m1))
```

#### pairwise relationship

By [pairwise relationship] we refer to the relationship between two [vector]'s in the [data]/[sample]. The investigation of [pairwise relationship]s between the explanatory variables could lead to the identification of possible problens in the [fit] of [interpretation] of a [model]. The identification of [pairwise relationship]s between the [response] variables could lead to better construction of a [multivariate distribution] via [vine copula].

#### parsimony

The principal of parsimony is what is also known as [Occam’s principle](#occams-principle) state that

#### partial effects

#### partial differential equations

#### parameters

We refer to [parameters] as an generic term for all the unknown quantities within a [model]. The model [fit] estimates those [parameters]. The [parameters] of a \[distribution regression\] framework are of three types: the [distribution parameters] which are modelled as functions of the [explanatory variables] i.e. $\boldsymbol{\theta}_k = g(\textbf{X}_k)$, the [coefficients] describing the relationship between the \[distribution parameter\]s and the [explanatory variables]. and the [hyper parameter]s which are parameters which tune the mdel to the direction of the [data].

#### Poisson distribution

#### population

The population is often the object we would like to study. The population is related to the [purpose] of the study. The idea behind is that we collect a [sample] ([data]) from the [population] of interest and use a [model] to drew [statistical inference] about the [population]. The process of [statistical inference] it is shown in @fig-population. We need information about the [population] but we only have a [sample]/[data] form the population which is assumed to be obtained after a \[data generation mechanism\]. We make [assumptions] to create [model] which is then use to say something about the [population]. Going from the model to the population is called [statistical inference]. Note that any [statistical inference] is heavily depends on the [model] which itself depends on the [assumptions] and the \[data generation mechanism\]. If anything go wrong with the [assumptions] \[statiscal inference\] can be wrong.

```{mermaid}
%%| fig-cap: "Statistical inference: how to go from the sample to the population."
%%| fig-width: 5
%%| label: fig-population
flowchart TB
  P{population} 
  D(data generation mechanism)  
  S{sample-data} 
  M[model] 
  I(statistical inference)
  A(assumptions)
  P --> D
  D --> S
  S --> M
  M --> I
  I --> P
  A --> M
  D --> A
```

#### principle component regression

#### properties of the distribution

Please see [characteristics of the distribution]

#### properties of the distribution parameters

The [distribution parameters] have different properties by how they are defined within a [distribution]. Any [distribution] can be re-parametrised differently as far as the re-parametrisation end up with the some number of parameters as the original distribution. For [distributional regression] the specific parametrization do matter in order to help the [interpretation] of the [model]. Some parametrisations are more useful in practice that others. For example in a [distributional regression] it worth to have a parameter dealing with, where the centre of the [distribution] is, and whether this centre changes with [explanatory variables]. Those parameters are called [location parameter]s. Other parameters could describe how far from the centre each observations could fall. Those are called the [scale parameter]s. To describe asymmetry in the [distribution] we have the [skewness parameter]s. To distinguish [distribution]s with fat [tail]s we have the [kurtosis parameter]s. Notice that any [distribution] could have more that one parametrisation and that the specific parametrization could affect different [characteristics of the distribution].

#### power transformation

By [power transformation] in [distributional regression] framwork we mean the [transformation] of one of the $\textbf{x}$ the \[explanatory vatiables\] to a more suitable form to a [feature] to make it suitable for [regression analysis]. The [power transformation] is defined as $x_p=\textbf{x}^p$ so the variable $\textbf{x}_p$ is entering the formula rather than $\textbf{x}$. The parameters $p$ takes values typically in the [range] $0 \le p \le 1.5$. The transformations tries to make the values of the $\textbf{x}$ more symmetric and more even spaced.

#### purpose

The purpose of the study is the [hypothesis] the study is working on.

#### pdf

#### penalised least squares

#### PIT residuals

The [PIT residuals] stands for the probability integral transformed residuals. Within a [distributional regression] framework are defined as, $$\hat{u}_i = F(y_i| \hat{\boldsymbol {\theta}}_i).$$ {#eq-PITresid} for $i=1\ldots,n,$ where $F(.,| \hat{\boldsymbol {\theta}}_i)$ is the [cdf] of the fitted [model]. If the [distribution] of $y_i$ is specified "correctly" then the [PIT residuals] are expected to behave as identical and independently distributed (i.i.d) random variable from a [uniform distribution] i.e $u_i \sim U(0,1)$. While the above definion of IT residuals works perfectly for [continuous distribution]s are not ideal for [discrete distribution]s especially if the [range] of the response is limited to small [range] i.e. $0,1,\ldots,10$, For [mixed distribution]s where there is small amount of discrete values or for [censored response]s. A randomisation of the PIT residuals between the values $y_i$ and $y_i+1$ usually solves the problem see for example Chapter 10 of @Stasinopoulosetal2017.

#### prediction

By [prediction], in a standard [regression analysis], we mean using the [model fitting] to predict the behaviour of the [response]. Note, that in a [distributional regression] [model] there are two uncentainties in associated with predicting any future value of $Y$.; i) uncertainty about the assumed [distribution] of $Y$; ii) uncertainty about the [model fitting].

#### prediction interval

A [prediction interval] inteval

#### probability

################################################################################ 

## Q

#### QQ-plot

#### quantile

#### quantile function

#### quantile residuals

The [quantile residuals]  also called [PIT residuals].



## R

#### $r$

$r$ is the number of [explanatory variables] or [input variables] in the data

#### random

The word [random] is used as equivalent to [stochastic].

#### random variable

A random variable $Y$ is a variable which has a distribution. We denote this as $Y \sim f( Y| \boldsymbol{\theta})$ where the symbol $\sim$ reads as "is distributed" and where $f()$ denote a probability [distribution] function [pdf] and the $| \boldsymbol{\theta}$ notation emphasise that usefull theoretical distributions depent on unknown [vector] of [distribution parameters] $\boldsymbol{\theta}$. The more the parameters in $\boldsymbol{\theta}$ the more flexible the [distribution] is. Another important feature of a random variable is the specification of its [range] $\mathbb{R}(Y)$.

#### range

This this refer to the [range] of possible values that a [random variable] takes. We use the notation $\mathbb{R}(Y)$ to describe the range of a random variable $Y$. There are two types of possible ranges in the real line i) continuous and ii) discrete range.

#### Rashomon

The term [Rashomon] originates from Akira Kurosawa’s 1950's film **Rashomon**, where a single incident, a murder and an assault, is recounted by four witnesses, the bandit, the samurai’s wife, the samurai via a medium, and a woodcutter, each giving a vastly different version of what occurred. The film ends without resolving which account, if any, is accurate.

![](rashomon.png){width="100"}

#### Rashomon effect

The idea that different models could possible provide different interpretations of the same data set.

#### Rashomon set

Is the set of [regression] [model]s with similar [measure of goodness of fit] therfore it is difficult to distinguish between them. [Rashomon set]s are real, practical statistician encounter them all the time. Note that [Rashomon set]s are comon to over-parametrized models

#### regression

A [regression] [model] is an [stochastic] [input-output model] where the [explanatory variables], $X$, affect the [response] $Y$.

#### regression analysis

A [regression analysis] is an statistical [input-output model] analysis, analysing [tabular data]. Typically regression analysis is refer to [linear model]s but here we use the term to describe any [input-output model] relationship.

#### regression tree

#### reference curves

#### residuals

The residuals is a [vector] of length $n$ with each element measuring the difference between observed and fitted values. For a [linear model] the residuals are defined as $y_i-\hat{y}_i$ for $i= 1,\ldots,n$. where $\hat{y}_i$ is the fitted values of observation $i$. The $n$-observations have $n$ residuals. Residuals can be defined in both the [training] or the [test] data sets. Residuals from the training data can be used to check \[underfitting\] while residuals from the test data can be use for checking \[overfitting\] the data.

Two transformations are needed to get the z-scores from a fitted distributional model. Let $y_i$ and $F_o(y_i, \hat{\boldsymbol {\theta}}_i)$ be the $ith$ observation of the response and its fitted \[cumulative distribution\] function (cdf), respectively. Then first transformation is to get the probability integral transformed (**PIT**) residuals which are defined as, $$\hat{u}_i = F(y_i, \hat{\boldsymbol {\theta}}_i).$$ If the [distribution] of $y_i$ is specified "correctly" then the PIT residuals are expected to behave as identical and independently distributed (i.i.d) random variable from a uniform [distribution] i.e $u_i \sim U(0,1)$. The problem is, that is rather hard to check deviation from the uniform distribution, so we take the PIT residuals and transform them to z-scores using $$\hat{z}_i = \Phi^{-1}(\hat{u}_i).$$ If the assumed [distribution] of the response is approximate "correct" the z-scores are i.i.d. standardised normally distributed random variables i.e. $z_i \sim N(0,1).$

The two transformations needed to create the z-score residuals are shown in @fig-resid_types. In @fig-resid_types (a) the response is transform to a PIT using the fitted cdf function $F(y_i, \hat{\boldsymbol {\theta}}_i)$ while In @fig-resid_types (b) the PIT are transformed to z-score using the inverse cdf of the normal [distribution] $\Phi^{-1}(0,1)$ (otherwise known as the q-function of the normal distribution). While the above residuals works perfectly for continuous distributions are not ideal for discrete counts especially if the range of the response is limited to small range i.e. $0,1,\ldots,10$. A randomisation of the PIT residuals between the values $y_i$ and $y_i+1$ usually solves the problem see for example Chapter 10 of @Stasinopoulosetal2017.

#### response

The response variables is the output variable in a [input-output model].

#### RMSE

The term stands for square Root of Mean Square Error. Its defined as $\text{MSE} = \sqrt{\frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2}$ and its a [measure of goodness of fit] for [regression analysis] [model]s. It can be calculated in both [training] or [test] data sets for a [measure of goodness of fit] or measure of [prediction] power, respectively. RMSE is **NOT** appropriate for [distributional regression] models unless the [purpose] of the study is on the centre of he assumed distributions.

#### risk

A [risk] function measure the accuracy of a [model] in terms for a specific [measure of goodness of fit]. The [risk] function is a function of both the [data] and the [model fitting] process. For \[regression model\]s where only the mean of the response is modelled as a function of explanatory variables , for example, $Y=g(X)+\epsilon$, it measure how accurate the function $g()$ is. The mathematical definition of a [risk] function is given as the average of a [loss] function. Let us define the [loss] function as the discrepancy measure between the [data] and the [model], $\ell oss()$. The expected value of a loss function $\ell oss$() is defined as the [risk] function. $$\mathbb{Risk}(g) = \mathbb{E}_{X,Y} [ \ell oss(\hat{g}(X), Y) ]$$ where $\hat{g}()$ is an estimate of the model unknown function $g()$, $X$, $Y$ are the pairs, of an \[input output model\] variables. The [risk] measures the average discrepancy between the assumed [model] and the [data].\
The problem with the above definition of the [risk] function is that involve taking expectations with respect to the input and output variables. Even if conditioning on the $X$'s we have to take expectation with repsect to $Y$. We can avoid the problem all together if we replace the [risk] function with the [empirical risk] which gives equal probability to each row of the [data] matrix. For example the [empirical risk] is defined as $$\mathbb{ER}(g) = \frac{1}{n}\sum_{i=1}^n [ \ell oss(\hat{g}(x_i), y_i)$$ Note that if the [loss] function is defined as minus the log [likelihood] (or the [deviance]) minimising the [empirical risk] is equivalent to maximising the [likelihood] function with rescect to the parameters of the model. Note that $\frac{1}{n}$ is just a constant not involved in the minimazation.

################################################################################ 

## S

#### sample

In statistics a [sample] has the same meaning as [data]. It called a [sample] because it is assumed to come from a bigger data set the [population] of interest.

#### saturated model

A saturated model is a model which has as many unkown parameters as the number of observations $n$

#### SBC

The SBC is another name for [BIC]. It stand for the Schwarz Bayesian Criterion for comparing fitted models.

#### scale parameter

#### Sklar's theorem

The Sklar's theorem , (@sklar1959fonctions) states that every multivariate [cdf] $F(x_1,\dots, x_r)$ with marginals [cdf]s $F1(x1)$, $\ldots$, $F_r(x_r)$ can be written as $$F(x_1,\ldots,x_r) = C_r[F_1(x_1),\ldots,F_r(x_r)]$$ for some appropriate r-dimensional copula $C_r$.

#### selection of terms

A selection of terms in distributional regression is the process of identify which [term] affect which parameter.

#### skewness

Skewness is a statistical measure indicating whether a sample [vector] is symmetric or not. Skewness is also applied to random vectors and therefore to theoretical distributions.

#### skewness parameter

#### simulation model

A [simulation model] is a [model] in which is computer simulated several times usually in thousand, in order to determine the future behaviour of the [subjects] of interest according the the [variables] in the model.

#### supervised learning model

A [supervised learning model] is a regression model in which a [response] exist.

#### statistic

The term [statistic] refers to a function of the [data]/[sample]. For example, the mean of the vector $\textbf{x}$ in the [data] defined as $\bar{x}= \frac{1}{n}\sum_{i=1}^{n} x_i$ is a [statistic], since it is a function of the [data]. A [statistic] captures properties of the [sample] the same way that [moment]s and \[centile based measures\] caption properties of a theoretical distribution. In the past, it was common practice, to equate [statistic]s with their equivalent [distribution] characteristic to draw inference from the [population]. The practice almost abandoned after the introduction of [MLE].

#### centile based measure

A [centile based measure] is equivalent to a \[quantile based measure\]. It describes a [characteristics of the distribution] based on [quantile]/[centile] rather [moment].

For example $\bar{x}$ can be used as estimate of the mean $\mathbb{E}(X)$ of the theoretical [distribution] assumed for $x$ i.e. $\mathbb{E}(X)= \int_{-\infty}^{\infty} X f(X) dx$, where $f(X)$ is the assumes [distribution] for the [vector] $x$.

#### statistical inference

Is the process of drawing inference from the [sample] to the [population] see @fig-population for more explanation. The path from to the specific ([sample]) to the general ([population]) can only be achieved using [assumptions] and therefore is very treacherous and irrelevant if the assumptions are not correct.

#### statistical model

A statistical model is a [mathematical model] containing a [stochastic] component. That is a stachatic model is build with [probability] statements on its [assumptions].

#### statistical modelling

Statistical modelling is the art of creating a [statistical model] to represents the [data] adequately. A [statistical model] extract information from the data to answer the [questions] of interest. The [questions] are the [purpose] of the study. Statistical modelling is a process. @fig-model shows how the [statistical modelling] process works: the [data], the [model] and the [questions] to be answered from the [data] are all inter-connected. Typicaly the data are collected to answer the questions and the data do that. Any [model] needs [assumptions] to be created and also needs a [measure of goodness of fit] as a way to evaluate its performance. The measure of [goodness of fit] is measuring the distance between the [data] and the [model] and therefore depend on both the [data] and the fitted [model]. Given the [fit] is an [adequate fit], the [interpretation] of the model helps to answer the [questions]. @fig-model describe the structure of [statistical modelling] process. Finding an [adequate model] is the essence of statistical modelling.

```{mermaid}
%%| fig-width: 5
%%| label: fig-model
%%| fig-cap: "The modelling process: the data, the model and the questions are inter-related. The model needs assumptions before the fit and an adequate fit help to answer the questions."
flowchart TB
  subgraph stats-modelling-process
   style stats-modelling-process fill:#e3f2fd,stroke:#1e88e5,stroke-width:2px
    direction LR
    D[data]
    M[model]
    Q[questions]
  end
  A(assumptions) --> M
  D <--> M
  Q <--> M
  M --> F(adequate fit)
  D <--> Q
  A <--> F
  G(goodness of fit) --> F
  M --> G
  D --> G
  F --> I(interpretation)
  I --> Q 
```

The question is how do we know that we reach an [adequate fit]?. By [adequate fit] we mean a model close to the [data] and be able to answer the [questions]. How close to the data a model is can be checked by the performance of the [goodness of fit] measure and by [diagnostics] tools. However the [goodness of fit] measure is often a relative measures in the sense that is useful for comparing between models but not to show if any of then is [adequate fit]. This task mainly lies with [diagnostics] tools. More specificity \[diagnostic\] tools based on [residuals]. The reason is because in most \[stochastic models\], if the the [assumptions] are correct or nearly correct, we expect the [residuals] to behave as a [white noise]. Diagnostic plots demonstrating [white noise] [residuals] are a good sign that an [adequate fit] has been achieved. In conclusion a [adequate fit] can be verified using [residuals] [diagnostics] on the [training] data set in compilation with a good performance in a [goodness of fit] measure.

There are two major statistical problems which could arising with [adequate model]s. The first is that for a given [data] set they could more than one [adequate model]s. The second problem has to do with [over fitting].

The set of all [adequate model]s sometime is called the [Rashomon set]. This is a set of different fitted [model]s all of them [adequate model]s, believable, and with no good argument to discard them see @fig-rashomon.

```{mermaid}
%%| fig-width: 5
%%| label: fig-rashomon
%%| fig-cap: "Showing the process of creating a Rashomon set of models"
flowchart TB
    direction TB
    R(Rashomon set)
    D[diagnostics]
    G(goodness of fit)
    M[models]
    A[assumptions] 
  A --> M
  M --> D
  M --> G
  G  --> R
  D --> R
```

Since [statistical modelling] is a process there are a lot occasions in it can go wrong. Mostly things can go wrong when the wrong [assumptions] are made about the [model] explicitly or implicitly. would like to investigate this further so we look the process in more details. Explicitly or implicitly we assume that there exist a [population] of interest and there is [data generating mechanism] which creates the observed [data]/[sample] as in @fig-mod_1.

```{mermaid}
%%| fig-width: 5
%%| label: fig-mod_1
%%| fig-cap: "The relationship of the population and the data/sample."
flowchart LR
  P[population] --> G(data generating mechanism) 
  G --> D[data]

```

@fig-mod_2 shows the [statistical model] as a filter which extract [information] from the [data]. This either because we would like to go back to the [population], \[a process known as [statistical inference]\] or because we would like to gain knowledge from the [information] in the data, a process kmown as [data analysis]. The [information] is needed to answer the [questions] posed by the researcher which are essential the [purpose] of the study.

```{mermaid}
%%| fig-width: 5
%%| label: fig-mod_2
%%| fig-cap: "Showing how a model is behaving as afilter to extract information from the data"
flowchart LR
  D[data] --> M[model] 
  M --> I(information)
```

In order to create a [model] we need to make [assumptions]. The [assumptions] should reflect the [data generating mechanism], the [data] itself but also the [purpose] of the study, see @fig-mod_3. Note that the assumtion also effect the measure of [goodness of fit] One should always ask the question on "whether the [data] or the [model] could answer the [questions]. The [assumptions] should account for the [natural variation] in the [data] but also able to capture the relationships between the variables in the [data].

```{mermaid}
%%| fig-width: 5
%%| label: fig-mod_3
%%| fig-cap: "A model need realistic assumptions to capture both the natural variable and ther interrealtioships in the model."
flowchart TB
  G(data generating mechanism)  --> D[data]
  D --> M[model] 
  A(assumptions) --> M
  G --> A
  M --> I(information)
  M --> Y[goodness of fit]
  D --> A
  P[Purpose] --> A
  I --> Q[questions]
```

The quality of the [information] to extracted in the [data] depends on the flexibility of the [model] and how realistic are the [assumptions]. Wrong [assumptions] could lead to wrong conclusions and therefore to wrong [interpretation]. This fact was recognise at an early stage of [statistical modelling] and lead to the introduction of [diagnostics] tools for checking the appropriateness of the [assumptions]. A diagnostic is a [statistic] or a [graphic] which as a function of both the [data] and the [model] could check whether the assumptions of the [model] are adequate. A lot of the graphical [diagnostics] are based on the [residuals] of the [model] which if the [assumptions] are correct are expected to behave as [vector] of [white noise]. Typical examples are [QQ-plot]s and [worm plot]s. @fig-mod_5 shows what we called the classical statistical modelling process. Note that there is a loop going around starting from the [assumptions], the [model], the [fit], the [diagnostics] and back into the [assumptions]. That is, assumptions are made the model is fitted and if the diagnostics indicate an inadequate [fit] different assumption are made and the process in the loop continues until an [adequate model] is reach and the modelling process is finished. Our impression is that this process of [statistical modelling] was mentioned first by @BoxJenkins70 in their famous book on [time series].

```{mermaid}
%%| fig-width: 5
%%| label: fig-mod_5
%%| fig-cap: "the classical statistical modelling process introducing a loop between assumptions. model, fit and diagnostics"
flowchart TB
  D[data] --> A(assumptions) 
   A --> M(model)
   M --> Y[adequate model]
   M --> W(fit)
   W --> V(diagnostics)
   V --> A
```

In a modern statistical modelling approach the data are partitioned to help detection of [over fitting] and prediction. @fig-modelling_modern show this new approach of statistical modelling. Instead of a single fitted model $B$ [candidate model]s are [fit]ted and subsequently aggregated (see [stacking]) before [model interpretation] and [model prediction].

```{mermaid}
%%| fig-width: 5
%%| label: fig-modelling_modern
%%| fig-cap: "The modern modelling process uses more models and aggregates them"
flowchart LR
  A[part. Data] --> B[Assumptions] 
  B --> C{fit K-Models}
   C -->|check| B
  C --> D[stacking]
  D --> E[interpretation & prediction]
```

The achieve this more modern approach to statistical modelling the following technique could be used: [boosting] where a lot of week models are fitted to a single data set in such a way that the averaged models supplement each other. This is done by taking into the account the [residuals] of the previously [model fitting]; [bagging] when $B$ models are fitted to $B$ bootstrap [data partition]s and the models are averaged; [stacking] when when $B$ different [model]s are fitted to a single data set (no partition) and the models are averaged.

#### fitted model

A [fitted model] is a model in which its parameters are estimaed using the data. Seee also [model fitting]

#### natural variation

The [natural variation] refers to the variability in the [data]. The idea behind is that even if we were able to collect data more that ones from the same [population], under similar conditions, the data will be different reflecting the [natural variation] of the data. This makes necessary to use a [stochastic model]. In [regression analysis] the [natural variation] is reflected in the variation of the [response] variable. Even if we be able to collect data at the same values of the explanatory variables the response will be different.

#### statistical model

A [statistical model] is a [stochastic model]. that is a [mathematical model] containing probabilistic statements to account for the [natural variation] of the data.

#### stacking

The idea of [stacking] is to [fit] several [model]s to single [data] set but instead of performing a [model selection] to proceed by averaging the resulting models. The problem with [stacking] is that very rare the procedure leads to good [model interpretation].

#### scope

The [scope] of a [step wise selection] procedure specify the range of all possible [candidate model]s to be checked. There is an order in the scope in that [main effect]s go before [first order interaction]s and before the higher order interactions. The scope can be specified by declaring the [lower model] and the [upper model]. Note that the [lower model] sould be a sub set of the [upper model].

#### scope of step wise selection

see [scope].

#### step wise

See [step wise selection]

#### step wise selection

A [step wise selection] procedure is a way of selecting [term]s in a [regression analysis]. The procedure to work needs a [scope] which specify the range of all possible [candidate model]s to be checked. Usually there is an order in the [scope], The [main effect]s go before [first order interaction]s and before the higher order interactions. The scope should contain a [lower model] and and [upper model]. The procedure also needs two competitive models; the [current model] and the [candidate model]. Those two models are compared using an appropriate measure of [goodness of fit] and the "best" model is them selected as the new [current model] this is shown in @fig-step_wise.

```{mermaid}
%%| fig-width: 5
%%| label: fig-step_wise
%%| fig-cap: "The step-wise approache for selecting a term"
flowchart TB
  S[starting model] --> C(current model)
    C ---> M(fit candidate model)
    C --> Y[final model]
    M --> G(compare using goodness of fit)
   G --> C
```

The [starting model] becomes the [current model] as soon the procedure is starting. The procedure can go **forwards** (that is, after setting the [starting model] to the [current model]) trying to fit only more complicated models in the scope see @fig-step_wise_forwards;

```{mermaid}
%%| fig-width: 5
%%| label: fig-step_wise_forwards
%%| fig-cap: "The forward selection of terms "
flowchart LR
  S[lower model] -.- C(current model)
  C ---> M(upper model)
```

The procedure can go **backwards**, that is, trying to fit only simpler models in the scope see @fig-step_wise_backwards;

```{mermaid}
%%| fig-width: 5
%%| label: fig-step_wise_backwards
%%| fig-cap: "The backwards selection of terms "
flowchart RL
  S[upper model] -.- C(current model)
  C --> M(lower model)
```

Or the procedure can got **both** ways, that is, choosing the best model either way from the [current model].

```{mermaid}
%%| fig-width: 5
%%| label: fig-step_wise_both
%%| fig-cap: "The selection of terms going  in both directions."
flowchart LR
  S[upper model] <--> C(current model)
  C <--> M(lower model)
```

#### starting model

A fitted [model] to start the [step wise] selection procedure for selecting terms in a model. The first step of the procedure is to make the [starting model] the [current model]. Often the [starting model] is the [null model].

#### stochastic

A [random] variable is [stochastic] if it has a probability distribution [pdf] associated with it. The term [stochastic] is equivalent with the term [random]

#### stochastic model

A [stochastic model] is a mathematical or an algorithmic model which incorporates randomness, interms of probabilistic statments. The output of a [stochastic model]is not completely predictable. Not all models need a stochastic component. A [mathematical model] or an [algorithmic model] donot have to be a stochastic model. A stochastic model contains [probability] assumtpions about one or more of his components.

#### stochastic regression

A stochastic regression models contain [probability] assumptions on how the input-output model is generated. A typical assumption is the response is a function of the explanatory variables plus an [error], for example, $\textbf{y} = g(\textbf{x}) + \boldsymbol{\epsilon}$ where The minimal assumption for a regression model is about the behaviour of the \`response. Not all problems need a stochastic component. Stochastic models are often used because many natural, social, and physical systems have an inherent variability.

#### study

In a [data analysis], a [study] is an investigation in which we are trying to understand the behaviour of the [subjects] at different values of the [variables]. Studies have a [purpose] consisting of the [questions] an investigator would like to answer.

#### smooth function

A [smooth function] $s()$ is a term in a [model] design to model non-linear relationships. In a [regression] model those relationships are between the [explanatory variables] and the [response], in a \[distributional regression model\] is between the explanatory variables and a \[distribution parameter\]. To simplify matters, consider the simple [regression] situation when we have only one explanatory variable, $\textbf{x}$ and one response $\textbf{y}$. If we suspect that the relationship is not linear we couls try a \[smooth\] **non-parametric** [model] for the response: $\textbf{y}= s(\textbf{x})+\boldsymbol{\epsilon}$. The function $s()$ is a general notation for a smoother. that is, a smooth function determined by tha [data]. It turns out that the **non-parametric** part is rather misleading because most of the smooth techniques are following this simple linear model $\textbf{y} = \textbf{B}  \boldsymbol{\gamma} + \boldsymbol{\epsilon}$ where $\textbf{B}$ is a basis matrix constructed by the values of the explanatory vector $\textbf{x}$. The estimation of the linear parameter $\boldsymbol{\gamma}$ (the length of which could be as big as the length of the data) is done using a [penalised least squares] method with solution $\hat{\boldsymbol{\gamma}}=(\textbf{B}^{\top} \textbf{B}+\lambda \textbf{I} )^{-1} \textbf{B}\textbf{y}$. Note that if more that one smooth functions exist in model [formula] then the estimation of the smooth functions is achieved using a [backfitting] algorithm.

#### smooth model

A smooth model is a model containing smooth functions. For example the simpler smooth model is $\textbf{y}= s(\textbf{x})+\boldsymbol{\epsilon}$.

#### smoother

A smoother is a \[smooth\] non-parametric function design to model non-linear relationships see [smooth function].

#### stack of models

A \[stack of model\] is set of [fitted model]s usually waiting for evaluation for their suitability to answer the [questions] of the study.

#### subjects

A subject is the basic unit of interest in a [study].

#### sufficient statistic

A [sufficient statistic] was a fundamental concept in statiscal inference in that tells you when a [statistic] captures all the information in the [data] about the [parameters]. It was a usefull concepts before computers but its usefulness is diminishing over yeaar since in most real lile situations [sufficient statistic] are not provided. The [exponential family] of [distribution] is the only family allow \[sufficient statistics\]ans this only for the mean, $\mu$.

#### summary statistic

A [summary statistic] is another wold for [statistic]

#### survival function

If $F(.)$ is the [cdf] of the random variable $Y$ its [survival function] is defined as $S(Y)=1-F(Y)$ see also [exceedance probability].

```{r}
library(gamlss.dist)
S <- function(q, ...) {1-pBCTo(q,...)}
curve(S, from=0.001, to=10)
```

Or using `ggplot2`;

```{r}
 library(ggplot2)
 ggplot(data.frame(x = c(0, 10)), aes(x)) +
   stat_function(fun = S) +
   labs(x = "x", y = "S(x)")
```

################################################################################ 

## T

#### target

The [target] variables is the [response] variable in a [regression analysis]

#### time series

A [time series] [data] set is a data set when observations were obtained over time and therefore that is a high probability that they are not independently distributed but there is autocorrelation between sequential observations. A [time series] analysis is a methodology to deal with a \[time seriess\] [data].

#### Tobit model

#### top down model

A [top down model] in data analysis is model in which the [variables] influence all the [subjects] of the model globally. That is, there exist an influence at a **macro** level and this influence is the same for all subject. A regression analysis model is a [top down model]. Note that different behaviour in a cluster of the subjects could be modelled if the cluster is part of the model. For example if there is a male, female difference in behaviour then the factor gender should be part of the [model].

#### training data

The training data are the data used in the [fit] stage of a modelling process. The contrast with the [test data] and the [validation data] sets.

#### test data

The test data are the data used to check the prediction power of the model.

#### t distribution

see Student-t

#### t copula

A [t copula] is based on the multivariate Student-t and allows symmetric log tails dependence and therefore is more robust than the \[normal copula\]

#### tabular data

Tabular data are spreadsheet type of data sets. Tabular data are rectangular in shape with variables vertically and observations horizontally. We refer to the number of observations as $n$ and the number of variables as $r$. Over recent years we have seen an increasing in the size of data sets for both $n$ and $r$. Traditionally classical regression models supported the cases where $n\gg r$, where $\gg$ refers to “$n$ is much greater than $r$”. More recent regression techniques i.e [LASSO] ans \[PCR\] support situation where $n  \simeq r$, or even $n<r$, Note that the notation $\simeq$ refers to “$n$ is nearly equal to $r$". A typical tabular data example is shown in @tbl-TheTableofData.

##### Tabular data example {.smaller}

| obs number | y      | x~1~    | x~2~    | x~3~    | ... | x~r-1~    | x~r~    |
|------------|--------|---------|---------|---------|-----|-----------|---------|
| 1          | y~1~   | x~11~   | x~12~   | x~13~   | ... | x~1r-1~   | x~1r~   |
| 2          | y~2~   | x~21~   | x~22~   | x~23~   | ... | x~2r-1~   | x~2r~   |
| 3          | y~3~   | x~31~   | x~32~   | x~33~   | ... | x~3r-1~   | x~3r~   |
| ...        | ...    | ...     | ...     | ...     | ... | ...       | ...     |
| n-1        | y~n-1~ | x~n-11~ | x~n-12~ | x~n-12~ | ... | x~n-1r-1~ | x~n-1r~ |
| n          | y~n~   | x~n1~   | x~n2~   | x~n3~   | ... | x~nr-1~   | x~nr~   |

: A tabular data example {#tbl-TheTableofData .striped .hover}

#### tail

The word [tail] in [distributional regression] framework is refer to the [tail]s of the [distribution] of the response. The [tail] of a [distribution] is very important if the [purpose] of the study is extreme values or exceedance probabilities.

#### term

A [term] is one of the [explanatory variables] in a [model], after possible a [transformation] to make it suitable for [distributional regression] analysis. For example a [factor] is a [term]. A first order interaction is a [term].

#### test

A [test] data set is the part of the [data] keep out of from fitting the [model] in order checking the predictive power of the model using a [measure of goodness of fit].

#### training

A [training] data set is the [data] set in which the [model] is [fit]ted.

#### transformation

By [transformation] we mean usualy a [vector] transformation, that is, a transformation of one of the columns of the [data] in something that it is more suitable for [statistical modelling]. In a [distributional regression] framework we rarely transforming the [response] variable, since the variability of the response is part of its assumed distribution. For example if we beleive that the response is highly \[skew\] to the left we caassume the log-[distribution] for $Y$ i.e `LOGNO` or `logTF`. Transformations in the $X$'s sometime do help the modelling process. Typically is if would suspect non-linearity in the relatioship and we would like ot use a [smoother] to model it ideally the $X$s should be equal spaced with no outliers. A [power transformation] could help this. In [centile estimation] when we are dealling with a single $X$ variable (often `age`) a [transformation] of the $X$ maybe is necessary to achieve a good [model]. \[reference here $\ldots$ Rigby et al. 2026\]. Note that the timing for creating a new [feature] by transforming $X$ could be achieved before or during the [fit]ting a [model]. For the formal case see the function `trans()` in the package `gamlss.prepdata` for the later case see Fasiolo at al. $\ldots$.

################################################################################ 

################################################################################ 

## U

#### under fit

A [model] under fits the [data] if it is not close enough to the data. The closeness is evaluated using a [goodness of fit] measure like [GAIC]. This happens if the model is not flexible enough and therefore misses important information in the [data]. See also [under fitting]

#### uniform distribution

#### univariate\` distribution

#### under fitting

Under fitting occurs when the [fit] is very poor accoding to a [goodness of fit] measure and it does not represent the [data] properly, see also [under fit].

#### unsupervised learning

An [unsupervised learning] [model] also called a [classification model] is a model which has [explanatory variables] but not [response]. The model tries to guess what class the response belong to given the [explanatory variables].

#### upper model

The [upper model] is the most complex model in the range of all possible models to be selected in a [step wise] procedure.

################################################################################ 

################################################################################ 

## V

#### validation data

The [validation data] set is used for tuning a model. That is useful in estimating the [hyper parameter]s of a model. The [validation data] set is an [out of bag] data set.

#### variables

Variables are attributes of the [subjects] of the study. I data analysis [variables] are [vector]s in a [tabular data] matrix. Thos vectors can take a wide variety of values **continuous variable** vectors a or can take few values **categorical variable** vectors or [factor]s.

#### variable importance

A variable importance should show the important of a [term] in the [model]. That is, how much this term contributed in explaining the model. Note that for [distributional regression] a \[terms\] affect both the model of [parameters] of the [distribution] but also the overall [measure of goodness of fit]. So, for example, one should be able to ask the question, of how important say **age** is for $\sigma$ the variation the response $Y$ but also how important is overall to the model compared with other terms that may influence other part of the model rather the scale parameter. For [mathematical model] parametric [model]s the size of the estimated coefficients could provide an indication of how important a variable could be but for other model could be more difficult. An \[agnostic\] method given in the literature (I think by Breimer) is to find the prediction power of the variables by scramble the variable of interest and compared it with the preditions ontain when the variable remain unscramble.This method couls produce indexes of importance both model parameter based and overall [measure of goodness of fit]

#### vector

In [statistical modelling] we refer to a [vector] as one of the columns in a [tabular data] set. In [regression analysis] both the [response] variable(s) and the [explanatory variables] are the [vector]s of interest for often we would like to explore their [pairwise relationship]s.

#### vine copula

The basic idea of a [vine copula] is, for a given [data] set, to build a multivariate [distribution] for the [response]s based their [pairwise relationship]s..

################################################################################ 

################################################################################ 

## W

#### white noise

The term [white noise] is used to describe a pure [random] process in the sense that knowing the past values will tell tell you nothing about the future values. More precise is refers to a sequence $\{\varepsilon_t\}$ of a [random] variable with $\mathbb{E}[\varepsilon_t] = 0$ $\operatorname{Var}(\varepsilon_t) = \sigma^2 < \infty$ and $\operatorname{Cov}(\varepsilon_t, \varepsilon_{t-k}) = 0$ for all $k \neq 0$. If in addition we assumed that $\varepsilon_t \sim \mathcal{N}(0,\sigma^2)$ then we have a [Gaussian white noise]. Note that [independence] implies [white noise] but the oposite is not correct unless we have a [Gaussian white noise].

#### worm plot

A worm plot, @vanBuurenFredriks01, is a [diagnostics] tool which could be applied to any standardised [residuals] of a [regression] model but more often to [z-scores] of a [distributional regression] model. A [worm plot] is a trended [QQ-plot]. Detrended to highlight departures from normality.

```{r}
resid_wp(m2)
```

################################################################################ 

################################################################################ 

## X

#### $X$

$X$ is used in here as a generic term for [explanatory variables], [input variables] or [independent variables] in a [regression analysis] but also is used to indicate a [random] variable.

################################################################################ 

################################################################################ 

## Y

#### $Y$

$Y$ is used as a generic terms for the [response], the [target], or the [y-variable] for [regression analysis] and [distributional regression]. More specifically within a [distributional regression] framework one would like to match the [range] of the [response] [vector] in the [data] with the [range] of the assumed [distribution] $D()$ for the response. This will ensure that in any [prediction] the right boundaries are observed.

#### y-variable

We refer to the [y-variable] as the variable of interest in a [regression analysis]. Other names are the [response], the [target] or the **depended variable**.

################################################################################ 

################################################################################ 

## Z

#### z-scores

In a [distributional regression] [model] the [z-scores] are the normalised [residuals] of the [model]. If the model is an [adequate fit], we expect 95% of the [z-scores] values to fall between -2 and 2. (More precisely between -1.96 and 1.96). Therefore [z-scores] for both [training] and [test] data can be used for detection of unusual observations. The straightforward calculation of the [z-scores] is one of the great advantages of the [distributional regression] compare with \[quantile regression\], @koenker2017quantile, where the [z-scores] could be calculate but with a substantial computational cost see for example, Chapter 13 of @Stasinopoulosetal2017.

################################################################################ 

################################################################################ 
