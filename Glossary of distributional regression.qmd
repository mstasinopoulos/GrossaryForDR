---
title: "A Glossary for Distributional Regression Models"
format:
  html: default 
  pdf: default 
number-sections: false   
number-depth: 3
editor: visual
quarto: add leovan/quarto-pseudocode
author: GAMLSS working party  

bibliography: book2025.bib       
---

## Introduction 

This is a glossary of terms and ideas related  to statistical models in general but more specifically to [distributional regression] models. The current glossary started  based on ideas presented in the talk [Regression Models; how to adapt for climate change challenges](https://mstasinopoulos.github.io/Brazil2025-talk/talk_2.html#/title-slide)  given by Mikis Stasinopoulos, University of Greenwich, to the XVII Encontro Mineiro de Statistica on Octomber 2025, in Lavras, Brazil.  It was soon realised that there was also a large gap between the [statistical modelling]
terminology and the terminology used by other  data analysis scientists. For example,  the [response] variable a well known expression to describe the variable of interest in a [regression analysis] its is called the [target] in [machine learning]. In this glossary we are trying to deal with those situation by describing both definitions. Also, we decided to include at several places of the  glossary, `R` output if  we thought it facilitate the understanding  ideas and concepts presented here.


The following package are needed for explaining some of the concepts below; 
```{r}
#| warning: false
library(gamlss)
library(gamlss2)
```
The data set  `rent99` will be used; 
```{r}
#| warning: false
data(rent99)
```

Two of the variables in the data set are not needed so we are taking them out.
```{r}
da <- rent99[,-c(2,9) ]
head(da)
```


# Glossary

################################################################################
## A

#### accumulated local effects 

Accumulated Local Effects, ALE, @apley2020visualizing, is a model agnostic technique for interpretatng  [term]s  in a [model].  It is showing how explanatory variable affects the predictions value of the model. It works  even with correlated explanatory variables, by analysing differences in predictions across local intervals rather than averaging whole predictions. ALE avoids issues with [multicollinearity] by focusing on the change in predictions within local windows, therefore providing more accurate and  unbiased global explanations for   variable importance. 


### acf

The term [acf] stands for [autocorrelation] function.  The [acf] is a [diagnostic] tool for detecting [autocorrelation] in a [vector] $\text{x}$.   

#### additive smoothers 

Additive smoothers occurs when the contribution of one or more continuous [term]s is added to the rest of the terms in the equation of  a model i.e. $s_1(x_1)+s_2(x_2)+ s_3(x_1, x_2)$, where the $s()$s  are different [smooth function]s.

#### additive model

An additive model occurs when the contribution of each [term]s is added to the rest of the terms in the [formula] of  a model. Let $x_1$ abd $x_2$  be two continuous variables and  $f_1$ and $f_2$ to be two [factor]s. i.e. $b_1 x_1+s_2(x_2)+ f_1*f_2+f_2*s_3(x_1, x_2)$, where the $s()$s  are different smooth functions $\ldots$


#### adequate fit

We call a [fit] adequate if the [model] represents the [data] well. Within a [distributional regression] framework we have two types of [adequate fit]s to consider. The first has to do of whether the [distribution] of the [response] is fitted adequately the second on  whether the chosen [term]s are adequate for modelling the [distribution parameters] model. 


#### agnostic method

An [agnostic method] in [statistical modelling] is a technique which could apply to any [model] independently if they are [mathematical model]s, [agent based model]s or [algorithmic model]s.  

#### agent

An [agent] is the basic unit of interest in a [agent based model].

#### agent based model

An agent based model is a simulation, bottom up model, where the unit of interest is called an [agent]. The model is build by simulating a lot of times how the agents interact between them within a given environment. The [agent] is the basic unit of interest and its behaviour is studied using simple mathematical or logic rules. The behaviour of the agent is studied after a long simulation exercise. Therefore important part of the model is how to set the parameters determine the [agent] behaviour.  


#### AIC

[AIC] stands for the Akaike Information Criterion, @Akaike73b. [AIC] is used to  compare different fitted models. The [AIC] is defined as [deviance] $+2\times df$  where $df$ are the [degrees of freedom] and 2 stands for the **penalty**, see also [GAIC] and [BIC].


#### ALE 

[ALE] is an abbreviation for  [accumulated local effects] 

#### algorithm 

An algorithm is a step-by-step computational procedure designed to perform a specified task. For [input-output model]s this task mostly has to do with finding an unknown function say $g()$, which connect the input with the output, i.e. $Y=g(X)$. 

#### algorithmic model

 An algorithmic model is a model based on an algorithm. In [regression] typically an algorithmic model trying to model  $X \rightarrow Y$, through an unknown function  $g()$ i.e. $Y=g(X)$.   No explicit assumptions are made for the unknown function $g()$  but a lot of implicit [assumptions] depending on the type of algorithm used. Note that an [algorithmic model], like a [mathematical model], can be deterministic or [stochastic model] The [stochastic model] can be written as  $g()$ i.e. $Y=g(X)+ \boldsymbol{\epsilon}$.

#### assumptions

Assumptions are axiomatic statements needed to be accepted for the model to work. Models need assumptions because of their simplified nature. The reasoning behind assumption is that if the assumptions are correct then the model can be useful. There are **explicit** and **implicit** assumptions. The **explicit** assumptions  are usually mathematical ones and could be  easily checked using [diagnostics] tools. The  **implicit** assumptions common in [algorithmic model]s are more difficult to check. **Incorrect** assumptions could lead to questionable scientific discoveries.  For an [agent based  model] the assumptions are the different ways  the agent are behaving.



#### autocorrelation

The term [autocorrelation] in a vector $\textbf{x}$  implies that sequential values of $\textbf{x}$ depend on previous values. Autocorrelation is very common on [time series] data and can be detected using [acf] and [pacf] functions.      

#### averaging models

################################################################################
## B

#### backfitting 

The [backfitting] algorithm is an algorithm discussed by  @HastieTibshirani00 for fitting more  that one smoother in a [formula]  for a [model].

Averaging models is a way of selecting a [final model] by averaging results from different fitted models. 

#### bagging 

Bagging refers to [bootstrapping] followed by an [averaging models] procedure. That is. when $B$ models are fitted to $B$ bootstrap [data partition]s  and the resulting models are then averaged. 


#### Bernoulli distribution

The [Bernoulli distribution] is special case of the [binomial distribution] when $n=1$. A Bernoulli [random] variables has range of values  $0$ or $1$, expected value $\mathbb{E}(Y)=p$ and variance $p(1-p)$.
@fig-bernoulli show three realization of the [Bernoulli distribution] at different values of $p=(0.1,0.3,0.5)$
SOMETHING IS WRoNG WITH IT the p should be in the first line not the second


```{r}
#| label: fig-bernoulli
#| warning: false
#| fig-cap: "The Bermoulli distribution with differenrt values of p=(0.1,0.3,0.5) and n=1"
library(gamlss.ggplots)
family_pdf(BI, mu=c(0.1,0.3,0.5), to=1, title="Bermoulli p=c(0.1,0.3,0.5) an n=1")
```


#### BIC 

BIC is the Bayesian information criterion of @Schwarz78. Within a [distributional regression] model the BIC can be used to  compare different fitted models.  The BIC is also refer to also as  [SBC], Schwarz  Bayesian Criterion. The BIC is defined as [deviance] $+ \log(n) \times DF$, when $n$ is the number of observations in the [data],  and $DF$ are the [degrees of freedom].  See also [GAIC].


#### binomial distribution

The pdf of a binomial distributed [random] variable is:
$$f(Y|p)= \binom{n}{p} p^{p} (1-p)^{n-p}$$
where $Y$ takes values in the range $0,1,2,\ldots,n$, $n$  is called here the **binomial denominator** and the parameter $p$ is a probability parameter taking values $0<p<1$. Note that $\binom{n}{p}$ can be written also as;   
$$ \binom{n}{p} =\frac{\Gamma(n+1)}{\Gamma(y+1) \Gamma{(n-y+1)}}$$
For more details see pp. 521-522   of @Rigbyetal2019. The expected value of the distribution is $\mathbb{E}(Y)=np$ and the variance is $\mathbb{V}ar(Y)=np(1-p)$. @fig-binomial show the three different binomial distributions with binomial denominator equal to 10 and different probabilities at   

```{r}
#| label: fig-binomial
#| fig-cap: "The binomial distribution with different values of p=(0.1,0.3,0.5) and n=10"
#| warning: false
library(gamlss.ggplots)
family_pdf(BI, mu=c(0.1,0.3,0.5), title="binomial p=c(0.1,0.3,0.5) and n=10")
```
Note that in the `gamlss.ggplots` documentation  the parameter of the [binomial distribution]  called here as $p$ is denote as `mu`. The binomial denominator in the function  `family_pdf` can be specified in by the argument `to`. In @fig-binimial  it take the default value which is 10.  


#### black box

A black box is a [model] with difficult interpretation. A lot of machine leaning models are black boxes.
There are two reason for black box model i) the function  $g()$ which the black box tries to estimate is too complex. ii) there are proprietary reasons. The [question] of the problem should determine whether a [black box] [model] is appropriate.  @rudin2019stop argued  "stop explaining black box machine learning models for **high stakes decisions** and use [interpretable model]s instead".  The argument came from the fact that among all adequate model in the [Rashomon set] few are interpretable [@rudin2024amazing],




#### bootstrapping

Bootstrapping is a way to fit multiple models to a single [data] set by repeatedly re-sampling with replacement from the original [data] set. The multiple fits can be used  to obtain variability of the parameters of the model. In this sense bootstrapping  complement Bayesian fits where the information about the variability comes from prior assumptions rather than the bootstrap replications. Bootstrapping is based on [data partition]s with replacement $B$ times so the different fits could produce $B$ estimates of fitted values and residuals. Averaging those values sometimes refer to as [bagging]. A good  book  about bootstrapping is @EfronTibshirani93.


#### boosting

Boosting is a way of fitting models using  many *complementary* sequential simple models and average them in order to build a unique fitted model. The individual models should be simple and easy to fit and should complement each other by modelling different characteristic of the [data]. Finally the individual fitted simple models contribute when averaging only to a small part say 10 or 20 percent of the original contribution. This percentage is one of the hyper parameters of boosting. The other hyper parameter is how many time we should repeat the fitting procedure in order to reach an adequate model. The latest is the main smoothing parameters of the boosting procedure. For determine the smoothing parameters cross validation can be used which requiring a good [measure of goodness of fit].



#### bucket plot

A bucket plot, @de2022bucket, is a residual based [diagnostic] tool which can detect [skewness] and [kurtosis] in the [residuals] of a [model] and therefore of the [response] variable .  Needs a plot with explanation...

```{r}

```

################################################################################
## C

#### centile

A centile value is defined as $100 \times$ [quantile]. Note that quantiles are taking values from 0 to 1 so centile sare defined from 1% to 100%.   The quantile at $0.50$ or 50% centile is defined as the median of the distribution.   In practice the term [centile] is used as synonymous to [quantile]. Let $F(Y)$ be the cdf of the random variable $Y$ tthen the   the median of $Y$ is defined as the value of $Y$ satisfying the equation $0.50=F(Y)$ or $\text{median}(Y)=F^{-1}(0.50)$ where $F^{-1}()$ is the inverse of the [cdf] also known as the [quantile function]. Note that a centile (quantile) is a characteristic of both of the [distribution] of a [random] variable but also  of a [vector] of the [sample] where the [empirical cdf] can be defined.  


#### centile estimation

Centile estimation is another term for [reference curves] fitting.  

#### censored response

A [censored response] is a [response] variable which takes [interval values],  

####  censored values 

A [random] variable which we do not know its exact value but  that it have been fallen  within a known interval it said to have [censored values] or [interval values].  

#### cdf 

A cdf is the **commutative distribution function** of a random variable $Y$, usually denoted as $F(Y)$. The first derivative of the cdf define the probability distribution function, pdf of $Y$ i.e.  $f(Y)=F'(Y)$ For a [distributional regression] where the [response] is assumed to have a proper distribution [pdf]  he [cdf] is usually refer to as the commutative distribution function of the response.   


#### continuous distribution 

A [continuous distribution] is a distribution function, [pdf], of a [random] variable $Y$ taking values in the real line. The only property a [continuous distribution] has to have is that if is integrated with respect to the random variable it should add up tp one i.e. $\int_{-\infty}^{\infty} f(Y)dy=1$. There are three types of continuous distributions depending on the range of the continuous random variable $Y$;

i) $(-\infty , \infty)$, continuous in the real line;
ii)  $(0, \infty)$,  continuous in the positive real line;
iii)  $(0 , 1)$  continuous on a finite  interval in the real line;

Next we show plots the different types of continuous distributions. We start with the normal distribution in  which has two parameters $\mu$ the mean and $\sigma$ the standard deviation and which is defined  on $(-\infty , \infty)$  
```{r}
#| label: fig-normal
#| fig-cap: "The normal distribution with differenrt values of sigma=(1,1.5,2) and mu=0"
#| warning: false 
library(gamlss.ggplots)
family_pdf("NO", mu=c(0, 0, 0), sigma =c(1,1.5, 2), from=-5, to=5,title="Normal with mu=(0,0,0) and sigma=(1,1.5,2)")
```
Similarly the gamma distribution in @fig-beta,  has two parameters  $\mu$ the mean and $\sigma$a scale parameter but its defined on the positive real line $(0, \infty)$:
```{r}
#| label: fig-gamma
#| fig-cap: "The gamma distribution with differenrt values of mu ar 1 and sigma=(1 ,0.8, 1.5)."
#| warning: false 
library(gamlss.ggplots)
family_pdf("GA", mu=c(1, 1, 1), sigma =c(1,0.8, 1.5), from=0.01, to=5,
        title="Gamma with mu=(1,1,1) and sigma=(1,0.8, 1.5)")   
```
The beta distribution in @fig-beta, is defined on the real line $(0, \infty)$ and  two parameters  $\mu$ the mean and $\sigma$ a scale parameter:
```{r}
#| label: fig-beta
#| fig-cap: "The beta distribution with differenrt values of mu=(0.2, 0.5, 0.8) and sigma=(0.2 ,0.8, 0.3)."
#| warning: false 
library(gamlss.ggplots)
family_pdf("BE", mu=c(.2, .5, .8), sigma =c(0.2 ,0.8, 0.3), from=0.01, to=0.99,
           title="Beta with mu=(.2, .5, .8) and sigma=(0.2,0.8, 0.3)") 
```
As rule the more parameters exist in a [distribution] the more flexible the [distribution] is. 


Note that any random variable in the interval $(\alpha, \beta)$ can be transformed to a distribution in the interval  $(0 , 1)$. Also note that  any distribution with range  $(-\infty , \infty)$ can be **log** or **logit** transformed  to a distribution in the positive real line  $(0, \infty)$ or  to the fine interval $(0 , 1)$, respectively.  


####  characteristics of the distribution

Each [distribution] has its own properties. We called those the [characteristics of the distribution] or the [properties of the distribution]. In a [distributional regression] framework  we are dealing both with [properties] of the [distribution parameters]  and [characteristics of the distribution] themselves. In the later case, that is, [characteristics of the distribution]  there are two major way to  characterise the [properties] of a [distribution] i) using [moment]s ii) using [quantile] based measures.


#### classification model

A [classification model] is an [unsupervised learning] [model]  in which the output that is the [response]  is a  [factor] with unknown  categories. The model tries to guess what is the best possible value for the response given the [explanatory variables]. The models for  [distributional regression] are all [supervised learning model]s in which a [response] exist  in the [data].  


#### copula

#### correlation coefficient

#### count data distribution

See [discrete distribution]



#### continuous rank probability scores

$\dots$

#### CRPS

The expression [CRPS] is an abbreviation for [continuous rank probability scores] which are a [measure of goodness of fit] appropriate for [distributional regression] [model]s.

################################################################################
## D

#### data

The term [data] is  a generic term to describe object which contain some kind of [information].
It  used to mean a file with many numbers, but [data] today could have different forms. Could be **texts**, **pixels**, or any other form containing some [information] to be extracted . The [model] tries to extract the [information] from the [data]. Note that at a pre-[statistical modelling] stage we can still extract useful information from the [data] to actually help us with  the statistical modelling. The package `gamlss.prepdata` is design to help in this direction. Within  [distributional regression] [model]s most of the data used are  [tabular data].
Note that [data] sets in the statistical programming language `R` are refereed to as `data.frame`s.  

#### `data.frame` 
   
`data.frame` is the way a [data] set is refer to in the `R` statistical programming language.   


#### data partition

The term [data partition] has two different meanings.  The fist has to do with [statistical modelling] the second on looking at whether the variables involved behave differently at different parts of the [data].

Data partition in the process of [statistical modelling] helps the model building process, [model] [interpretation], the checking for [over fit] and  also helps to improve the [statistical inference] by providing extra [information] about variations in the parameters. There are  different types of data partition;
 a **single** partition of a data set provides holdout samples for prediction and validation purposes. A **multiple** partitions as for example [bootstrapping] and  [K-fold cross validation]  to help inference. 
   
 ![Diffrent data partitions](data_partition.png){width=500}     


 The second meaning of [data partition] tries to answer the following question. Is there a way to partition the data in such a way that the relationships between the variables in the [data] behave differently?; or more generally can I divide the [data] in such way that different models can apply to different parts of the [data]?.  Models like [regression tree]s try to separate the [data] in such a way that the resulted subclass are homogeneous by fitting differenr constant models withing each part.. $\ldots$ 
 
#### data analysis

Data analysis is art of extraction information from the [data]. The first question any researcher has to ask is; can the [data] or the [model] answer the [question] in hand? 

#### data  generating mechanism

The [data generating mechanism] is a set of assumptions on how the [data] are generated. It is often difficult to check  those [assumptions] because of the possibility of having a  [Rashomon effect] where different assumptions lead to similar results. In distributional regression the [data] generating mechanism is assumed mostly on the way the [response] is generated given the [explanatory variables].    


#### degrees of freedom

The [degrees of freedom] of a model measure the complexity of the model. For mathematical parametric models the degrees of freedom are the number of independent parameters used in the model. For mathematical [smooth model]s the degrees are defined as the diagonal elements of the smoothing matrix i.e. $\hat{\textbf{y}}= S(\textbf{x})$ where $\hat{\textbf{y}}$ are the [fitted values] of the smooth function and $\textbf{x}$ is the [explanatory variables]. For algorithmic models the degrees of freedom often are difficult to calculate.       


#### design matrix

By [design matrix] we usually refer to the matrix $\textbf{X}$ containing  all relevant explanatory variables. Note that for distributional regression model each paramerter of the distribution could have its own design matrix i.e.   $\textbf{X}_{\theta_k}$    

#### deviance

The [deviance] is a measure of goodness of fit and defined as $-2 \ell(\boldsymbol{\theta})$, that is, minus twice the [log likelihood]. The  [deviance] can be used  for model comparison.The  [deviance]  can be defined in both the [training] or the [test] [data] set so we can have a [training] or [test] [deviance].  In [GAMLSS] modelling the [training] [deviance], is  used  with its penalised form the Generalise Akaike Information Criterion, [GAIC],  for model comparison. The deviance can be seeing  as an [empirical risk] measure based on [information criterion] concepts. It is a [summary statistic], because it is a sum of the individual deviance increments and therefore an overall measure of goodness of fit. It tell us how well the conditional distribution of $D(Y|\boldsymbol{\theta})$  fits the [data] overall. It does not tells whether the tail or the middle of the distribution is fitting well. The [deviance increment]s may help to obtain such information. 

#### deviance increment

#### density function

At a theoretical level a [density function] is the [pdf] of a [random] variable. At the [sample] level a density function of a [vector] in the [data] is an empirical distribution function of a variable in the [data] obtain using smoothing techniques. In thios case the [density function] is a smooth [histogram].


```{r}
#| label: fig-hist-density
#| fig-cap: "An estimated density funtion on top of a histogram for the variable the price of `rent` in the Munich `rent99` data set." 
#| warning: false 
library(gamlss.data)
library(gamlss.ggplots)
y_hist(rent99$rent)
```


#### diagnostics 

Diagnostics are tools for helping to check the [assumptions] of a fitted [model]. Within [distributional regression]  can help to identifying whether the [distribution] of the [resposnse] is fitted well or whether the [term]s in the  different [distribution parameters] models are fitted appropriately.  


#### discrete distribution 

A [discrete distribution] is distribution function [pdf] of a [random] variable which takes only integer values i.e. $0,1,2,\ldots, 10$. Another name for a discrete distribution  is [count data distribution]. 
The only property that any discrete distribution must have is that it should sums up to one. $\sum_{i=1}^{\infty}  f(y_i)=1$. There are mainly two types of [discrete distribution]s depending on the range of the discrete [random] variable involve i) infinite count ii) finite count [distribution]s.  The most common example of an infinite count distribution is the [Poisson distribution] and the most common example of the finite count is the [binomial distribution]. 

#### distribution

The term [distribution] refer to a probability distribution function  [pdf],  $f(Y)$. The only property for $f(Y)$ is required is that it should sum up to one.  In distributional regression we write the [pdf]  of the [response] as $D(Y | \boldsymbol{\theta})$ not $f()$ a notation which could be used for describing the relationships between the [explanatory variables] and the [response]. We are also  emphasising  that the [pdf] of the response is a function the different [distribution parameters], $\boldsymbol{\theta}$.  The  function $D(Y | \boldsymbol{\theta})$ describes how the behaviour of the [response] variable is affected by the [explanatory variables] and it is the main [stochastic] component of a [distributional regression]. 


#### distribution parameters

Theoretical distributions depend on [distribution parameters] $\boldsymbol{\theta}$ where $\boldsymbol{\theta}=(\theta_1, \theta_2, \ldots, \theta_K)$ is a [vector] of length $K$. The more independent [distribution parameters] exist in a [distribution] family the more flexible the [distribution] is. For example 
one parameter distributions can only model one characteristic of the [distribution] of the [response] mainly the [location parameter].

#### distributional regression 

A distributional regression model is an [input-output model]  model in which the response $Y$  is assumed to have a distribution in which all the parameters of the distribution could depend on explanatory variables. This can be represented as $X \rightarrow D(Y|\boldsymbol{\theta}(X))$ where $\boldsymbol{\theta}$ are the k parameters of the distribution the conditional distribution $D(Y|\boldsymbol{\theta})$.  

#### dummy variables

A dummy variables is a set of binary (0 ,1) vectors indication whether certain conditions are present or not. Any `factor` is represented as a set of dummy variables in a `design matrix` of a model. The number of columns in the dummy variable set representation of a factor are the number of `levels` of factor minus one to avoid collinearity with the constant a vector of ones in the design matrix.   


################################################################################
## E

#### empirical cdf

The [empirical cdf], [ECDF] is the cumulative function function of any continuous or ordered categorical variable in the [sample], ([data]). The [empirical cdf] is a step function that shows the proportion of observed [data] points less than or equal to a specific value in the [data] . It creates a cumulative probability distribution directly from a vector say $\textbf{x}$ in the [data], without assuming a theoretical model therefore is the source of a many non-parametric procedures in statistics. Any [vector]  $\textbf{x}$ in a [tabular data] set can have an [empirical cdf] but it is more  difficult to justify it  for unordered categorical variables since it implies an explicit order in $\textbf{x}$. For a given value $x_o$ in   $\textbf{x}$ for any continuous or ordered categorical variable in the [data] the [empirical cdf], [ECDF] gives the percentage of [data] points in $\textbf{x}$  less than or equal $x_o$ for example,  #$[\textbf{x} \le x_o] \times \frac{1}{n}$. 
@fig-ECDF-continuous shows the ECDF curve of the continuous variable `rent` from the `rent99` data set. 

```{r}
#| warning: false
#| label:  fig-ECDF-continuous
#| fig.cap: "The ECDF of a continuous variables. "
library(gamlss)
plot(ecdf(rent99$rent), main="ECDF", xlab="rent")
```
@fig-ECDF-ordered shows the ECDF curve of the oerdered categorical variable `location` from the `rent99` data set. 
```{r}
#| label:  fig-ECDF-ordered
#| fig.cap: "The ECDF of an ordered categorical variables."
plot(ecdf(rent99$location), main="ECDF", xlab="rent")
```
The [empirical cdf] is also defined on the [residuals] of a fitted [model]. In [distributional regression] if the model represent correctly the [data generating mechanism] then the [residuals] of a [distributional regression] [model] should be behave as a [white noise]. In this case the empirical cumulative distribution function of the residuals should be very close to the cumulative distribution function of the Normal distribution.
Next we fit a GAMLSS model using the `gamlss2()` function to the `rent99` [data]. We use the [formula] `rent~s(area)+s(yearc)+location+kitchen+kitchen` for the $\mu$ model and the [formula] `s(area)+s(yearc)` for $\sigma$ and we fit a [BCT]o distribution. The [ECDF] of the residual of the fitted [model] should behave, if the model is a [adequate fit]   as normally distributed [random] variables. @fig-ECDF-resid_normal show the  cdf of the [residuals] with a normal [cdf]  superimposed in red. From the plot there is no reason he doubt the adequacy of the model. Note that the same information could be extracted by plotting the [QQ-plot] or the [worm plot] of the residuals. 

```{r}
#| label:  fig-ECDF-resid_normal
#| fig.cap: "The ECDF of the residuals from a GAMLSS fitted model with the normal cdf superimposed."
#| warning: false
library(ggplot2)
library(gamlss.data)
library(gamlss2)
m1 <- gamlss2(rent~s(area)+s(yearc)+location+kitchen+kitchen|
                s(area)+s(yearc), family=BCTo, data=rent99 )
library(gamlss.ggplots)
gg <- resid_ecdf(m1)
gg+ggplot2::stat_function(fun = pNO, args=list(mu=0, sigma=1), col="red")
```


#### ECDF

See [Empirical cdf] 

#### empirical risk

The [empirical risk]  function is defined by dropping the expecations from the definition of the [risk] function, The definition of the  [empirical risk] is given in [risk] as;
$$\mathbb{ER}(g) = \frac{1}{n}\sum_{i=1}^n [ \ell oss(\hat{g}(x_i), y_i).$$ 
where $\ell oss()$ is the [loss] function.

#### entropy

The [entropy] for a discrete [random] variable $X$ is defined as: 
$$
\begin{split}
H(X) & = -\sum_{x \in X} p(x) \log p(x), \\
     &=  - E_p \log p(x) ,\\
     &= E_p \log \frac{1}{p(x)}
\end{split} $$  
That is, the [entropy] for a discrete random variables $X$ is equal to its expected value of minus the its log [probability]. Note that this quantity  is identical to the [log likelihood] (but the [log likelihood]  it is a function of the parameters and not the [random] variables $X$).   The [entropy] of a continuous random variable $X$ is ginen as:
$$\begin{split}
H(X) &=-\int_{X}f(x) \log f(x)  dx \\
      &= - E_f \log f(x)  \nonumber \\
     &= E_f \log \frac{1}{f(x)}  \nonumber 
\end{split} $$
The [entropy] is a [summary statistic] describing the randomness of the distribution of a [random] variables and it is a function of probabilities **not** the values or the range of the [random] variable $X$. The highest value of entropy is when we have equal probabilities for all values of $X$'s. That is when the $X$ has a [uniform distribution].  Note that the minus the log likelihood of a fitted [distributional regression] [model] which is defined as $- \ell(\boldsymbol{\theta})=\sum_{i=1}^{n} - \log f(y_i|\boldsymbol{\theta})$ is proportional to the  empirical estimated of the  entropy of the [response] variable with the constant of proportionately equal to $\frac{1}{n}$. This shows the close relationship between the concept of [entropy]  and the concept of the [log likelihood] important for statistical [inference]. 


#### error

The term [error] in a [model] is what chaharacterised a modes to  [stochastic], that is, it the model conails a set of  [probability] elements. In [regression analysis] the [error] is often a terms used to describe the residual variation of a model. 


#### errors

The term [errors] refers to the statistical part of a [stochastic] model which accounts for the natural variability of the data. The [errors] often refer to the [residuals] part of the [model] that is what is left after the model has been applied.  


####  exceedance probabilities

#### explanatory variables 

Explanatory variables in an [input-output model] are the input variables $X$. In a [regression analysis] are the variables in which  affect the response.  In a [distributional regression] are the variables on which the distribution of the response is conditioned on.

#### exponential family

The [exponential family] is a family of theoretical [distribution]s with  the propety of allowing [sufficient statistic]s 


#### expected value

 The [expected value] (the [mean] or first [moment]) of a [distribution] called distribution is defined as $\mathbb{E}(Y)=\int Y f(y)dY$  for [continuous distribution]s and  

################################################################################
## F

#### factor

A factor is  a categorical variables, that is, a variable which take limited unordered or orderer values.  


#### fit

A model fitting process involves the [data] the [model] and the [purpose]  of the study. The model is fitted by  minimise a [measure of goodness of fit] see @fig-fit.  So a [fit]   refers to a model fitted to the data for specific purpose. Note that a model fit can be an [adequate fit], [over-fit] or [under fit]. A single fit could provides unique [fitted values] and [residuals] for both [training] and [test] data sets.

```{mermaid}
%%| fig-width: 5
%%| label: fig-fit
%%| fig-cap: "The model fitting process needs both data and model which to conform with the purpose of the study  "
flowchart TB
  subgraph model-fitting-process
   style model-fitting-process fill:#e3f2fd,stroke:#1e88e5,stroke-width:2px
    direction LR
    A[data]
    B[model]
    D[purpose]
  end

  A --> C[fit]
  B --> C
  D --> C
```


#### feature

A [feature] in [data] analysis is an one of the [explanatory variables]  used for the model possible after a [transformation].

#### function

There are several occasions we use the terms [function] in [statistical modelling]. 
In an [input-output model] we refer to the function  $g()$ as an true unknown relationship between $X$ and $Y$  which the [model] tries to approximate. In define distributions we use the functions $f()$ ans $F()$ to describe [pdf]'s and [cdf]'s respectively. In [GLM]'s [GAM]'s  and [GAMLSS]'s we use $g()$ to describe [link] functions. In [distributional regression] we use $D()$ for the [pdf] of the response.   


#### final model

Is the [model] chosen after a selection procedure is applied to model selection. 


#### formula 

The term [formula] and formulae  refer to the `R` [formula] object which is used in the definition of a model. A formula in `R` stars with $\sim$, for example,  `rent~area+yearc+location` wuold indicate that the response is `rent` and the variables `area`, `yearc` ans `location`.    Note that because within  [distributional regression]  there could be one formula for each parameter 

################################################################################
## G

#### GAIC

GAIC stand for the Generalised Akaike Criterion @Akaike83 defined as `deviance` $+ K \times df$  where $df$ are the `degrees of freedom` and $K$ stands for the `penalty`.

#### GLM

 GLM stands for  Generalized Linear Model, @NelderWeddeburn72, a [mathematical model] which dominated the 1980's. As [input-output model] it can be written as $$
    X  {\longrightarrow} \fbox{f()}  {\longrightarrow} \mathbb{E}(Y) 
    $$ 
  where $\mathbb{E}(Y)$ is the expected values of the output $Y$ and $f()$ is a general function connecting the $X$'s with the expected value of $Y$.
As a [statistical model] a GLM can be written as;
$$
\begin{split}
\textbf{y}    &  \stackrel{\small{ind}}{\sim }  D( \boldsymbol{\mu},  \phi) \nonumber \\
\eta &= g(\boldsymbol{\mu}) &= \textbf{X}\boldsymbol{\beta} \nonumber \\
\end{split} $$
where $\stackrel{\small{ind}}{\sim}$ is read as the vector of the response is "independently distributed [random] [vector] having a distribution $D()$."  The distribution  $D()$  belongs to the [exponential family] which has as sub-models the [normal distribution], the [gamma distribution], the [inverse Gaussian], the [Poisson distribution] ans the [binomial distribution].  The fact that the elements of the vector $\textbf{y}$ are independent from each has the consequence that the log-likelihood is simply the sum of the individual log-[likelihood]s.    
 
 
 other with different mean $\mu$ depending linearly on the $X$'s but with a common second parameter  $\phi$.
   which make sure that values of any distribution parameter  $\theta$ are in the right range.

There were two major problems with the assumption for $g(\boldsymbol{\eta})$

#### GAM

GAM stands for Generalized Additive Model, @HastieTibshirani90,  ...

#### GAMLSS

GAMLSS stand for Genaralized Additive Model for Location, Scaler and Shape, @RigbyStasinopoulos05  ...


#### `gamlss`

`gamlss` is the original package in `R` for fitting a GAMLSS model @StasinopoulosRigby07.

#### `gamlss2`

`gamlss2` the new package in R for fitting a GAMLSS model.

#### `gamlss.data`

`gamlss.data`  is a package in `R` containing data sets use to demonstrate GAMLSS models.

#### `gamlss.prepdata`

`gamlss.prepdata` is a package in R to help users to preper data for analysis using a [distributional regression] model.


#### `gamlss.ggplots`

`gamlss.ggplots` is a package in `R` to help with diagnostics and other graphics  using the `ggplot2` package.  The functions in the package can be applied to fitted GAMLSS model independently whether were fitted  using the  [`gamlss`] or [`gamlss2`].


#### `gamlss.dist`

`gamlss.dist` a package containing all the theoretical distribution which can be assumed for the response when fitting a GAMLSS  model.

#### `gamlss.tr`

`gamlss.tr` can take any distribution family in `gamlss.dist` and truncated `left` `right` of in `both` directions. 

#### `gamlss.cens`

`gamlss.cens` can take any distribution family in `gamlss.dist` and apply  `left` `right` or  `interval` censoring. 


#### generalized Tobit model 

#### generalized Tobit distribution

#### goodness of fit 

A goodness of fit is a measure (a quantity) design to evaluate how close the model is to the data. 

#### GAIC

GAIC is  the  Generalise Akaike Information Criterion defined as `deviance` $+k \times df$ where $k$ is the penalty applied to the [degrees of freedom] $df$. 


#### graphics

################################################################################
## H

#### histogram

A [histogram] is a estimated density probability function function for a [sample] [vector]. See [density function] for a [smooth function] of a [histogram].

#### hypothesis

The hypothesis is what the researcher tries to understand and answer see also the `purpose` of the study 

#### hyper parameter


################################################################################
## I

#### information

The word [information] represent the uncertainty about an unknown quantity, in our case the  [data]. In 
[statistical modelling] is refereed to the way  [information] is extracted from [data] using a [model].   

#### information criterion

By [information criterion] we refer to a [measure of goodness of fit] which it based on [information theory]

#### information theory 

By [information theory] we mean the theory of of communication developed by @shannon1948mathematical which is  based on the idea of [entropy].

#### interpretable model

A model is an [interpretable model] if its is easy to explain to others. Most  [mathematical model]s but not all [algorithmic model]s are [interpretable model]s. The following words have the same meaning  as [interpretable model], **transparent** model, **explainable** model or **comprehensive** model. 


#### interval values

A [random] variable is taking [interval values] if its exact value observed value is not known but we know that  it occurred in a specific interval range i.e $10< Y <15$.  Another expression of this behaviour is [censored values].

#### input-output model

An input-output model is a model where the variables $X$, the input, affect the variables $Y$, the output i.e. $X \rightarrow Y$. Input-output model are [supervised learning model]s since a response variable, $Y$ always exist. @breiman2003statistical use the diagram 
$$
X  {\longrightarrow}  \fbox{NATURE} {\longrightarrow} Y, \ 
$$
but nature is too complex so we use a `model` to help us. 
$$
X  {\longrightarrow}  \fbox{Model} {\longrightarrow} Y \
$$
In practice we use a mathematical function $g(X)$ to describe the relationship. 
$$
X  {\longrightarrow} \fbox{g()}  {\longrightarrow} Y \ 
$$
The function $g(.)$ is unknown so the task of the modeller is to find such a function. In a distributional regression framework the represanation is more complex. $$
X  {\longrightarrow} \fbox{g()}  {\longrightarrow} D(Y|\theta(X)) \ 
$$


#### independent random variable

A [vector] of [random variable]s say $\textbf{y}$ is said to be independent (or independently distributed) if the distribution of the component of the vector can be written as the product of the individual probabilities i.e. $Pr(\textbf{y}) =  \prod_{i=1}^n Pr({y}_i)$  


#### independent variables

The [explanatory variables] in a [regression analysis]

#### inference

In statistics [inference] refers to the idea that under certain circumstances we can go from the [sample] [data] to draw conclusions about the [population] of interest, That is we can can go from the **small** to the the **big**. The circumstances depend on [assumptions] about the [data generation mechanism].   

#### interpretation 

The interpretation of a model is the story behind the fitted model, what it is telling you. 


#### input variables

The [explanatory variables] in a [regression analysis] same as  the input in an [input-output model].


################################################################################

## J

################################################################################
## K


#### K-fold cross validation

A K-fold Cross Validation provides [training] and [test] data for all the observations by going through all the K-folds, It provides unique `fitted values` and `residualas` and in addition provides multiple $K-1$ values for fitted values and residualas for the training $K-1$ folds.

#### kurtosis

#### kurtosis parameter

################################################################################
## L

#### LASSO

#### least squares

#### linear model 

By linear model we refer to models described by the [assumptions] $\textbf{y}=\textbf{X}\boldsymbol{\beta}+e$ where $e_i \sim N(\boldsymbol{0}, \sigma^2)$.  That is the relationship between the response and the explanatory variables is linear determined by the coefficients $\boldsymbol{\beta}$ and the error term has a normal distribution with constant variance $\sigma^2$. The unknwon parameters in this case are the  $\boldsymbol{\beta}$ and $\sigma^2$. I we know those parameters we can predict the behaviour of the response if the model is correct.  

#### link function 

A function connecting  the model  predictor with the distribution parameter for example $\eta_{\sigma}= g(\sigma)$ The link function ensure that the distribution parameter is on the right range. For example, since $\sigma$ take values in the positive real line $0< \sigma <- \infty$  modelling sigma as $\eta_{\sigma}=log(\sigma)$ will make sure that $\exp(\eta_{\sigma})$ is always positive.  For modelling purposes the the link function $g(\theta)$ its inverse $g^{-1}(\eta_{\theta})$ and the first derivative $d \eta_{\theta}/d\theta$ must be defined.

#### likelihood 

The likelihood  function is define as the [probability] of observing the [sample] seeing not as a function of the random variable involed but as a function of the parameters of its distribution. For example, let the random vector variable $Y$ to come   from an assumed distribution $f(\textbf{Y}|\boldsymbol{\theta})$. We obsrved the $n$ dimensional vector $\textbf{y}$. If in addition we assume that the elements of  $\textbf{y}$ are comming from  $f(\textbf{Y}|\boldsymbol{\theta})$ and they are independed fraom each other, then the likelihood funtyion for $\boldsymbol{\theta}$ is defined as $$L(\boldsymbol{\theta})=\prod_{i=1}^{n} f(y_i|\boldsymbol{\theta})$$ and its log-likelihood as

$$\ell(\boldsymbol{\theta})=\sum_{i=1}^{n} \log f(y_i|\boldsymbol{\theta})$$
In both cases the functions are seen as functions of $\boldsymbol{\theta}$ rather than $\textbf{y}$.

#### log likelihood 

The [log likelihood] is a [measure of goodness of fit] for a  [model]. It is denoted as $\ell(\boldsymbol{\theta})$  see [likelihood] for its definition.

#### location parameter

A parameter of the distribution describing the centre of the [distribution]. The most common  [location parameter]s are the [mean] and the [median].

#### loss

A [loss] function  is a [measure of goodness of fit]. The [loss] function $\ell oss()$ is a function of both the [data]  and the fitted [model] and measures how far we are prepared to accept that the model is correct. In gambling the [loss] function has a monetary value but in [statistical modelling] is a measure of difference between the [model] and the [data], (see also [risk]).  Typical loss functions are; i) [squared errors]  i.e.  $\sum_{i}^n (y_i - \hat{y_i})^2$ and  [absolute errors] i.e. $\sum_{i}^n \left|y_i - \hat{y_i} \right|$ determine how far the actual value $y$ is from its estimating value $\hat{y}$. Note that  in [distributional regression] $\hat{y}$ is an estimate of the [location parameter] of the the model  therefore is not an appropriate measure we are interest in other parts of the [distribution] fo the [response]  for example the [tail]. a more appropriate measure in this case could be minus the  the [log likelihood] $-\ell(\hat{\boldsymbol{\theta}}_i)$. Notice that [loss] functions could include penalty terms penalising the complexity of the model.


################################################################################
## M  

#### mathematical model     

A mathematical model is a model which is build using mathematical equations.

#### maximum likelihood estimation 

The Maximum Likelihood Estimation, MLE, is method of fiting a distribution to a vector $\textbf{y}$ using minus the [log likelihood] as [measure of goodness of fit].  Note the minimise $-\ell(\boldsymbol{\theta})$ is equivalent of ninimise the [deviance]  $-2\ell(\boldsymbol{\theta})$ or maximising the [likelihood] $L(\boldsymbol{\theta})$ or the [log likelihood], $\ell(\boldsymbol{\theta})$.   
      

####  measure of association
     
####  measure of goodness of fit

A measure of goodness of fit is a way to evaluate the fidelity of the data for a given. model.  That is, how close, is the model to the data using an objective measure. of goodness of fit. 



#### mean

The mean of a [vector] in a [sample] is the average value of the [vector] i.e. $\bar{x}= \frac{1}{n}\sum_{i=1}^n x_i$. The mean or [expected value] of a [distribution] sometimes called the first [moment] of the distribution is defined as $\mathbb{E}(Y)=\int Y f(y)dY$  

#### median

#### moment

#### mixed distribution 

#### question 

The question is the purpose of the study in hand;


#### mathematical model

A mathematical model is a model which is using mathematical equations to describe the variables involved.
A popular mathematical model partial or  ordinary differential equations  model


#### machine learning

Machine learning refers to a selection of algorithmic models.  Some of the machine learning are [model]s are [supevised learnig] models i.e [regression] type and some [classification] type.    A typical [regression] [machine learning] [model] has the form $\textbf{y}=g(\textbf{x})+\boldsymbol{\epsilon}$ where the error $\boldsymbol{\epsilon}$, a [vector], is assumed to be an independently distributed [random] variable and $g()$ an unkown function to be estimated form the data.  Note that one of the implicit [assumptions] of the model is that the [error], $\boldsymbol{\epsilon}$,   has a symmetric distribution. This explicit assumption can be recrtified by using a [transformation] on $\textbf{y}$ which would possible make the transformed response $\textbf{y}'$ symmetric before fitting.  Note however that this transformation is not always exist therefore [distributional regression] models do not rely on it but relying on the fact that the a theoerical [distribution] of the [response] exist.  

#### measure of goodness of fit

A measure of goodness of fit is usually an [empirical Risk] measure. For exampe the mean square error, MSE use in LS model, or the log-likelihood used in GLM and GAMLSS. 

#### model

A model is a simplification of reality by provides an easier way to understand the structure of a problem. Relevent to regression analysis there are mathematical and algorithmic models  A [mathematical model] is  model is using mathematical equations. An [algorithmic model] is  using  using a set of rules to perform a task. An [adequate model] is a model which fits the [training] data well. `All models are wrong but some are useful`, George  @box1979robustness. Whether a model is useful depends on the [purpose] of the study The model should be adequate to answer the [question]  in hand.


#### model average

A model average prevent from choosing a single model by summarizing resul fits from multiple models. In my opinion model average is worth it if the fitted models complement each other. In terms of distributional regression this could mean the one model could fit the tail better while an other fits the middle of the data better. 

#### model formula

A model formula in R takes a  

#### model selection 

Model selection` in GAMLSS has two meanings i) how to choose the distribution of the response and ii) to find out how the $x$'s effects the distribution of $y$. The latest is done by checking how x's effect the parameters of the distribution.


#### model interpretation 


#### model prediction

#### multicollinearity



################################################################################
## N

#### $n$

$n$  is number of observations in a [tabular data] set

#### normalised randomosed quantile ressiduals

#### null model 

As null model we refer to a model with no [terms] in it. In R notation this is the model with  a [model formula] equal to `~1`. For distributional regression a null model is model which has   [model formula] `~1` for all the [distribution parameters] models.   



################################################################################
## O

#### observations

We usually refer to the rows of a [tabular data] as the [observations]. Note that the number of obsrvations in the [data] is denoted as [$n$]. 

#### ordinary differential equations 

#### Occams principle

The [Occams principle] from the Latin "Entia non sunt multiplicanda praeter necessitatem"
(Entities should not be multiplied beyond necessity.)  it says that  "among competing explanations, the one with the fewest assumptions should be preferred".  The principle  does not say the simplest explanation is always true but a simpler models is preferable if  explain the data equally well and complexity 	should not be introduce unnecessary. In [statistical modelling] for [mathematical model] it means [model] with fewer parameters are preferable if they explain the model well or more  generally if two models fit the data similarly, choose the one with fewer predictors. Note that if models fit the data similarly they belong the  [Rashomon set] of relevant models. 

#### over fitting 

Over fitting happens when the [fit] is too close to the [data] and therefore do not generalised well when try to predict, see also [over fit].

#### over fit

By [over fit] we mean situations where the model is too close to the actual [data] but not close to the [population]. An over fitted model is not good for predilection purpose, see also [over fitting]. 


#### out of bag

By [out of bag] [data] we refer to [data] that they have not be used to [fit] the [model]. That is, [test] or [validation] [data] sets.

################################################################################
## P

#### pacf

The term [pacf] stands for partial [autocorrelation] function. The [acf] and [pacf] are [diagnostic] tools for detecting [autocorrelation] in a [vector], $\text{x}$. 

#### parsimony

The principal of parsimony is what is also known as  [Occams principle] state that   

#### partial effects

#### partial differential equations

#### papameters

The parameters in a distribution regression set-up could be of three types: the  `distribution parameters` which could be functions of the explanatory variables i.e. $\theta_k = g(X_k)$, coefficients describing the relationship between the distribution parameters ans the explanatory variables. and the `huper- parameters`.    

#### Poisson distribution

#### population

The population is the set of object we would like to study. The population is related with the [purpose] of the study. The idea behind is that we collect a [sample] ([data] set) from the population of interest to drew inference form the population using the sample. The process is called [statistical inference].

#### principle component regression

#### properties of the distribution

Please see [characteristics of the distribution]

#### properties of the distribution parameters

The [distribution parameters] have different properties by how they are defined within a [distribution]. Any [distribution] can be re-parametrised differently as far as the re-parametrisation end up with the some number of parameters as the original distribution. For [distributional regression] the specific parametrization  do matter in order to help the [interpretation] of the [model]. Some  parametrisations are more useful in practice that others. For example in a [distributional regression] it worth to have a parameter dealing with, where the centre of the [distribution] is, and whether this centre changes with [explanatory variables].  Those parameters are called  [location parameter]s.  Other parameters could describe  how far from the centre each observations could fall. Those are called the  [scale parameter]s. To describe asymmetry in the distribution we have the [skewness parameter]s. To distinguish distribution with fat [tail]s  we have the [kurtosis parameter]s.  Notice that any distribution could have more that one parametrisation and that the specific parametrization could affect different [characteristics of the distribution].


#### power transformation

ABy  [power transformation] in  [distributional regression]  we mean the transformation of the an $X$ variable to a [feature] for analysis using the transformation $\textbf{x}^p$ instead of $\textbf{x}$. where takes values typically in the range $0 \le p \le 1.5$. The transformations tries to make  the values od the $X$ more symmetric and more even spaced.  

#### purpose

The purpose of the study is the [hypothesis] the study is working on.

#### pdf 

#### penalised least squares 

#### PIT residuals

The [PIT residuals] stands for the probability integral transformed  residuals. Within a [distributional regression] framework  are defined as, $$\hat{u}_i = F(y_i| \hat{\boldsymbol {\theta}}_i).$$ {#eq-PITresid} for $i=1\ldots,n,$ where $F(.,| \hat{\boldsymbol {\theta}}_i)$ is the [cdf] of the fitted [model]. 
If the distribution of $y_i$ is specified "correctly" then the [PIT residuals] are expected to behave as identical and independently distributed (i.i.d) random variable from a [uniform distribution] i.e $u_i \sim U(0,1)$. 
While the above definion of IT residuals works perfectly for [continuous distribution]s are not ideal for [discrete distribution]s especially if the range of the response is limited to small range i.e. $0,1,\ldots,10$, For [mixed distribution]s where there is small amount of discrete values or for [censored response]s. A randomisation of the PIT residuals between the values $y_i$ and $y_i+1$ usually solves the problem see for example Chapter 10 of @Stasinopoulosetal2017.



#### probability 


################################################################################
## Q

#### QQ-plot

#### quantile

#### quantile function

#### quantile residuals

## R

#### $r$

$r$ is the number of [explanatory variables] or [input variables] in the data 


#### Rashomon

#### Rashomon set 

#### random 

The word [random]   is used as equivalent to [stochastic].

#### random variable

A random variable $Y$ is a variable which has a distribution. We denote this  as $Y \sim f( Y| \boldsymbol{\theta})$ where the symbol $\sim$ reads as "is distributed" and where $f()$ denote  a probability distribution function [pdf]. Another important feature of a random variable is the specification of its [range] R(Y).  

#### range

This this refer to the range of values that a [random variable] takes. We use the notation $R(Y)$ to describe the range of a random variable $Y$. There are two types of possible ranges i) continuous  and ii) discrete range.  

#### Rashomon

The term [Rashomon] originates from Akira Kurosawas 1950's film **Rashomon**, where a single incident,  a murder and an assault, is recounted by four witnesses, the bandit, the samurais wife, the samurai via a medium, and a woodcutter, each giving a vastly different version of what occurred. The film ends without resolving which account, if any, is accurate. 

![](rashomon.png){width=100}

#### Rashomon effect 

The idea that different models could possible provide different interpretations of the same data set. 

#### Rashomon set 

Is the set of [regression] [model]s with similar [measure of goodness of fit] therfore it is difficult to distinguish between them. [Rashomon set]s are real, practical statistician encounter them all the time.  Note that [Rashomon set]s are comon to over-parametrized models

#### regression 

A [regression] [model] is  an [stochastic] [input-output model] where the [explanatory variables], $X$, affect the [response] $Y$.   


#### regression analysis
A [regression analysis] is an statistical [input-output model] analysis, analysing [tabular data].  Typically regression analysis is refer to [linear model]s but here we use the term to describe any [input-output model] relationship. 

#### regression tree

#### reference curves

#### residuals

The residuals  is a [vector] of length $n$ with each element measuring the difference between observed and fitted values. For a [linear model] the residuals are defined as $y_i-\hat{y}_i$ for $i= 1,\ldots,n$. where $\hat{y}_i$ is the fitted values of observation $i$. The $n$-observations have $n$ residuals. Residuals can be defined in both the [training] or the [test] data sets. Residuals from the training data can be used to check [underfitting] while residuals from the test data can be use for checking  [overfitting] the data.

Two transformations are needed to get the z-scores from a fitted distributional model. Let $y_i$ and $F_o(y_i, \hat{\boldsymbol {\theta}}_i)$ be the $ith$ observation of the response and its fitted cumulative distribution function (cdf), respectively. Then first transformation is to get the probability integral transformed (**PIT**) residuals which are defined as, $$\hat{u}_i = F(y_i, \hat{\boldsymbol {\theta}}_i).$$ {#eq-PITresid} If the distribution of $y_i$ is specified "correctly" then the PIT residuals are expected to behave as identical and independently distributed (i.i.d) random variable from a uniform distribution i.e $u_i \sim U(0,1)$. The problem is, that is rather hard to check deviation from the uniform distribution, so we take the PIT residuals and transform them to z-scores using $$\hat{z}_i = \Phi^{-1}(\hat{u}_i).$$ If the assumed distribution of the response is approximate "correct" the z-scores are i.i.d. standardised normally distributed random variables i.e. $z_i \sim N(0,1).$

The two transformations needed to create the z-score residuals are shown in @fig-resid_types. In @fig-resid_types (a) the response is transform to a PIT using the fitted cdf function $F(y_i, \hat{\boldsymbol {\theta}}_i)$ while In @fig-resid_types (b) the PIT are transformed to z-score using the inverse cdf of the normal distribution $\Phi^{-1}(0,1)$ (otherwise known as the q-function of the normal distribution). While the above residuals works perfectly for continuous distributions are not ideal for discrete counts especially if the range of the response is limited to small range i.e. $0,1,\ldots,10$. A randomisation of the PIT residuals between the values $y_i$ and $y_i+1$ usually solves the problem see for example Chapter 10 of @Stasinopoulosetal2017.

#### response 

The response variables is the output variable in a [input-output model].


#### risk

A [risk] function measure the accuracy of a [model] in terms for a specific [measure of goodness of fit]. 
The [risk] function  is a function of  both the [data] and the [fit]ted [model]. For [regression model]s where only the mean of the response is modelled as a function of explanatory variables , for example,  $Y=g(X)+\epsilon$,  it measure how accurate the function $g()$ is. The mathematical definition of a [risk] function is given as the average of a [loss] function.  Let us define the [loss]  function as the discrepancy measure between the [data] and the [model], $\ell oss()$.    The expected value of a loss function $\ell oss$() is defined as the [risk] function.  $$\mathbb{R}(g) = \mathbb{E}_{X,Y} [ \ell oss(\hat{g}(X), Y) ]$$ where  $\hat{g}()$ is an estimate of the model unknown function $g()$, $X$, $Y$ are the pairs, of an [input output model] variables.   The [risk] measures the average discrepancy between the assumed [model] and the [data].   
The problem with the above definition of the [risk] function is that involve taking expectations with respect to the input and output variables. Even if conditioning on the $X$'s we have to take expectation with repsect to $Y$. We can avoid the problem all together if we replace the [risk] function with the [empirical risk]  which gives equal probability to each row of the [data] matrix. For example the [empirical risk] is defined as
$$\mathbb{ER}(g) = \frac{1}{n}\sum_{i=1}^n [ \ell oss(\hat{g}(x_i), y_i)$$ 
Note that  if the [loss] function is defined as  minus the log [likelihood] (or the [deviance]) minimising the [empirical risk]  is equivalent to maximising the [likelihood] function with rescect to the parameters of the model.  Note that  $\frac{1}{n}$ is just a constant not involved in the minimazation.


################################################################################
## S

#### sample 

In statistics a [sample] has  the same meaning as [data]. It called a [sample] because it is assumed to come from a bigger data set the [population] of interest. 

#### saturated  model

A saturated  model is a model which has as many unkown parameters as the number of observations $n$ 

#### SBC

The SBC is another name for [BIC]. It stand for  the Schwarz  Bayesian Criterion for comparing fitted models.


#### scale parameter

#### selection of terms

A selection of terms in distributional regression is the process of identify which [term] affect which parameter. 

#### skewness

Skewness is a statistical measure indicating whether a sample [vector] is symmetric or not. Skewness is also applied to random vectors and therefore to theoretical distributions.

#### skewness parameter

#### supervised learning model

A [supervised learning model] is a regression model in which a [response] exist.

#### statistic

The term [statistic] refers to a functin of the [sample] ([data]). For example, the mean of a sample is a statistic $\bar{x}= \frac{1}{n}\sum_{i=1}^{n} x_i$. Stastistcs are used to calculated characteristics of the the teoretical distribution assumed for the random variable $X$. For example $\bar{x}$ can be ussed as estimate of the mean $\mathbb{E}(X)$ of the theoretical distribution assumed for $x$ i.e. $\mathbb{E}(X)= \int_{-\infty}^{\infty} X f(X) dx$, where $f(X)$ is the assumes ditribution for the [vector] $x$.  

#### statistical inference 

Is the process of drawing inference from the [sample] to the [population], from going to a specific case to the general situation. The path from to the spefic to the general can only be achieved using [assuptions] and therefore is very treacherous ans irrelevant if the assumtions are not right.    

#### statistical model 

A statistical model is a [mathematical model] containing a [stochastic] component. That is a stachatic model is build with [probability] statements on its [assumptions].  

#### statistical modelling

Statistical modelling is the art of creating a statistical model which represents the data generating mechanism adequately. Statistical model extract information from the data in a way to answer question of interest (see [purpose]).

```{mermaid}
%%| fig-width: 5
%%| label: fig-modelling
%%| fig-cap: "The classical modelling process which uses a single model and tries to improve it"
flowchart LR
  A[Data] --> B[Assumptions] 
  B --> C{one-Model}
  C -->|check| B
  C --> D[Interpretation & predict]
```
In a modern statistical modelling approach the data are partitioned to help detection of [over fitting] and prediction. @fig-modelling_modern show this new approach of statistical modelling. Instead of a single fitted model $B$ candidate [model]s are [fit]ted and subsequently aggregated (see [stacking]) before [model interpretation] and [model prediction].

```{mermaid}
%%| fig-width: 5
%%| label: fig-modelling_modern
%%| fig-cap: "The modern modelling process uses more models and aggregates them"
flowchart LR
  A[part. Data] --> B[Assumptions] 
  B --> C{fit K-Models}
   C -->|check| B
  C --> D[stacking]
  D --> E[interpretation & prediction]
```
The achieve this more  modern approach to statistical modelling the following technique could be used: [boosting] where a lot of week models are fitted to a single data set in such a way that the averaged models supplement each other. This is done by taking into the account the [residuals] of the previously [fit]ted [model]; [bagging] when $B$ models are fitted to $B$ bootstrap [data partition]s and the models are averaged; [stacking] when when $B$ different [model]s are fitted to a single data set (no partition) and the models are averaged.

#### stacking

The idea of [stacking] is to [fit] several [model]s to single [data] set but instead of performing a [model selection] to proceed by averaging the resulting models. The problem with [stacking] is that very rare the procedure leads to good [model interpretation].

#### stochastic

A [random] variable is [stochastic] if it has a probability distribution [pdf] associated with it. The term [stochastic] is equivalent with the rerm  [random]  

#### stochastic model

A [stochastic  model]  is a mathematical or an algorithmic model which incorporates randomness, interms of probabilistic statments.  The output of a [stochastic  model]is not completely predictable. Not all models need a stochastic component. A [mathematical model] or an [algorithmic model] donot have to be a stochastic model. A stochastic model contains [probability] assumtpions about one or more of his components. 

#### stochastic regression

A stochastic regression models contain [probability] assumptions on how the input-output model is generated. A typical assumption is the response is a function of the explanatory variables plus an [error], for example, $\textbf{y} = g(\textbf{x}) + \boldsymbol{\epsilon}$ where The minimal assumption for a regression model is about the behaviour of the  `response. Not all problems need a stochastic component.   Stochastic models are often used because many natural, social, and physical systems have an inherent variability.


#### smooth function

A [smooth function] $s()$ is a term in a [model] design to model non-linear relationships.  In a [regression] model those relationships are between the [explanatory variables] and the [response], in a [distributional regression model] is between the explanatory variables and a distribution parameter.  To simplify matters, consider the simple [regression] situation  when we have only one explanatory variable, $\textbf{x}$ and one response $\textbf{y}$. If we suspect that the relationship is not linear we couls try a [smooth] **non-parametric** [model] for the response:  $\textbf{y}= s(\textbf{x})+\boldsymbol{\epsilon}$. The function $s()$ is a general notation for a smoother. that is, a smooth function determined by tha [data]. It turns out that the **non-parametric** part is rather misleading because most of the smooth techniques are following this simple linear model  $\textbf{y} = \textbf{B}  \boldsymbol{\gamma} + \boldsymbol{\epsilon}$ where $\textbf{B}$ is a basis matrix constructed by the values of the explanatory vector $\textbf{x}$. The estimation of the linear parameter $\boldsymbol{\gamma}$ (the length of which could be as big as the length of the data) is done using a [penalised least squares] method with solution $\hat{\boldsymbol{\gamma}}=(\textbf{B}^{\top} \textbf{B}+\lambda \textbf{I} )^{-1} \textbf{B}\textbf{y}$. Note that if more that one smooth functions exist in model [formula] then the estimation of the smooth functions is achieved using a [backfitting] algorithm.     



#### smooth model

A smooth model is a model containing smooth functions.  For example the simpler smooth model is  $\textbf{y}= s(\textbf{x})+\boldsymbol{\epsilon}$.  

#### smoother 

A smoother is a [smooth] non-parametric function  design to model non-linear relationships see  [smooth function].

#### sufficient statistic

#### summary statistic

################################################################################

## T

#### target

The target variables is the [response] variable in a [regression analysis]

#### time series

A [time series] [data] set is a data set when observations were obtained over time and therefore that is a high probability that they are not independently distributed but there is autocorrelation between sequential observations. A [time series] analysis is a methodology to deal with a [time seriess] [data]. 


#### Tobit model 


#### training data

The training data are the data used in the [fit] stage of a modelling process. The contrast with the [test data] and the [validation data] sets.

#### test data

The test data are the data used to check the prediction power of the model.  


#### t-distribution


#### tabular data

Tabular data are  spreadsheet type of data sets. Tabular data   are rectangular in shape with variables vertically  and observations horizontally. We refer to the number of observations as $n$ and the number of variables as $r$.  Over recent years we have seen an increasing in the size of data sets for both $n$ and $r$. Traditionally classical regression models  supported  the cases where $n\gg r$, where $\gg$ refers to $n$ is much greater than $r$. More  recent regression  techniques i.e [LASSO] ans [PCR] support situation where $n  \simeq r$, or even $n<r$, Note that the notation $\simeq$  refers to $n$ is nearly equal to $r$".  A typical tabular data example is shown in @tbl-TheTableofData.


##### Tabular data example {.smaller}

| obs number | y      | x~1~    | x~2~    | x~3~    | ... | x~r-1~    | x~r~    |
|------------|--------|---------|---------|---------|-----|-----------|---------|
| 1          | y~1~   | x~11~   | x~12~   | x~13~   | ... | x~1r-1~   | x~1r~   |
| 2          | y~2~   | x~21~   | x~22~   | x~23~   | ... | x~2r-1~   | x~2r~   |
| 3          | y~3~   | x~31~   | x~32~   | x~33~   | ... | x~3r-1~   | x~3r~   |
| ...        | ...    | ...     | ...     | ...     | ... | ...       | ...     |
| n-1        | y~n-1~ | x~n-11~ | x~n-12~ | x~n-12~ | ... | x~n-1r-1~ | x~n-1r~ |
| n          | y~n~   | x~n1~   | x~n2~   | x~n3~   | ... | x~nr-1~   | x~nr~   |

: A tabular data example {#tbl-TheTableofData .striped .hover}


#### tail 

The word [tail] in [distributional regression]   framework is refer to the [tail]s of the distribution of the response.  The [tail] of a distribution is very important if the [purpose] of the study is extreme values or exceedance probabilities.


#### term

A [term] is one of the  [explanatory variables] in a [model],  after possible a [transformation] to make it suitable for [distributional regression] analysis. For example a [factor] is a [term].  A first order interaction is a [term]. 

#### test 

A [test] data set is the part of the [data] keep out of from fitting the [model] in order checking the predictive power of the model using a [measure of goodness of fit].  

#### training

A [training] data set is the [data] set in which the [model] is [fit]ted.


#### transformation

By [transformation] we mean usualy a [vector] transformation, that is, a transformation of one of the columns of the [data] in  something that it is more suitable for [statistical modelling]. In a [distributional regression] framework we rarely transforming  the [response] variable, since the variability of the response is part of its assumed distribution. For example if we beleive that the response is highly [skew] to the left we caassume the log-distribution for $Y$ i.e `LOGNO` or `logTF`.  Transformations in the $X$'s sometime do help the modelling process. Typically is if would suspect non-linearity in the relatioship and we would like ot use a [smoother] to model it ideally the  $X$s should  be equal spaced with no outliers. A [power transformation] could help this. In [centile estimation] when we are dealling with  a single $X$ variable (often `age`) a [transformation] of the  $X$ maybe is necessary to achieve a good [model]. [reference here $\ldots$ Rigby et al. 2026]. Note that the timing for creating a new [feature]  by  transforming $X$  could be achieved before or during the [fit]ting a [model]. For the formal case see the function `trans()` in the package `gamlss.prepdata` for the later case see Fasiolo at al. $\ldots$.           

################################################################################
################################################################################

## U


#### under fit

A [model]  under fits the [data] if it is not close enough to the data. The closeness is evaluated using a [goodness of fit] measure like [GAIC]. This happens if the model is not flexible enough and therefore misses important information in the [data].  See also [under fitting]


#### uniform distribution

#### univariate` distribution 

#### under fitting 

Under fitting occurs when the [fit] is very poor accoding to a [goodness of fit]  measure and it does not represent the [data] properly, see also [under fit]. 


#### unsupervised learning 

An [unsupervised learning] [model] also called a [classification model]  is a model which has [explanatory variables] but not [response]. The model tries to guess what class the response belong to given the [explanatory variables]. 

################################################################################
################################################################################
## V

#### validation data

The [validation data] set is used for tuning a model. That usullu taking the form of estimating the [hyper parameter]s of a model.   The [validation data] is an [out of bag] data set.

#### variable importance

A variable importance should show the important of a [term] in the [model]. That is, how much this term contributed in explaining the model. Note that for [distributional regression] a [terms] affect both the model of  [parameters] of the distribution but also the overall [measure of goodness of fit]. 
So, for example, one should be able to ask the question,  of how important say **age** is for $\sigma$  the variation the response  $Y$  but also how important is overall to the model compared with other terms that may influence other part of the model rather the scale parameter.  For [mathematical model] parametric [model]s the size of the estimated coefficients could provide an indication of how important a variable could be  but for other model could be more difficult. An [agnostic] method given in the literature (I think by Breimer) is to find the prediction power of the variables by scramble the variable of interest and compared it with the preditions ontain when the variable remain unscramble.This method couls produce indexes of importance both model parameter based and overall [measure of goodness of fit]     
     

#### vector 

################################################################################
################################################################################
## W

#### white noise

The term [white noise] refers to an identical distributed normally distributed variable. A random variable behaving as a [white noise] has no pattern. There should not be any autocorrelation or heterogeneity in the process.    

#### worm plot

A worm plot is a [diagnostics] tool applied to the [residuals] of a [regression] model but more often to [z-scores] of a [distributional regression]. A [worm plot] is a detrended [QQ-plot]. Detrended to highlight departures from normality. 


################################################################################
################################################################################
## X

#### $X$

$X$ is used in present glossary as a generic term for [explanatory variables], [input variables] or [independent variables] in a [regression analysis] but also is used to indicate a [random] variable.  


################################################################################
################################################################################
## Y

#### $Y$

$Y$ is used as a generic terms for the [response],  the [target], or the [y-variable] within [regression analysis] and [distributional regression].


#### y-variable

The [response] or [target] variable in a [regression analysis].

################################################################################
################################################################################
## Z

#### z-scores 
In a  [distributional regression] model the [z-scores] are the normalised [residuals] of the [model]. 
95% of  their values  the if the model is  [adequate fit] of the data should fail between -2 and 2. (More precisely between -1.96 and 1.96). Therefore [z-scores] for both [training] and [test] data can vbe use for dtaection of unusual obsrvations.  

################################################################################
################################################################################
