---
title: "A Glossary for Distributional Regression Models"
format:
  html: default 
  pdf: default 
number-sections: false   
number-depth: 3
editor: visual
quarto: add leovan/quarto-pseudocode
author: GAMLSS working party  

bibliography: book2026.bib       
---

## Introduction 

This is a glossary of terms and ideas related  to statistical models in general but more specifically to [distributional regression] models. The current glossary started  based on ideas presented in the talk [Regression Models; how to adapt for climate change challenges](https://mstasinopoulos.github.io/Brazil2025-talk/talk_2.html#/title-slide)  given by Mikis Stasinopoulos, University of Greenwich, to the XVII Encontro Mineiro de Statistica on Octomber 2025, in Lavras, Brazil.  It was soon realised that there was also a large gap between the [statistical modelling]
terminology and the terminology used by other  data analysis scientists. For example,  the [response] variable a well known expression to describe the variable of interest in a [regression analysis]  but it is called the [target] in [machine learning]. Similarly the [explanatory variables] are refer to as [feature]s. In this glossary we are trying to deal with those problrm by describing both definitions. We also decided to include at several places of the  glossary, `R` output if  we thought it would  facilitate the understanding  of the ideas and concepts presented here.


The following package are needed for explaining some of the concepts below; 
```{r}
#| warning: false
library(gamlss)
library(gamlss2)
```
We shall use the `rent99`  as a main source of a [data] set we  show it here; 
```{r}
#| warning: false
data(rent99)
```
Note that two of the variables in `rent99` are not needed so we they are taking them out.
```{r}
da <- rent99[,-c(2,9) ]
head(da)
```


# Glossary

################################################################################
## A

#### accumulated local effects 

Accumulated Local Effects, ALE, @apley2020visualizing, is a model [agnostic] technique for interpretatng  [term]s  in a [model].  It is showing how explanatory variable affects the predictions value of the model. It works  even with correlated explanatory variables, by analysing differences in predictions across local intervals rather than averaging whole predictions. ALE avoids issues with [multicollinearity] or collinearity by focusing on the change in predictions within local windows, therefore providing more accurate and  unbiased global explanations for   variable importance. 

::: {.callout-note}
example here 
:::
 

#### acf

The term [acf] stands for [autocorrelation] function.  The [acf] is a [diagnostics] tool for detecting [autocorrelation] in a [vector] $\textbf{x}$. In a [regression analysis] the [vector] is often the [residuals] of the [model].  It is used together with [pacf], the parcial [autocorrelation] function, to identify autocorrelation in [time series] data.    

```{r}
# #| warning: false
# library(gamlss.ggplots)
# y_acf(resid(m1))
```

#### additive smoothers 

Additive smoothers occurs when the contribution of one or more continuous [term]s [smoother] is added to the rest of the terms in the equation of a model For example the [formula] $s_1(x_1)+s_2(x_2)+ s_3(x_1, x_2)$, where the $s()$s  are different [smooth function]s will [fit] two main effects [smoother]s  for $x_1$ and $x_2$ and a first order interaction [smoother] for for $x_1$ and $x_2$.
```{r}
#| label: fig-m0
#| fig-cap: "The fitted smoothing terms for model `m0`."
library(gamlss)
#| warning: false 
library(gamlss2)
m0 <- gamlss2(rent~s(area)+s(yearc)+s(area, yearc), 
              data=rent99, famlily=GA) 
plot(m0)
```
Smooth first order interaction models have relatively easier [interpretation]. 

#### additive model

An additive model occurs when the contribution of each [term]sin a [model] is added to the rest of the terms in the [formula]. Let $x_1$ abd $x_2$  be two continuous variables and  $f_1$ and $f_2$ to be two [factor]s. The [formula] $y \sim b_1 x_1 + s_2(x_2) + f_1*f_2 + s_{12}(x_1, x_2, \texttt{by}=f_2)$, will fit a linear [term] for $x_1$ a [smoother] for $x_2$, a linear first order interaction for the [factor]s $f_1$ and $f_2$ and a varied coefficient [smoother] for the first order interaction  for  for $x_1$ a and $x_2$ varying according to factor $f_2$.   
```{r}
#| warning: false 
library(gamlss)
m1 <- gamlss2(rent~area+s(yearc) + location*cheating +
            s(area, yearc, by=cheating), data=rent99, famlily=GA) 
summary(m1)
```
The fitted smooth curves can be plotted by
```{r}
#| label: fig-m1
#| fig-cap: "The fitted smoothing terms for model `m1`."
plot(m1)
```
Note that an [additive model] is usually easy to [interpretation]  but more difficult to reach because one has to decide  which   [term]s should be included in the [model]. The [additive model]s need more thinking on how to reach them while a [machine learning] [model] let the computer to do the hard work.  Which we should trust best depends on the [purpose] of the study. My personal view in this that we should \textbf{not} let the computers decide when the life of people is stake see also [black box].  


#### adequate fit

We call a [model] an  [adequate  fit] if the [final model]  is  an  [adequate  model]. 

#### adequate model

We call a [model] an  [adequate  model] if the [model] represents the [data] well.  This can be checked through model [diagnostics] and [goodness of fit] measures. 
Within a [distributional regression] framework we have two types of [adequate model]s to consider. The first has to do of whether the [distribution] of the [response] is fitted adequately the second on  whether the chosen [term]s are adequate represent the relationships  in the [distribution parameters]. 


#### agnostic method

An [agnostic method] in [statistical modelling] is a technique which could apply to any [model] independently if they are [mathematical model]s, [agent based model]s or [algorithmic model]s.  Note, however, that a lot of the techniques claimed to be agnostic may depend on the [assumptions] of the model or on the type of [measure of goodness of fit] used to [fit] the model. The [assumptions] and the [measure of goodness of fit] are interconnected with the [model]. Strictly one should defined an [agnostic method] a techniques which is not associated  with any of them.

#### agent

An [agent] is the basic unit of interest in a [agent based model].

#### agent based model

An agent based model is a simulation, bottom up model, where the unit of interest is called an [agent]. The model is build by simulating a lot of times how the agents interact between them within a given environment. The [agent] is the basic unit of interest and its behaviour is studied using simple mathematical or logic rules. The behaviour of the agent is studied after a long simulation exercise. Therefore important part of the model is how to set the parameters determine the [agent] behaviour.  


#### AIC

[AIC] stands for the Akaike Information Criterion, @Akaike73b. [AIC] it is a measure of [goodness of fit] used to  compare different fitted models. The [AIC] is defined as [deviance] $+2\times df$  where $df$ are the [degrees of freedom] and 2 stands for the **penalty**, see also [GAIC] and [BIC]. The [model] with the minimum  [AIC] is the best.


```{r}
AIC(m0,m1)
```

#### ALE 

[ALE] is an abbreviation for  [accumulated local effects] 

#### algorithm 

An algorithm is a step-by-step computational procedure designed to perform a specified task. For example in a  [input-output model]s when typically we have $Y=g(X)+\epsilon$, the task is mainly to  find the unknown function $g()$, which connect the input with the output. 

#### algorithmic model

 An algorithmic model is a model based on an algorithm. In [regression] typically an algorithmic model trying to model  $X \rightarrow Y$, through an unknown function  $g()$ i.e. $Y=g(X)$.   No explicit assumptions are made for the unknown function $g()$  but a lot of implicit [assumptions] depending on the type of algorithm used. Note that an [algorithmic model], like a [mathematical model], can be a deterministic or an [stochastic model]. The [stochastic model] can be written as  $g()$ i.e. $Y=g(X)+ \boldsymbol{\epsilon}$ where the last term in the equation, the [error] is a [random] variable and the stochastic part of the [model].

#### Archimedean copulas

The [Archimedean copulas] are  a wide class of families of copula  with subclass; 

   -  the [Clayton copula] which cal model strong lower tail dependence, that is, useful when joint small values matter.
   
   - the [Gumbel copula]  supporting strong upper tails and its useful for modelling extreme values.
   
   - the [Frank copula] with symmetric dependence and not fat tail dependence.
   
   - the [Joe copula] with strong upper tail dependence and more flexible than [Gumbel copula]

  - the [extreme value copulas] suitable for modelling jointly  extreme values. (Gumbel–Hougaard, Galambos, Hüsler–Reiss) Popular in hydrology, climate extremes and Risk analysis.
  
  
  - the [vine copulas] (or pair copula constructions) whicjh are highly flexible, built from bivariate copulas with types: i) C-vines copula ii) D-vines copula and iii)	R-vines copula. All have the advantages that handle high dimensions couls mix different copula families and have very flexible dependence structures. They are widely in Finance, Insurance and	Machine learning.

- the [hierarchical Archimedean copulas] (HAC) which allow nested Archimedean copulas, clustering of dependence and in genaral are more flexible than single [Archimedean copulas]	


- the [empirical nonparametric copulas] Empirical copula and Bernstein copula
with advantages of fewer assumptions and [data]-driven.


- the [special purpose copulas] like the [Plackett copula] the 	[FGM copula] (Farlie–Gumbel–Morgenstern) whicj allow limited dependence and the  Marshall–Olkin copula with  asymmetric dependence.

::: {.callout-note}
More work needed here
:::

#### assumptions

Assumptions are axiomatic statements needed to be accepted for the model to work. Models need assumptions because of their simplified nature. The reasoning behind assumption is that if the assumptions are correct then the model can be useful. There are **explicit** and **implicit** assumptions. The **explicit** assumptions  are usually mathematical ones and could be  easily checked using [diagnostics] tools. The  **implicit** assumptions common in [algorithmic model]s are more difficult to check. **Incorrect** assumptions could lead to questionable scientific discoveries.  For an [agent based  model] the assumptions are the different ways  the agent are behaving.


::: {.callout-note}
maybe assumptions  of the linear model here 
:::

#### autocorrelation

The term [autocorrelation] in a vector $\textbf{x}$  implies that sequential values of $\textbf{x}$ depend on previous values. Autocorrelation is very common on [time series] data and can be detected using [acf] and [pacf] functions. 

In [distributional regression] one should check whether the [z-scores] are autocorrelated. This can be done using the  `resid_plots(., theme="ts")`: 

```{r}
#| warning: false 
library(gamlss.ggplots)
resid_plots(m1, theme="ts")
```

#### averaging models

The  [averaging models] idea is is when we select a [final  model] by averaging different [fit]ted [model]s. 
If the fitte. There are a lot of ways to average [model]s see also [model average] and [stacking].


::: {.callout-note}
example? 
:::
################################################################################
## B

#### backfitting 

The [backfitting] algorithm is an algorithm discussed by  @HastieTibshirani90 for fitting more  that one smoother in a [formula]  for a [model] see also Chapter 3 of @Stasinopoulosetal2017.

#### bagging 

Bagging refers to [bootstrapping] followed by an [averaging models] procedure. That is. when $B$ models are fitted to $B$ bootstrap [data partition]s  and the resulting models are then averaged. 


#### Bernoulli distribution

The [Bernoulli distribution] is special case of the [binomial distribution] when $n=1$. A Bernoulli [random] variables has range of values  $0$ or $1$, expected value $\mathbb{E}(Y)=p$ and variance $p(1-p)$.
@fig-bernoulli show three realization of the [Bernoulli distribution] at different values of $p=(0.1,0.3,0.5)$
SOMETHING IS WRoNG WITH IT the p should be in the first line not the second


```{r}
#| fig-width: 4
#| fig-height: 4
#| label: fig-bernoulli
#| warning: false
#| fig-cap: "The Bermoulli distribution with differenrt values of p=(0.1,0.3,0.5) and n=1"
library(gamlss.ggplots)
family_pdf(BI, mu=c(0.1,0.3,0.5), to=1, title="Bermoulli p=c(0.1,0.3,0.5) an n=1")
```

::: {.callout-note}
Is there a bug in the function?
The probabilities should be the other way around. 
:::

#### BIC 

BIC is the Bayesian information criterion of @Schwarz78. Within a [distributional regression] model the BIC can be used to  compare different fitted models.  The BIC is also refer to also as  [SBC], Schwarz  Bayesian Criterion. The BIC is defined as [deviance] $+ \log(n) \times DF$, when $n$ is the number of observations in the [data],  and $DF$ are the [degrees of freedom].  See also [GAIC].


#### binomial distribution

The pdf of a binomial distributed [random] variable is:
$$f(Y|p)= \binom{n}{p} p^{p} (1-p)^{n-p}$$
where $Y$ takes values in the [range] $0,1,2,\ldots,n$, $n$  is called here the **binomial denominator** and the parameter $p$ is a probability parameter taking values $0<p<1$. Note that $\binom{n}{p}$ can be written also as;   
$$ \binom{n}{p} =\frac{\Gamma(n+1)}{\Gamma(y+1) \Gamma{(n-y+1)}}$$
For more details see pp. 521-522   of @Rigbyetal2019. The expected value of the distribution is $\mathbb{E}(Y)=np$ and the variance is $\mathbb{V}ar(Y)=np(1-p)$. @fig-binomial show the three different binomial distributions with binomial denominator equal to 10 and different probabilities at   

```{r}
#| label: fig-binomial
#| fig-cap: "The binomial distribution with different values of p=(0.1,0.3,0.5) and n=10"
#| warning: false
library(gamlss.ggplots)
family_pdf(BI, mu=c(0.1,0.3,0.5), title="binomial p=c(0.1,0.3,0.5) and n=10")
```
Note that in the `gamlss.ggplots` documentation  the parameter of the [binomial distribution]  called here as $p$ is denote as `mu`. The binomial denominator in the function  `family_pdf` can be specified in by the argument `to`. In @fig-binimial  it take the default value which is 10.  


#### black box

A black box is a [model] with difficult interpretation. A lot of machine leaning models are black boxes. There are two reason for black box model i) the function  $g()$ which the black box tries to estimate is too complex to explain. ii) there are proprietary reasons. The [question] of the problem should determine whether a [black box] [model] is appropriate.  @rudin2019stop argued  "stop explaining black box machine learning models for **high stakes decisions** and use [interpretable model]s instead".  The argument came from the fact that among all [adequate fit]ted models in the [Rashomon set] few are interpretable [@rudin2024amazing].



#### bootstrapping

The method of [bootstrapping] is a way to fit multiple models to a single [data] set by repeatedly re-sampling with replacement from the original [data] set. The multiple [fit]s can be used  to obtain variability of the [parameters] in the model. In this sense bootstrapping  complement Bayesian fits where the information about the variability comes from prior assumptions rather than the bootstrap replications. Bootstrapping is based on [data partition] with replacement for $B$ times so the different fits could produce $B$ [estimates] of [fitted values] and [residuals]. Averaging those values sometimes refer to as [bagging]. A good  book  about [bootstrapping] is @EfronTibshirani93.


#### boosting

The method od [boosting] is a way of [model fitting] using  many **complementary** sequential simple models and average them in order to build a unique [fit]ted [final model]. The individual models should be simple and easy to fit and should complement each other by modelling different characteristic of the [data]. Finally the individual [fitted] simple [model]s contribute when averaging only to a small part say 10 or 20 percent of their original contribution. This percentage is one of the [hyper parameter]s of the boosting method. The other [hyper parameter] is how many time we should repeat the fitting procedure in order to reach an [adequate fit]ted model. The latest is the main smoothing parameters of the [boosting] procedure. For determine the smoothing parameters [cross validation] can be used which requiring a good [measure of goodness of fit].

#### cross validation

The idea behind [cross validation] is that the [model]  performance is validated on [data] that were not be  used to fit the [model] that is, the [training] data.

#### bucket plot

A [bucket plot], @de2022bucket, is a [diagnostics] toot which can detect [skewness] and [kurtosis] in any [vector] in the [data] but more important in the [residuals] ([z-scores]) of a [distributional regression] [model]. Detection of  [skewness] and [kurtosis] in  the residuals of a [distributional regression] [model] amounts to detection of [skewness] and [kurtosis] in the [response] variable. 
There two types of [bucket plot]s: i) the first is based on [moment] [summary statistic]s; the second on [centile] based [summary statistic]s. The functions `moment_bucket()` and `centile_bucket()` of the package `gamlss.ggplots` can be used respectively to produce a [bucket plot], in `R`. 

@fig-mom_bucket shows a [moment] [bucket plot] of the model `m0` fitted earlier in the section [additive smoothers] where two [smooth function]s [main effect]s `s(area)+s(yearc)` and one [first order interaction], `s(area, yearc)`,  were fitted to the [response] Munich `rent` taken  from the [data.frame] `rent99` using a Gamma (`GA`) distribution.   

The moment based [bucket plot] in @fig-mom_bucket plots the transformed [moment] [skewness] of the residuals of the model `mo` against the   transformed [moment] excess [kurtosis].  



```{r}
#| label: fig-mom_bucket
#| fig-cap: "The moment bucket plot of the residuals of model `m0` "
#| warning: false
library(gamlss.ggplots)
moment_bucket(m0)
```


```{r}
#| label: fig-cen_bucket
#| fig-cap: "The centile bucket plot of the residuals of model `m1` "
#| warning: false
centile_bucket(m1)
```

Here we up data the distribution of the response from the two parameter `GA` to the four parameter `BCTo`.
```{r}
#| label: fig-model_mom_bucket
#| fig-cap: "The moment bucket plot of the reiduals from models `m0`, `m1` and `m2`.  "
#| warning: false
m2 <- update(m1, family=BCTo)
model_mom_bucket(m0, m1, m2)
```

::: {.callout-note}
This needs more work
:::

################################################################################
## C

#### centile

A [centile] value is defined as $$100 \times \text{quantile}.$$ Since a [quantile] is taking values from 0 to 1 a [centile] is defined in the interval  0 to 100 for example  the [quantile] at $0.50$ or 50% [centile] is the [median].   In practice the term [centile] is used as synonymous to [quantile]. A [centile]/[quantile] is a characteristic of both of the [distribution] of a [random] variable and  of a [vector] in the [data]/[sample]. Centiles in the leter case can be defined through the [empirical cdf].  


#### centile estimation

Centile estimationis another term for [reference curves] fitting.  

#### censored response

A [censored response] is a [response] variable which could takes [interval values],  

####  censored values 

A [random] variable which we do not know its exact value but  that only that it have been fallen  within a known interval it said to have [censored values] or [interval values].  

#### cdf 

A cdf is the **commutative distribution function** of a random variable $Y$, usually denoted as $F(Y)$. The first derivative of the cdf define the probability distribution function, pdf of $Y$ i.e.  $f(Y)=F'(Y)$ For a [distributional regression] where the [response] is assumed to have a proper distribution [pdf]  he [cdf] is usually refer to as the commutative distribution function of the response.   

#### Clayton copula

The [Clayton copula] for two random variable $u_1$ and $u_2$ both taking values in the [range] $[0,1]$ is defined as; 
$$C_\theta(u_1,u_2)
=
\left(u_1^{-\theta} + u_2^{-\theta} - 1\right)^{-1/\theta}$$ where $\theta$ is the parameter of the copula.
The [Clayton copula]  belongs to  to the [Archimedean copulas] family. It models only positive dependence and only lower tail and its asymmetric  with stronger lower tail.
The interpretation of the parameter $\theta$ is when $\theta = 0$ implies independence, larger $\theta$ stronger dependence. It  emphasises joint small values and it is related with Kendall’s $\tau$:
$\tau = \frac{\theta}{\theta + 2}
\quad \Rightarrow \quad
\theta = \frac{2\tau}{1-\tau}.$


#### coefficients

The [coefficients] are [parameters] in a [model]. In [distributional regression] we  refer to as  [coefficients] when they are the [parameters] describing the relationship between the [distribution parameters] and the [explanatory variables].

#### continuous distribution 

A [continuous distribution] is a distribution function, [pdf], of a [random] variable $Y$ taking values in the real line. The only property a [continuous distribution] has to have is that if is integrated with respect to the random variable it should add up tp one i.e. $\int_{-\infty}^{\infty} f(Y)dy=1$. There are three types of continuous distributions depending on the [range] of the continuous random variable $Y$;

i) $(-\infty , \infty)$, continuous in the real line;
ii)  $(0, \infty)$,  continuous in the positive real line;
iii)  $(0 , 1)$  continuous on a finite  interval in the real line;

Next we show plots the different types of continuous distributions. We start with the normal distribution in  which has two parameters $\mu$ the mean and $\sigma$ the standard deviation and which is defined  on $(-\infty , \infty)$  
```{r}
#| label: fig-normal
#| fig-cap: "The normal distribution with differenrt values of sigma=(1,1.5,2) and mu=0"
#| warning: false 
library(gamlss.ggplots)
family_pdf("NO", mu=c(0, 0, 0), sigma =c(1,1.5, 2), from=-5, to=5,title="Normal with mu=(0,0,0) and sigma=(1,1.5,2)")
```
Similarly the gamma distribution in @fig-beta,  has two parameters  $\mu$ the mean and $\sigma$a scale parameter but its defined on the positive real line $(0, \infty)$:
```{r}
#| label: fig-gamma
#| fig-cap: "The gamma distribution with differenrt values of mu ar 1 and sigma=(1 ,0.8, 1.5)."
#| warning: false 
library(gamlss.ggplots)
family_pdf("GA", mu=c(1, 1, 1), sigma =c(1,0.8, 1.5), from=0.01, to=5,
        title="Gamma with mu=(1,1,1) and sigma=(1,0.8, 1.5)")   
```
The beta distribution in @fig-beta, is defined on the real line $(0, \infty)$ and  two parameters  $\mu$ the mean and $\sigma$ a scale parameter:
```{r}
#| label: fig-beta
#| fig-cap: "The beta distribution with differenrt values of mu=(0.2, 0.5, 0.8) and sigma=(0.2 ,0.8, 0.3)."
#| warning: false 
library(gamlss.ggplots)
family_pdf("BE", mu=c(.2, .5, .8), sigma =c(0.2 ,0.8, 0.3), from=0.01, to=0.99,
           title="Beta with mu=(.2, .5, .8) and sigma=(0.2,0.8, 0.3)") 
```
As rule the more parameters exist in a [distribution] the more flexible the [distribution] is. 

Note that any random variable in the interval $(\alpha, \beta)$ can be transformed to a distribution in the interval  $(0 , 1)$. Also note that  any distribution with [range]  $(-\infty , \infty)$ can be **log** or **logit** transformed  to a distribution in the positive real line  $(0, \infty)$ or  to the fine interval $(0 , 1)$, respectively.  


####  characteristics of the distribution

Each [distribution] has its own properties. We called those properties the [characteristics of the distribution]. Within a [distributional regression] framework and for better [interpretation]  we should be ware   with both the properties of the [distribution parameters] but also other  [characteristics of the distribution] itself. The  [characteristics of the distribution] can be defined  i) using [moment]s or ii) using [quantile based measures].


#### quantile based measure

A [quantile based measure] is defined as a [characteristics of the distribution] based on the quantiles  rather [moment]s.




#### classification model

A [classification model] is an [unsupervised learning] [model]  in which the output that is the [response]  is a  [factor] with unknown  categories. The model tries to guess what is the best possible value for the response given the [explanatory variables]. The models for  [distributional regression] are all [supervised learning model]s in which a [response] exist  in the [data].  


#### copula

The [copula] methodology is a way of constructing a [multivariate distribution] binding its [marginal distribution]s with a [copula]. The methodology is  based on [Sklar’s theorem]. It is a convenient way of introducing multiple responses in a [distributional. regression] [model].

While [copula] can be defined in higher dimesions here for clarity we are concentrate in 2-dinemseions. Let $X_1$ and $X_2$ two random variables with marginal [pdf] distributions $f_1()$ and $f_2()$ and marginal [cdf]s $F_1()$ and $F_2()$, respectively.   Using the [Sklar's theorem] the joint [pdf] of the variables $X_1$ and $X_2$ can be written as $$f(X_1, X_2)= C[F_1(X_1, F_2(X_2)] f_1(X_1) f_2(X_2),$$ where the function C() is a [copula], that is, a  [cdf] function of two [random]  variables $u_1=F_1(X1)$ and $u_2=F_1(X2)$ defined in the  [range] of the two dimensional cube $[0,1]^2$.  Note that for the construction of the [likelihood] function of the joint distribution of two random   [random] variables $X_1$ and $X_2$, assuming that the [pdf]'s and the [cdf]'s are easy to evaluate, we only need the tyoe of the copula function  $C()$ to use. 

::: {.callout-note}
We need to say someining about the types, the generalazation  to motre than two  dimensions , [vine copula] and minimal and maximal copula
:::


There are different types of copula; 
i ) [elliptical copulas]  ii) [t copula] iii) Archimedean copulas 
	
In general when someone tries to construct a [multivariate distribution] using copula she/he should look the  tail behaviour, symmetry, dimensionality and	interpretability of the copula.


#### correlation coefficient


A [correlation coefficient] is a pairwise [measure of association] between two [continuous variables] in the [data].  
@brito2025exploring


#### measure of association

A [measure of association] is a [statistic] describing the pairwise connection between variables in a the [data].  Note that there are differnt types of [measure of association]  dependenting on the type of the [vecror]s involve. For exaple association between 

- **continuous** against **continuous** variables

- **continuous** variable against **categorical** variable

- **categorical** variable against **categorical** variable


```{r}
#| fig-width: 5
#| label: fig-association
#| fig-cap: "Poirwise measures of association in the Munich `rent99` data set." 
library(gamlss.prepdata)
data_association(da, method = "circle")
```


#### count data distribution

See [discrete distribution]


#### continuous rank probability scores

$\dots$

#### CRPS

The expression [CRPS] is an abbreviation for [continuous rank probability scores] which are a [measure of goodness of fit] appropriate for [distributional regression] [model]s.

################################################################################
## D

#### data

The term [data] is  a generic term to describe object which contain some kind of [information].
It  used to mean a file with many numbers, but [data] today could have different forms. Could be **texts**, **pixels**, or any other form containing some [information] to be extracted . The [model] tries to extract the [information] from the [data]. Note that at a pre-[statistical modelling] stage we can still extract useful information from the [data] to actually help us with  the statistical modelling. The package `gamlss.prepdata` is design to help in this direction. Within  [distributional regression] [model]s most of the data used are  [tabular data].
Note that [data] sets in the statistical programming language `R` are refereed to as [data.frame]s.  

#### data.frame 
   
A `data.frame` is the way a [data] set is refer to in the `R` statistical programming language.   


#### data partition

The term [data partition] has two different meanings.  The fist has to do with [statistical modelling] the second on looking at whether the variables involved behave differently at different parts of the [data].

Data partition in the process of [statistical modelling] helps the model building process, [model] [interpretation], the checking for [over fit] and  also helps to improve the [statistical inference] by providing extra [information] about variations in the parameters. There are  different types of data partition;
 a **single** partition of a data set provides holdout samples for prediction and validation purposes. A **multiple** partitions as for example [bootstrapping] and  [K-fold cross validation]  to help inference. 
```{mermaid}
%%| label: fig-data_split
%%| fig-cap: Different ways of partitioning data in order to get more information aboout the model."
flowchart TB
  A[Data] --> B{Holdout} 
  A --> C{K-fold-CV}
  A --> D{Bootstrap}
  B --> E[Training]
  E --> F[Validate]
  F --> G[Test]
  D --> H[Non-parametric]
  D --> K[Bayesian]
  C -->  L[LOO]
```

 The second meaning of [data partition] tries to answer the following question. Is there a way to partition the data in such a way that the relationships between the variables in the [data] behave differently?; or more generally can I divide the [data] in such way that different models can apply to different parts of the [data]?.  Models like [regression tree]s try to separate the [data] in such a way that the resulted subclass are homogeneous by fitting differenr constant models withing each part.. $\ldots$ 
 
#### data analysis

Data analysis is the art of extraction information from the [data]. In any [data analysis] the first question any researcher has to ask is whwther the [data] or the [model] could answer the [question] in hand. 

#### data generating mechanism

The [data generating mechanism] is a set of assumptions on how the [data] are generated. It is often difficult to check those [assumptions] because different [data generating mechanism]s could possible  lead to similar results. Thet is a have a [Rashomon effect] on the bdata rather than the. model. In [distributional regression] often the [data generating mechanism] is an assumption abouth the distribution of the  [response] given the [explanatory variables].    


#### degrees of freedom

The [degrees of freedom] of a model measure the complexity of the model. For mathematical parametric models the degrees of freedom are the number of independent parameters used in the model. For mathematical [smooth model]s the degrees are defined as the diagonal elements of the smoothing matrix i.e. $\hat{\textbf{y}}= S(\textbf{x})$ where $\hat{\textbf{y}}$ are the [fitted values] of the smooth function and $\textbf{x}$ is the [explanatory variables]. For algorithmic models the degrees of freedom often are difficult to calculate.       


#### design matrix

By [design matrix] we usually refer to the matrix $\textbf{X}$ containing  all relevant explanatory variables. Note that for distributional regression model each paramerter of the distribution could have its own design matrix i.e.   $\textbf{X}_{\theta_k}$    

#### deviance

The [deviance] is a measure of goodness of fit and defined as $-2 \ell(\boldsymbol{\theta})$, that is, minus twice the [log likelihood]. The  [deviance] can be used  for model comparison.The  [deviance]  can be defined in both the [training] or the [test] [data] set so we can have a [training] or [test] [deviance].  In [GAMLSS] modelling the [training] [deviance], is  used  with its penalised form the Generalise Akaike Information Criterion, [GAIC],  for model comparison. The deviance can be seeing  as an [empirical risk] measure based on [information criterion] concepts. It is a [summary statistic], because it is a sum of the individual deviance increments and therefore an overall measure of goodness of fit. It tell us how well the conditional distribution of $D(Y|\boldsymbol{\theta})$  fits the [data] overall. It does not tells whether the tail or the middle of the distribution is fitting well. The [deviance increment]s may help to obtain such information. 

#### deviance increment

#### density function

At a theoretical level a [density function] is the [pdf] of a [random] variable. At the [sample] level a density function of a [vector] in the [data] is an empirical distribution function of a variable in the [data] obtain using smoothing techniques. In thios case the [density function] is a smooth [histogram].


```{r}
#| label: fig-hist-density
#| fig-cap: "An estimated density funtion on top of a histogram for the variable the price of `rent` in the Munich `rent99` data set." 
#| warning: false 
library(gamlss.data)
library(gamlss.ggplots)
y_hist(rent99$rent)
```

#### diagnostics 

The diagnostics are statistical or graphical tools for helping to check the [assumptions] of a fitted [model]. Within [distributional regression]  [diagnostics] can help to identifying whether the [distribution] of the [response] is fitted well or whether the [term]s in the  different [distribution parameters] models are fitted appropriately.  

#### discrete distribution 

A [discrete distribution] is distribution function [pdf] of a [random] variable which takes only integer values i.e. $0,1,2,\ldots, 10$. Another name for a discrete distribution  is [count data distribution]. 
The only property that any discrete distribution must have is that it should sums up to one. $\sum_{i=1}^{\infty}  f(y_i)=1$. There are mainly two types of [discrete distribution]s depending on the [range] of the discrete [random] variable involve i) infinite count ii) finite count [distribution]s.  The most common example of an infinite count distribution is the [Poisson distribution] and the most common example of the finite count is the [binomial distribution]. 

#### distribution

The term [distribution] refer to a probability distribution function  [pdf],  $f(Y)$. The only property for $f(Y)$ is required is that it should sum up to one.  In distributional regression we write the [pdf]  of the [response] as $D(Y | \boldsymbol{\theta})$ not $f()$ a notation which could be used for describing the relationships between the [explanatory variables] and the [response]. We are also  emphasising  that the [pdf] of the response is a function the different [distribution parameters], $\boldsymbol{\theta}$.  The  function $D(Y | \boldsymbol{\theta})$ describes how the behaviour of the [response] variable is affected by the [explanatory variables] and it is the main [stochastic] component of a [distributional regression]. 


#### distribution parameters

Within a [distributional regression] we use the  term  distribution parameters as almost synonymous to [distributional parameters] to refer to the parameters $\boldsymbol{\theta}$ in the distribution of the [response] $D(y|\boldsymbol{\theta})$, see also [distributional parameters].

#### distributional parameters

Theoretical distributions depend on [distribution parameters] $\boldsymbol{\theta}$ where $\boldsymbol{\theta}=(\theta_1, \theta_2, \ldots, \theta_K)$ is a [vector] of length $K$. The more (independent) [distribution parameters] exist in a [distribution] family the more flexible the [distribution] is. For example 
one parameter distributions can only model one characteristic of the [distribution] of the [response] mainly the [location parameter].

#### distributional regression 

A distributional regression model is an [input-output model]  model in which the response $Y$  is assumed to have a distribution in which all the parameters of the distribution could depend on explanatory variables. This can be represented as $X \rightarrow D(Y|\boldsymbol{\theta}(X))$ where $\boldsymbol{\theta}$ are the k parameters of the distribution. 

#### dummy variables

A dummy variables is a set of binary (0 ,1) vectors indication whether certain conditions are present or not. Any `factor` is represented as a set of dummy variables in a `design matrix` of a model. The number of columns in the dummy variable set representation of a factor are the number of `levels` of factor minus one to avoid collinearity with the constant a vector of ones in the design matrix.   


################################################################################
## E

#### empirical cdf

The [empirical cdf], [ECDF] is the cumulative function function of any continuous or ordered categorical variable in the [sample], ([data]). The [empirical cdf] is a step function that shows the proportion of observed [data] points less than or equal to a specific value in the [data] . It creates a cumulative probability distribution directly from a vector say $\textbf{x}$ in the [data], without assuming a theoretical model therefore is the source of a many non-parametric procedures in statistics. Any [vector]  $\textbf{x}$ in a [tabular data] set can have an [empirical cdf] but it is more  difficult to justify it  for unordered categorical variables since it implies an explicit order in $\textbf{x}$. For a given value $x_o$ in   $\textbf{x}$ for any continuous or ordered categorical variable in the [data] the [empirical cdf], [ECDF] gives the percentage of [data] points in $\textbf{x}$  less than or equal $x_o$ for example,  #$[\textbf{x} \le x_o] \times \frac{1}{n}$. 
@fig-ECDF-continuous shows the ECDF curve of the continuous variable `rent` from the `rent99` data set. 

```{r}
#| warning: false
#| label:  fig-ECDF-continuous
#| fig.cap: "The ECDF of a continuous variables. "
library(gamlss)
plot(ecdf(rent99$rent), main="ECDF", xlab="rent")
```
@fig-ECDF-ordered shows the ECDF curve of the oerdered categorical variable `location` from the `rent99` data set. 
```{r}
#| fig-width: 5
#| label:  fig-ECDF-ordered
#| fig.cap: "The ECDF of an ordered categorical variables."
plot(ecdf(rent99$location), main="ECDF", xlab="rent")
```
The [empirical cdf] is also defined on the [residuals] of a fitted [model]. In [distributional regression] if the model represent correctly the [data generating mechanism] then the [residuals] of a [distributional regression] [model] should be behave as a [white noise]. In this case the empirical cumulative distribution function of the residuals should be very close to the cumulative distribution function of the Normal distribution.
Next we fit a GAMLSS model using the `gamlss2()` function to the `rent99` [data]. We use the [formula] `rent~s(area)+s(yearc)+location+kitchen+kitchen` for the $\mu$ model and the [formula] `s(area)+s(yearc)` for $\sigma$ and we fit a [BCT]o distribution. The [ECDF] of the residual of the fitted [model] should behave, if the model is a [adequate fit]   as normally distributed [random] variables. @fig-ECDF-resid_normal show the  cdf of the [residuals] with a normal [cdf]  superimposed in red. From the plot there is no reason he doubt the adequacy of the model. Note that the same information could be extracted by plotting the [QQ-plot] or the [worm plot] of the residuals. 

```{r}
#| fig-width: 5
#| label:  fig-ECDF-resid_normal
#| fig.cap: "The ECDF of the residuals from a GAMLSS fitted model with the normal cdf superimposed."
#| warning: false
library(ggplot2)
library(gamlss.data)
library(gamlss2)
m1 <- gamlss2(rent~s(area)+s(yearc)+location+kitchen+kitchen|
                s(area)+s(yearc), family=BCTo, data=rent99 )
library(gamlss.ggplots)
gg <- resid_ecdf(m1)
gg+ggplot2::stat_function(fun = pNO, args=list(mu=0, sigma=1), col="red")
```


#### ECDF

See [Empirical cdf] 


#### elliptical copulas 

The [elliptical copulas] are based on the multivariate normal distribution and they can capture linear dependencies but no extreme tails


#### empirical copulas


#### empirical risk

The [empirical risk]  function is defined by dropping the expecations from the definition of the [risk] function, The definition of the  [empirical risk] is given in [risk] as;
$$\mathbb{ER}(g) = \frac{1}{n}\sum_{i=1}^n [ \ell oss(\hat{g}(x_i), y_i).$$ 
where $\ell oss()$ is the [loss] function.


#### entropy

The [entropy] for a discrete [random] variable $X$ is defined as: 
$$
\begin{split}
H(X) & = -\sum_{x \in X} p(x) \log p(x), \\
     &=  - E_p \log p(x) ,\\
     &= E_p \log \frac{1}{p(x)}
\end{split} $$  
That is, the [entropy] for a discrete random variables $X$ is equal to its expected value of minus the its log [probability]. Note that this quantity  is identical to the [log likelihood] (but the [log likelihood]  it is a function of the parameters and not the [random] variables $X$).   The [entropy] of a continuous random variable $X$ is given as:
$$\begin{split}
H(X) &=-\int_{X}f(x) \log f(x)  dx \\
      &= - E_f \log f(x)  \nonumber \\
     &= E_f \log \frac{1}{f(x)}  \nonumber 
\end{split} $$
The [entropy] is a [summary statistic] describing the randomness of the distribution of a [random] variables and it is a function of probabilities **not** the values or the [range] of the [random] variable $X$. The highest value of entropy is when we have equal probabilities for all values of $X$'s. That is when the $X$ has a [uniform distribution].  Note that the minus the log likelihood of a fitted [distributional regression] [model] which is defined as $- \ell(\boldsymbol{\theta})=\sum_{i=1}^{n} - \log f(y_i|\boldsymbol{\theta})$ is proportional to the  empirical estimated of the  entropy of the [response] variable with the constant of proportionately equal to $\frac{1}{n}$. This shows the close relationship between the concept of [entropy]  and the concept of the [log likelihood] important for statistical [inference]. 


#### error

The term [error] of a [model] is the component which  makes the model  [stochastic], that is, it the model contains a set of  [probability] elements. In [regression analysis] the [error] is often a terms used to describe the residual variation of a model. 


#### errors

The term [errors] refers to the statistical part of a [stochastic] model which accounts for the natural variability of the data. The [errors] often refer to the [residuals] part of the [model] that is what is left after the model has been applied.  


#### estimates 

Let us assume that the [model] is specified by the [parameters] $\boldsymbol{\theta}$, After a [fit] we have [estimates] for the $\boldsymbol{\theta}$ which often present them with a hat $\hat{\boldsymbol{\theta}}$.

####  exceedance probability

By [exceedance probability] we refer to the probably that  a values of a [random] variable, $Y$, will exceed a certain pre specified value $c$, that is  $Pr[Y \ge c.$ In fact if you know the [cdf], $F(.)$ of the random variable $Y$ its  [exceedance probability] is defined as $S(Y)=1-F(Y)$ where the function $S()$ often call in statistics the [survival  function].    


#### explanatory variables 

Explanatory variables in an [input-output model] are the input variables $X$. In a [regression analysis] are the variables in which  affect the response.  In a [distributional regression] are the variables on which the distribution of the response is conditioned on.

#### exponential family

The [exponential family] is a family of theoretical [distribution]s with  the propety of allowing [sufficient statistic]s 


#### expected value

 The [expected value] (the [mean] or first [moment]) of a [distribution] is defined as $\mathbb{E}(Y)=\int Y f(Y)dY$  for [continuous distribution]s and  $\mathbb{E}(Y)=\sum Y f(Y=y)$  for [discrete distribution]s where the ingration of summetion, repsectively, is over the [range] of the [random] variables $Y$.

#### extreme value copulas


################################################################################
## F

#### factor

A [factor] is  a [categorical variable] in the [data]/[sample]. A [factor]  is a [vector] column in the [data] set which takes only limited values which rae called the [level]s. The [level]s  can be **unordered**  or **orderer**.    

The function `str()` in `R` show the tyoe of [vector]s in the [data]. 
```{r}

```


#### fit

By [fit] we mean fitting a [model] to the [data] by minimise a [measure of goodness of fit] to select appropriate values for the [parameters] of the model. As a final results of the [fit] we have [estimates] fo the [parameters]. A single [fit] could provides unique [fitted values] and [residuals] for both [training] and [test] data sets. 


#### fitted values

In [distributional regression] we use the expression [fitted values] to indicate  estimates for the [distribution parameters] $\boldsymbol{\theta}$. In each fitted model there are as many [vector]s (of length $n$) of [fitted parameters] as the number of the assumed [distribution parameters]. 

To extract in `R` the fitted values of a fitted `gamlss2` model use;

```{r}
head(fitted(m1))
```
By default `head()` give only the first 6 values of the $n$ length vectors. 
Note that the resulted vectors are in the scale of the 'predictors' not the original [paramatrers]. Similar  result can be onbtain using function `predic(., type="link)`

```{r}
head(predict(m1,  type="link"))
```


#### feature

A [feature] in [data] analysis is an one of the [explanatory variables]  used for the model possible after a [transformation].

#### function

There are several occasions we use the terms [function] in [statistical modelling]. 
In an [input-output model] we refer to the function  $g()$ as an true unknown relationship between $X$ and $Y$  which the [model] tries to approximate. In define distributions we use the functions $f()$ ans $F()$ to describe [pdf]'s and [cdf]'s respectively. In [GLM]'s [GAM]'s  and [GAMLSS]'s we use $g()$ to describe [link] functions. In [distributional regression] we use $D()$ for the [pdf] of the response.   


#### final model

Is the [model] chosen after a selection procedure is applied to model selection. 


#### finite mixture


#### first order interaction

#### Frank copula

#### formula 

The term [formula] and formulae  refer to the `R` [formula] object which is used in the definition of a model. A formula in `R` stars with $\sim$, for example,  `rent~area+yearc+location` wuold indicate that the response is `rent` and the variables `area`, `yearc` ans `location`.    Note that because within  [distributional regression]  there could be one formula for each parameter 

################################################################################
## G

#### GAIC

GAIC stand for the Generalised Akaike Criterion @Akaike83 defined as `deviance` $+ K \times df$  where $df$ are the `degrees of freedom` and $K$ stands for the `penalty`.

#### GLM

 GLM stands for  Generalized Linear Model, @NelderWeddeburn72, a [mathematical model] which dominated the 1980's. As [input-output model] it can be written as $$
    X  {\longrightarrow} \fbox{f()}  {\longrightarrow} \mathbb{E}(Y) 
    $$ 
  where $\mathbb{E}(Y)$ is the expected values of the output $Y$ and $f()$ is a general function connecting the $X$'s with the expected value of $Y$.
As a [statistical model] a GLM can be written as;
$$
\begin{split}
\textbf{y}    &  \stackrel{\small{ind}}{\sim }  D( \boldsymbol{\mu},  \phi) \nonumber \\
\eta &= g(\boldsymbol{\mu}) &= \textbf{X}\boldsymbol{\beta} \nonumber \\
\end{split} $$
where $\stackrel{\small{ind}}{\sim}$ is read as the vector of the response is "independently distributed [random] [vector] having a distribution $D()$."  The distribution  $D()$  belongs to the [exponential family] which has as sub-models the [normal distribution], the [gamma distribution], the [inverse Gaussian], the [Poisson distribution] ans the [binomial distribution].  The fact that the elements of the vector $\textbf{y}$ are independent from each has the consequence that the log-likelihood is simply the sum of the individual log-[likelihood]s.    
 
 
 other with different mean $\mu$ depending linearly on the $X$'s but with a common second parameter  $\phi$.
   which make sure that values of any distribution parameter  $\theta$ are in the right [range].

There were two major problems with the assumption for $g(\boldsymbol{\eta})$

#### GAM

GAM stands for Generalized Additive Model, @HastieTibshirani90,  ...

#### GAMLSS

GAMLSS stand for Genaralized Additive Model for Location, Scaler and Shape, @RigbyStasinopoulos05. The classical GAMLSS model as first defined by @RigbyStasinopoulos05; $$
\begin{split}
y| \boldsymbol{\gamma}     &  \stackrel{\small{ind}}{\sim } \mathcal{D}( \boldsymbol{\theta}_1, \ldots, \boldsymbol{\theta}_k) \nonumber \\
g_1(\boldsymbol{\theta}_1) &= \textbf{X}_1 \boldsymbol{\beta}_1+ s_{\theta_1,1}(\textbf{x}_{\theta_1,1})+\ldots+ s_{\theta_1, J_k} (\textbf{x}_{\theta_1,J_{\theta_1}}) \nonumber \\
 \cdots &= \cdots \nonumber\\
g_k(\boldsymbol{\theta}_K) &= \textbf{X}_K \boldsymbol{\beta}_K + 
s_{\theta_K,1}(\textbf{x}_{\theta_K,1})+\ldots+ s_{\theta_K, J_K} (\textbf{x}_{\theta_K,J_{\theta_K}})
\nonumber 
 \end{split}
$$ {#eq-gamlssclassical} where we assume that the response variable $y_i$ for $i=1,\ldots, n$, is independently distributed having a distribution $\mathcal{D}( \theta_1, \ldots, \theta_k)$ with $k$ parameters and where $s_{\theta_k, j} (.)$ for $k=1,\ldots,K$ and $j=1,\ldots, J_{\theta_k}$ are smoothers for different explanatory variables. Note that smoother are depending on smoother parameters $\lambda$. Because of the duality of smoothers and random effects it was soon realise that the model can be a written in its **mixed** random effect model form as: 

$$
\begin{split}
y| \boldsymbol{\gamma}   &  \stackrel{\small{ind}}{\sim }  \mathcal{D}( \boldsymbol{\theta}_1, \ldots, \boldsymbol{\theta}_k) \nonumber \\
g_1(\boldsymbol{\theta}_1) &= \textbf{X}_1 \boldsymbol{\beta}_1+ \textbf{Z}_{1,1} \boldsymbol{\gamma}_{1,1}+ \dots +\textbf{Z}_{K, J_1} \boldsymbol{\gamma}_{K, J_1} \nonumber \\
 \cdots  &= \cdots \nonumber\\
g_k(\boldsymbol{\theta}_K) &= \textbf{X}_K \boldsymbol{\beta}_K + \textbf{Z}_{K,1} \boldsymbol{\gamma}_{K,1}+\dots+\textbf{Z}_{K,1} \boldsymbol{\gamma}_{K,J_K}  \nonumber
 \end{split}
$$ {#eq-gamlssmixed} where $\boldsymbol{\gamma}_{k,j}$ are random effect, for $k=1, \ldots,K$ and $j=1,\ldots,J_K$ and where each $\boldsymbol{\gamma}_{k,j}$ is distributed as a Normally distributed variable with zero mean and standard deviation given by $\sigma_{b_{k,j}}$. In fact there is set of $\sigma_{b_{k,j}}$'s which length is the length of all smoothers in the model. Note that smoothing parameters $\lambda_{\theta_k, j}$ and standard errors $\sigma_{b_{k,j}}$ are inverse related  $\lambda_{k, j}=\frac{1}{\sigma_{b_{k,j}}}.$ 



#### `gamlss`

`gamlss` is the original package in `R` for fitting a GAMLSS model @StasinopoulosRigby07.

#### `gamlss2`

`gamlss2` the new package in R for fitting a GAMLSS model.

#### `gamlss.data`

`gamlss.data`  is a package in `R` containing data sets use to demonstrate GAMLSS models.

#### `gamlss.prepdata`

`gamlss.prepdata` is a package in R to help users to preper data for analysis using a [distributional regression] model.


#### `gamlss.ggplots`

`gamlss.ggplots` is a package in `R` to help with diagnostics and other graphics  using the `ggplot2` package.  The functions in the package can be applied to fitted GAMLSS model independently whether were fitted  using the  [`gamlss`] or [`gamlss2`].


#### `gamlss.dist`

`gamlss.dist` a package containing all the theoretical distribution which can be assumed for the response when fitting a GAMLSS  model.

#### `gamlss.tr`

`gamlss.tr` can take any distribution family in `gamlss.dist` and truncated `left` `right` of in `both` directions. 

#### `gamlss.cens`

`gamlss.cens` can take any distribution family in `gamlss.dist` and apply  `left` `right` or  `interval` censoring. 

#### 	Gaussian white noise

An identical distributed standardised normal [random] variable with no pattern, see [white noise] see [white noise].


```{r}
#| fig-width: 5
#| label: fig-gaussianwhitenoise
#| fig-cap: "Gaussian white noise of 1000 observations"
y <- rnorm(1000)
plot(y)
```
A [worm plot] of a [Gaussian white noise] should show no pattern:
```{r}
#| fig-width: 5
#| label: fig-wpgaussianwhitenoise
#| fig-cap: "A worm Gaussian white noise of 1000 observations"
library(gamlss)
wp(resid=y)
```

#### generalized Tobit model 

#### generalized Tobit distribution

#### goodness of fit 

A goodness of fit is a measure (a quantity) design to evaluate how close the model is to the data. 

#### GAIC

GAIC is  the  Generalise Akaike Information Criterion defined as `deviance` $+k \times df$ where $k$ is the penalty applied to the [degrees of freedom] $df$. 


#### graphic

A [graphic] is a plot which help the analysis of [data]. A post-[fit] graphic that is a plot depending on the [fitted model] usually is either a [diagnostic] to help  checking assumptions or it is helping the [interpretation] of the model.   

#### Gumbel copula


For [random] variables $U_1$ and $U_2$ with [range] in $(0,1)$  and parameter $\theta \ge 1$  the [Gumbel copula] is defined as 

$$C_\theta(U_1,U_2)
=
\exp\!\left(
-\Big[(-\log U_1)^\theta + (-\log U_2)^\theta\Big]^{1/\theta}
\right)$$
The [Gumbel copula] belong to the [Archimedean copula] family. It can model only positive dependence only and only the upper tail. It can not model the lower tail dependence. It is asymmetric stronger in upper tail. If  the parameter $\theta = 1$ implies  independence. Large $\theta$ implies stronger dependence and $\theta \to \infty$  implies comonotonicity. The relationship of $\theta$ with Kendall’s $\tau$ is:

$\tau = 1 - \frac{1}{\theta}
\quad \Rightarrow \quad
\theta = \frac{1}{1-\tau}$
The [Gumbel copula] supports strong upper tails and its useful for modelling extremes high values cluster. The upper tail dependence coefficient is:
$\lambda_U = 2 - 2^{1/\theta}$ ans the ;ower tail dependence is:
$\lambda_L = 0$.
The [Gumbel copula] should be use if: i) joint large outcomes are important, ii) dependence increases in the upper tail, or the work involves extreme rainfall / floods, financial booms, large insurance  claims.

################################################################################
## H


#### hazard function

The [hazard function] of a [random] variable $Y$, $h()$, which shows the risk of the instantaneous fail  of $Y$  is defined as the [pdf] divided by the [survival function] i.e. $h(Y)= \frac{f(y)}{S(Y)}=\frac{f(y)}{1-F(Y)}.$ 


#### histogram

A [histogram] is a estimated density probability function function for a [sample] [vector]. See [density function] for a [smooth function] of a [histogram].
```{r}
library(MASS)
truehist(rent99$rent)
```


#### hierarchical Archimedean copulas

#### hypothesis

The hypothesis is what the researcher tries to understand and answer see also the `purpose` of the study 

#### hyper parameter


################################################################################
## I

#### independence

The word [independence] is refereeing to the state of two or more [random] variables. For example two random variance $Y_1$ and $Y_2$ are independent if their joint density [probability] can be written as  the product of their marginal; $f(Y_1, Y_2)= f(Y_1) \times f(Y_2)$. In the same way if the probability of more than two random variables can be written as the product of their marginals then then the [random] variables are independent.  
	

#### information

The word [information] represent the uncertainty about an unknown quantity, in our case the  [data]. In 
[statistical modelling] is refereed to the way  [information] is extracted from [data] using a [model].   

#### information criterion

By [information criterion] we refer to a [measure of goodness of fit] which it based on [information theory]

#### information theory 

By [information theory] we mean the theory of of communication developed by @shannon1948mathematical which is  based on the idea of [entropy].

#### interpretable model

A model is an [interpretable model] if its is easy to explain to others. Most  [mathematical model]s but not all [algorithmic model]s are [interpretable model]s. The following words have the same meaning  as [interpretable model], **transparent** model, **explainable** model or **comprehensive** model. 


#### interval values

A [random] variable is taking [interval values] if its exact value observed value is not known but we know that  it occurred in a specific interval [range] i.e $10< Y <15$.  Another expression of this behaviour is [censored values].

#### input-output model

An input-output model is a model where the variables $X$, the input, affect the variables $Y$, the output i.e. $X \rightarrow Y$. Input-output model are [supervised learning model]s since a response variable, $Y$ always exist. @breiman2003statistical use the diagram 
$$
X  {\longrightarrow}  \fbox{NATURE} {\longrightarrow} Y, \ 
$$
but nature is too complex so we use a `model` to help us. 
$$
X  {\longrightarrow}  \fbox{Model} {\longrightarrow} Y \
$$
In practice we use a mathematical function $g(X)$ to describe the relationship. 
$$
X  {\longrightarrow} \fbox{g()}  {\longrightarrow} Y \ 
$$
The function $g(.)$ is unknown so the task of the modeller is to find such a function. In a distributional regression framework the represanation is more complex. $$
X  {\longrightarrow} \fbox{g()}  {\longrightarrow} D(Y|\theta(X)) \ 
$$


#### independent random variable

A [vector] of [random variable]s say $\textbf{y}$ is said to be independent (or independently distributed) if the distribution of the component of the vector can be written as the product of the individual probabilities i.e. $Pr(\textbf{y}) =  \prod_{i=1}^n Pr({y}_i)$  


#### independent variables

The [explanatory variables] in a [regression analysis]


#### interaction

#### inference

In statistics [inference] refers to the idea that under certain circumstances we can go from the [sample] [data] to draw conclusions about the [population] of interest, That is we can can go from the **small** to the the **big**. The circumstances depend on [assumptions] about the [data generation mechanism].   

#### interpretation 

The interpretation of a model is the story behind the fitted model, what it is telling you. 


#### input variables

The [explanatory variables] in a [regression analysis] same as  the input in an [input-output model].


################################################################################

## J

################################################################################
## K

#### K-fold cross validation

A K-fold Cross Validation provides [training] and [test] data for all the observations by going through all the K-folds, It provides unique `fitted values` and `residualas` and in addition provides multiple $K-1$ values for fitted values and residualas for the training $K-1$ folds.

#### kurtosis

#### kurtosis parameter

################################################################################
## L

#### LASSO

#### least squares

#### level 

By [level]s we refent to the distict values of a [factor].  

#### linear model 

By linear model we refer to models described by the [assumptions] $\textbf{y}=\textbf{X}\boldsymbol{\beta}+e$ where $e_i \sim N(\boldsymbol{0}, \sigma^2)$.  That is the relationship between the response and the explanatory variables is linear determined by the coefficients $\boldsymbol{\beta}$ and the error term has a normal distribution with constant variance $\sigma^2$. The unknwon parameters in this case are the  $\boldsymbol{\beta}$ and $\sigma^2$. I we know those parameters we can predict the behaviour of the response if the model is correct.  

#### link function 

A function connecting  the model  predictor with the distribution parameter for example $\eta_{\sigma}= g(\sigma)$ The link function ensure that the distribution parameter is on the right [range]. For example, since $\sigma$ take values in the positive real line $0< \sigma <- \infty$  modelling sigma as $\eta_{\sigma}=log(\sigma)$ will make sure that $\exp(\eta_{\sigma})$ is always positive.  For modelling purposes the the link function $g(\theta)$ its inverse $g^{-1}(\eta_{\theta})$ and the first derivative $d \eta_{\theta}/d\theta$ must be defined.

#### likelihood 

The likelihood  function is define as the [probability] of observing the [sample] seeing not as a function of the random variable involed but as a function of the parameters of its distribution. For example, let the random vector variable $Y$ to come   from an assumed distribution $f(\textbf{Y}|\boldsymbol{\theta})$. We obsrved the $n$ dimensional vector $\textbf{y}$. If in addition we assume that the elements of  $\textbf{y}$ are comming from  $f(\textbf{Y}|\boldsymbol{\theta})$ and they are independed fraom each other, then the likelihood funtyion for $\boldsymbol{\theta}$ is defined as $$L(\boldsymbol{\theta})=\prod_{i=1}^{n} f(y_i|\boldsymbol{\theta})$$ and its log-likelihood as

$$\ell(\boldsymbol{\theta})=\sum_{i=1}^{n} \log f(y_i|\boldsymbol{\theta})$$
In both cases the functions are seen as functions of $\boldsymbol{\theta}$ rather than $\textbf{y}$.

#### log likelihood 

The [log likelihood] is a [measure of goodness of fit] for a  [model]. It is denoted as $\ell(\boldsymbol{\theta})$  see [likelihood] for its definition.

#### location parameter

A parameter of the distribution describing the centre of the [distribution]. The most common  [location parameter]s are the [mean] and the [median].

#### loss

A [loss] function  is a [measure of goodness of fit]. The [loss] function $\ell oss()$ is a function of both the [data]  and the fitted [model] and measures how far we are prepared to accept that the model is correct. In gambling the [loss] function has a monetary value but in [statistical modelling] is a measure of difference between the [model] and the [data], (see also [risk]).  Typical loss functions are; i) [squared errors]  i.e.  $\sum_{i}^n (y_i - \hat{y_i})^2$ and  [absolute errors] i.e. $\sum_{i}^n \left|y_i - \hat{y_i} \right|$ determine how far the actual value $y$ is from its estimating value $\hat{y}$. Note that  in [distributional regression] $\hat{y}$ is an estimate of the [location parameter] of the the model  therefore is not an appropriate measure we are interest in other parts of the [distribution] fo the [response]  for example the [tail]. a more appropriate measure in this case could be minus the  the [log likelihood] $-\ell(\hat{\boldsymbol{\theta}}_i)$. Notice that [loss] functions could include penalty terms penalising the complexity of the model.


################################################################################
## M  

#### main effect

#### marginal distribution

#### mathematical model     

A mathematical model is a model which is build using mathematical equations.

#### MLE

see [aximum likelihood estimation]

#### maximum likelihood estimation 

The Maximum Likelihood Estimation, MLE, is method of fiting a distribution to a vector $\textbf{y}$ using minus the [log likelihood] as [measure of goodness of fit].  Note the minimise $-\ell(\boldsymbol{\theta})$ is equivalent of ninimise the [deviance]  $-2\ell(\boldsymbol{\theta})$ or maximising the [likelihood] $L(\boldsymbol{\theta})$ or the [log likelihood], $\ell(\boldsymbol{\theta})$.   
      

####  measure of association
     
####  measure of goodness of fit

A measure of goodness of fit is a way to evaluate the fidelity of the data for a given. model.  That is, how close, is the model to the data using an objective measure. of goodness of fit. 



#### mean

The mean of a [vector] in a [sample] is the average value of the [vector] i.e. $\bar{x}= \frac{1}{n}\sum_{i=1}^n x_i$. The mean or [expected value] of a [distribution] sometimes called the first [moment] of the distribution is defined as $\mathbb{E}(Y)=\int Y f(y)dY$  

#### median

Let $F(Y)$ be the [cdf] of the random variable $Y$ then the  [median] of $Y$ is defined as the value of $Y$ satisfying the equation $0.50=F(Y)$ or $\text{median}(Y)=F^{-1}(0.50)$ where $F^{-1}()$ is the inverse of the [cdf] also known as the [quantile function]. For [vector] in the [data]/[sample] the median can be defined similarly using the [empirical cdf] of the [vector]. 


#### moment

#### mixed distribution 

#### question 

The question is the purpose of the study in hand;


#### mathematical model

A mathematical model is a model which is using mathematical equations to describe the variables involved.
A popular mathematical model partial or  ordinary differential equations  model


#### machine learning

Machine learning refers to a selection of algorithmic models.  Some of the machine learning are [model]s are [supevised learnig] models i.e [regression] type and some [classification] type.    A typical [regression] [machine learning] [model] has the form $\textbf{y}=g(\textbf{x})+\boldsymbol{\epsilon}$ where the error $\boldsymbol{\epsilon}$, a [vector], is assumed to be an independently distributed [random] variable and $g()$ an unkown function to be estimated form the data.  Note that one of the implicit [assumptions] of the model is that the [error], $\boldsymbol{\epsilon}$,   has a symmetric distribution. This explicit assumption can be recrtified by using a [transformation] on $\textbf{y}$ which would possible make the transformed response $\textbf{y}'$ symmetric before fitting.  Note however that this transformation is not always exist therefore [distributional regression] models do not rely on it but relying on the fact that the a theoerical [distribution] of the [response] exist.  



#### MSE

The term stands for  Mean Square Error. Its defined as $$\text{MSE} = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2$$ and it is an [empirical risk] and a [measure of goodness of fit] for the standard [regression analysis] [model]s. It can be calculated in both [training] or [test] data sets for a [measure of goodness of fit] or a measure of [prediction] power, respectively. MSE is **NOT** appropriate for [distributional regression] models unless the [purpose] of the study is on the centre of he assumed distributions.



#### MAE

The term {MAE}  stands for  Mean Absolute Error. It is defined as $$\text{MAE} = \frac{1}{n}\sum_{i=1}^n |y_i - \hat{y}_i|$$ and it is an [empirical risk] and  a [measure of goodness of fit] for the standard [regression analysis] [model]s. It can be calculated in both [training] or [test] data sets for a [measure of goodness of fit] or measure of [prediction] power, respectively. MAE is **NOT** appropriate for [distributional regression] models unless the [purpose] of the study is on the centre of he assumed distributions. 


#### measure of goodness of fit

A measure of goodness of fit is usually an [empirical Risk] measure. For exampe the mean square error, MSE use in LS model, or the log-likelihood used in GLM and GAMLSS. 

#### model


A [model] is a simplification of reality by provides an easier way to understand the structure of a problem. In [data analysis] a [model] is a filter which tries to extract the right information from the data to answer the [question] the scientist  tries to answer. Related to [regression analysis] there are [mathematical model]s and [algorithmic model]s.  

- A [mathematical model] tries to answer the [question] using mathematical equations. 

- An [algorithmic model] is  using instead a set of rules to perform the task of answering the [question]. 

An [adequate fit]ted [model] is a model which fits the [training] data well and its [interpretation] will help to answer the [question], but remember that; 

> `all models are wrong but some are useful`.
>
> -- George Box  (@box1979robustness)

Whether a model is useful depends on the [purpose] of the study.

#### model average

The  term  [model average] or [stacking] is when the [final  model] is chosen by averaging different [fit]ted [model]s. This  prevents us from choosing a single model by summarizing the results from fits from multiple models. [model average] is advantageous  if the fitted  models complement each other. For example, in a [distributional regression] framework, this could mean the one model fit the [tail] of the distribution better while an other fits the middle better. By  averaging the [model]s well we could  possible fit both the middle and the [tail] well. There are a lot of way to average [model]s. In [distributional regression] a possibility is a [finite mixture] distribution with elements all fitted distributions. For example 
$$ f_F(y_i)= \sum_{i=k}^{K} f_k(y_i| \hat{\boldsymbol{\theta}}^{k}_{i}) $$ where $i=1,2,\ldots, n$ and we assumed K different  [model]s with  assumed distributions $f(|\boldsymbol{\theta}^{k})$.

#### model fitting 

A [model fitting] process involves the [data], the [model] and the [purpose]  of the study. The model is fitted  to the [data] by  minimise a [measure of goodness of fit] see @fig-fit which should reflect the [purpose] of the studty.  Note that [model fitting] could provide  an [adequate fit], an [over-fit] or an [under fit]. A single fit could provides unique [fitted values] and [residuals] for both [training] and [test] data sets.

```{mermaid}
%%| fig-width: 5
%%| label: fig-fit
%%| fig-cap: "The model fitting process needs both data and model which to conform with the purpose of the study  "
flowchart TB
  subgraph model-fitting-process
   style model-fitting-process fill:#e3f2fd,stroke:#1e88e5,stroke-width:2px
    direction LR
    D[data]
    M[model]
    P[purpose]
  end
  A[assumptions] --> M
  M --> F[fit]
  F --> P
```

#### model formula

A model formula in R takes a  

#### model selection 

Model selection` in GAMLSS has two meanings i) how to choose the distribution of the response and ii) to find out how the $x$'s effects the distribution of $y$. The latest is done by checking how x's effect the parameters of the distribution.


#### model interpretation 


#### model prediction

#### multicollinearity


#### multivariate distribution 

A distribution function [pdf] for more that one [random] variable.

################################################################################
## N

#### $n$

$n$  is number of observations in a [tabular data] set

####  natural variability


#### normalised randomosed quantile ressiduals

#### null model 

As null model we refer to a model with no [terms] in it. In R notation this is the model with  a [model formula] equal to `~1`. For distributional regression a null model is model which has   [model formula] `~1` for all the [distribution parameters] models.   



################################################################################
## O

#### observations

We usually refer to the rows of a [tabular data] as the [observations]. Note that the number of obsrvations in the [data] is denoted as [$n$]. 

#### ordinary differential equations 

#### Occam’s principle

The [Occam’s principle] from the Latin "Entia non sunt multiplicanda praeter necessitatem"
(“Entities should not be multiplied beyond necessity.”)  it says that  "among competing explanations, the one with the fewest assumptions should be preferred".  The principle  does not say the simplest explanation is always true but a simpler models is preferable if  explain the data equally well and complexity 	should not be introduce unnecessary. In [statistical modelling] for [mathematical model] it means [model] with fewer parameters are preferable if they explain the model well or more  generally if two models fit the data similarly, choose the one with fewer predictors. Note that if models fit the data similarly they belong the  [Rashomon set] of relevant models. 

#### over fitting 

Over fitting happens when the [fit] is too close to the [data] and therefore do not generalised well when try to predict, see also [over fit].

#### over fit

By [over fit] we mean situations where the model is too close to the actual [data] but not close to the [population]. An over fitted model is not good for predilection purpose, see also [over fitting]. 


#### out of bag

By [out of bag] [data] we refer to [data] that they have not be used to [fit] the [model]. That is, [test] or [validation] [data] sets.

################################################################################
## P


#### pacf

The term [pacf] stands for partial [autocorrelation] function. The [acf] and [pacf] are [diagnostic] tools for detecting [autocorrelation] in a [vector], $\text{x}$. 

```{r}
#| warning: false
# library(gamlss.ggplots)
# y_pacf(resid(m1))
```


#### pairwise relationship

By [pairwise relationship] we refer to the relationship between two [vector]'s in the [data]/[sample]. 
The investigation of [pairwise relationship]s between the explanatory variables could lead to  the identification of possible problens in the [fit] of [interpretation] of a [model]. The identification of [pairwise relationship]s between the [response] variables could lead to  better construction of a [multivariate distribution] via [vine copula]. 

#### parsimony

The principal of parsimony is what is also known as  [Occam’s principle] state that   

#### partial effects

#### partial differential equations

#### parameters

We refer to [parameters] as an generic term for all the unknown quantities within a [model]. The model [fit] estimates those [parameters]. The [parameters] of a [distribution regression] framework  are of three types: the  [distribution parameters] which are modelled as functions of the [explanatory variables] i.e. $\boldsymbol{\theta}_k = g(\textbf{X}_k)$, the  [coefficients] describing the relationship between the [distribution parameter]s and the [explanatory variables]. and the [hyper parameter]s which are parameters which tune the mdel to the direction of the [data].    

#### Poisson distribution

#### population

The population is the set of object we would like to study. The population is related with the [purpose] of the study. The idea behind is that we collect a [sample] ([data]) from the population of interest to drew inference form the [population] using the [sample]([data]) and the [model]. The process is called [statistical inference] and it is shown in @fig-assumptions.

```{mermaid}
%%| fig-cap: "How statistical inference working "
%%| fig-width: 5
%%| label: fig-assumptions
flowchart LR
  A{population} -->|data generation| B[sample] 
  C(model) -->|fit| B
  B -->|check| C
    C[model] -->|statistical inference| A
```


#### principle component regression

#### properties of the distribution

Please see [characteristics of the distribution]

#### properties of the distribution parameters

The [distribution parameters] have different properties by how they are defined within a [distribution]. Any [distribution] can be re-parametrised differently as far as the re-parametrisation end up with the some number of parameters as the original distribution. For [distributional regression] the specific parametrization  do matter in order to help the [interpretation] of the [model]. Some  parametrisations are more useful in practice that others. For example in a [distributional regression] it worth to have a parameter dealing with, where the centre of the [distribution] is, and whether this centre changes with [explanatory variables].  Those parameters are called  [location parameter]s.  Other parameters could describe  how far from the centre each observations could fall. Those are called the  [scale parameter]s. To describe asymmetry in the distribution we have the [skewness parameter]s. To distinguish distribution with fat [tail]s  we have the [kurtosis parameter]s.  Notice that any distribution could have more that one parametrisation and that the specific parametrization could affect different [characteristics of the distribution].


#### power transformation

By [power transformation] in [distributional regression] framwork we mean the [transformation] of one of the $\textbf{x}$ the [explanatory vatiables] to a more suitable form to a [feature] to make it suitable for [regression analysis]. The  [power transformation] is defined as  $x_p=\textbf{x}^p$ so the variable $\textbf{x}_p$  is entering the formula rather than $\textbf{x}$. The parameters $p$ takes values typically in the [range] $0 \le p \le 1.5$. The transformations tries to make  the values of the $\textbf{x}$ more symmetric and more even spaced.  

#### purpose

The purpose of the study is the [hypothesis] the study is working on.

#### pdf 

#### penalised least squares 

#### PIT residuals

The [PIT residuals] stands for the probability integral transformed  residuals. Within a [distributional regression] framework  are defined as, $$\hat{u}_i = F(y_i| \hat{\boldsymbol {\theta}}_i).$$ {#eq-PITresid} for $i=1\ldots,n,$ where $F(.,| \hat{\boldsymbol {\theta}}_i)$ is the [cdf] of the fitted [model]. 
If the distribution of $y_i$ is specified "correctly" then the [PIT residuals] are expected to behave as identical and independently distributed (i.i.d) random variable from a [uniform distribution] i.e $u_i \sim U(0,1)$. 
While the above definion of IT residuals works perfectly for [continuous distribution]s are not ideal for [discrete distribution]s especially if the [range] of the response is limited to small [range] i.e. $0,1,\ldots,10$, For [mixed distribution]s where there is small amount of discrete values or for [censored response]s. A randomisation of the PIT residuals between the values $y_i$ and $y_i+1$ usually solves the problem see for example Chapter 10 of @Stasinopoulosetal2017.

#### prediction

By [prediction], in a standard [regression analysis], we mean using the [model fitting] to predict the behaviour of the [response]. Note, that in a [distributional regression] [model] there are two uncentainties in associated with  predicting  any future value of $Y$.; i) uncertainty about the assumed distribution of $Y$; ii) uncertainty about the [model fitting].  

#### prediction interval

A [prediction interval] inteval 

#### probability 


################################################################################
## Q

#### QQ-plot

#### quantile

#### quantile function

#### quantile residuals

## R

#### $r$

$r$ is the number of [explanatory variables] or [input variables] in the data 


#### Rashomon

#### Rashomon set 

#### random 

The word [random]   is used as equivalent to [stochastic].

#### random variable

A random variable $Y$ is a variable which has a distribution. We denote this  as $Y \sim f( Y| \boldsymbol{\theta})$ where the symbol $\sim$ reads as "is distributed" and where $f()$ denote a probability distribution function [pdf] and the $| \boldsymbol{\theta}$ notation emphasise that usefull theoretical distributions depent on unknown [vector] of distribution parameters $\boldsymbol{\theta}$. The more the parameters in $\boldsymbol{\theta}$ the more flexible the distribution is.  Another important feature of a random variable is the specification of its [range] $\mathbb{R}(Y)$.  

#### range

This this refer to the [range] of possible values that a [random variable] takes. We use the notation $\mathbb{R}(Y)$ to describe the range of a random variable $Y$. There are two types of possible ranges in the real line i) continuous  and ii) discrete range.  

#### Rashomon

The term [Rashomon] originates from Akira Kurosawa’s 1950's film **Rashomon**, where a single incident,  a murder and an assault, is recounted by four witnesses, the bandit, the samurai’s wife, the samurai via a medium, and a woodcutter, each giving a vastly different version of what occurred. The film ends without resolving which account, if any, is accurate. 

![](rashomon.png){width=100}

#### Rashomon effect 

The idea that different models could possible provide different interpretations of the same data set. 

#### Rashomon set 

Is the set of [regression] [model]s with similar [measure of goodness of fit] therfore it is difficult to distinguish between them. [Rashomon set]s are real, practical statistician encounter them all the time.  Note that [Rashomon set]s are comon to over-parametrized models

#### regression 

A [regression] [model] is  an [stochastic] [input-output model] where the [explanatory variables], $X$, affect the [response] $Y$.   


#### regression analysis
A [regression analysis] is an statistical [input-output model] analysis, analysing [tabular data].  Typically regression analysis is refer to [linear model]s but here we use the term to describe any [input-output model] relationship. 

#### regression tree

#### reference curves

#### residuals

The residuals  is a [vector] of length $n$ with each element measuring the difference between observed and fitted values. For a [linear model] the residuals are defined as $y_i-\hat{y}_i$ for $i= 1,\ldots,n$. where $\hat{y}_i$ is the fitted values of observation $i$. The $n$-observations have $n$ residuals. Residuals can be defined in both the [training] or the [test] data sets. Residuals from the training data can be used to check [underfitting] while residuals from the test data can be use for checking  [overfitting] the data.

Two transformations are needed to get the z-scores from a fitted distributional model. Let $y_i$ and $F_o(y_i, \hat{\boldsymbol {\theta}}_i)$ be the $ith$ observation of the response and its fitted cumulative distribution function (cdf), respectively. Then first transformation is to get the probability integral transformed (**PIT**) residuals which are defined as, $$\hat{u}_i = F(y_i, \hat{\boldsymbol {\theta}}_i).$$ {#eq-PITresid} If the distribution of $y_i$ is specified "correctly" then the PIT residuals are expected to behave as identical and independently distributed (i.i.d) random variable from a uniform distribution i.e $u_i \sim U(0,1)$. The problem is, that is rather hard to check deviation from the uniform distribution, so we take the PIT residuals and transform them to z-scores using $$\hat{z}_i = \Phi^{-1}(\hat{u}_i).$$ If the assumed distribution of the response is approximate "correct" the z-scores are i.i.d. standardised normally distributed random variables i.e. $z_i \sim N(0,1).$

The two transformations needed to create the z-score residuals are shown in @fig-resid_types. In @fig-resid_types (a) the response is transform to a PIT using the fitted cdf function $F(y_i, \hat{\boldsymbol {\theta}}_i)$ while In @fig-resid_types (b) the PIT are transformed to z-score using the inverse cdf of the normal distribution $\Phi^{-1}(0,1)$ (otherwise known as the q-function of the normal distribution). While the above residuals works perfectly for continuous distributions are not ideal for discrete counts especially if the range of the response is limited to small range i.e. $0,1,\ldots,10$. A randomisation of the PIT residuals between the values $y_i$ and $y_i+1$ usually solves the problem see for example Chapter 10 of @Stasinopoulosetal2017.

#### response 

The response variables is the output variable in a [input-output model].



#### RMSE

The term stands for  square Root of Mean Square Error. Its defined as $\text{MSE} = \sqrt{\frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2}$ and its a [measure of goodness of fit] for [regression analysis] [model]s. It can be calculated in both [training] or [test] data sets for a [measure of goodness of fit] or measure of [prediction] power, respectively. RMSE is **NOT** appropriate for [distributional regression] models unless the [purpose] of the study is on the centre of he assumed distributions.

#### risk

A [risk] function measure the accuracy of a [model] in terms for a specific [measure of goodness of fit]. The [risk] function  is a function of  both the [data] and the [model fitting] process. For [regression model]s where only the mean of the response is modelled as a function of explanatory variables , for example,  $Y=g(X)+\epsilon$,  it measure how accurate the function $g()$ is. The mathematical definition of a [risk] function is given as the average of a [loss] function.  Let us define the [loss]  function as the discrepancy measure between the [data] and the [model], $\ell oss()$.    The expected value of a loss function $\ell oss$() is defined as the [risk] function.  $$\mathbb{Risk}(g) = \mathbb{E}_{X,Y} [ \ell oss(\hat{g}(X), Y) ]$$ where  $\hat{g}()$ is an estimate of the model unknown function $g()$, $X$, $Y$ are the pairs, of an [input output model] variables.   The [risk] measures the average discrepancy between the assumed [model] and the [data].   
The problem with the above definition of the [risk] function is that involve taking expectations with respect to the input and output variables. Even if conditioning on the $X$'s we have to take expectation with repsect to $Y$. We can avoid the problem all together if we replace the [risk] function with the [empirical risk]  which gives equal probability to each row of the [data] matrix. For example the [empirical risk] is defined as
$$\mathbb{ER}(g) = \frac{1}{n}\sum_{i=1}^n [ \ell oss(\hat{g}(x_i), y_i)$$ 
Note that  if the [loss] function is defined as  minus the log [likelihood] (or the [deviance]) minimising the [empirical risk]  is equivalent to maximising the [likelihood] function with rescect to the parameters of the model.  Note that  $\frac{1}{n}$ is just a constant not involved in the minimazation.


################################################################################
## S

#### sample 

In statistics a [sample] has  the same meaning as [data]. It called a [sample] because it is assumed to come from a bigger data set the [population] of interest. 

#### saturated  model

A saturated  model is a model which has as many unkown parameters as the number of observations $n$ 

#### SBC

The SBC is another name for [BIC]. It stand for  the Schwarz  Bayesian Criterion for comparing fitted models.


#### scale parameter


#### Sklar's theorem

The Sklar's theorem , (@sklar1959fonctions) states that every multivariate [cdf] $F(x_1,\dots, x_r)$ with marginals [cdf]s $F1(x1)$, $\ldots$, $F_r(x_r)$ can be written as
$$F(x_1,\ldots,x_r) = C_r[F_1(x_1),\ldots,F_r(x_r)]$$
for some appropriate r-dimensional copula $C_r$. 

#### selection of terms

A selection of terms in distributional regression is the process of identify which [term] affect which parameter. 

#### skewness

Skewness is a statistical measure indicating whether a sample [vector] is symmetric or not. Skewness is also applied to random vectors and therefore to theoretical distributions.

#### skewness parameter

#### supervised learning model

A [supervised learning model] is a regression model in which a [response] exist.

#### statistic

The term [statistic] refers to a functin of the [data]/[sample]. For example, the mean of the  vector $\textbf{x}$ in the [data] defined as  $\bar{x}= \frac{1}{n}\sum_{i=1}^{n} x_i$ is a [statistic], since it is a function of the [data]. A [statistic] captures properties of the [sample] the same way that [moment]s and [centile based measures] caption properties of a theoretical  distribution.  In the past, it was common practice,  to equate [statistic]s with their equivalent distribution characteristic to draw infernce from the [population]. The pracice almost abandoned after the introduction of [MLE]. 


#### centile based measure

A [centile based measure] is equivalent to a [quantile based measure]. It  describes  a [characteristics of the distribution] based on [quantile]/[centile]  rather [moment].

For example $\bar{x}$ can be used as estimate of the mean $\mathbb{E}(X)$ of the theoretical distribution assumed for $x$ i.e. $\mathbb{E}(X)= \int_{-\infty}^{\infty} X f(X) dx$, where $f(X)$ is the assumes distribution for the [vector] $x$.  

#### statistical inference 

Is the process of drawing inference from the [sample] to the [population], from going to a specific case to the general situation. The path from to the spefic to the general can only be achieved using [assuptions] and therefore is very treacherous ans irrelevant if the assumtions are not right.    

#### statistical model 

A statistical model is a [mathematical model] containing a [stochastic] component. That is a stachatic model is build with [probability] statements on its [assumptions].  

#### statistical modelling

Statistical modelling is the art of creating a [statistical model] which represents the [data] adequately. A [statistical model] extract information from the data to answer [question]s of interest (see [purpose]). 
<!-- and possible the [data generating mechanism]  -->
 @fig-model shows the [statistical modelling] process. The [data], the [model] and the [question] to be answered by the [data] are inter-related. The model needs [assumptions] to be created and in ordered to be able to be evaluated it also needs a measure of [goodness of fit] an indicator which  measure the distance  between the [data] and the  the [model]. Therefore
  the  measure of [goodness of fit] depends on both the [data] and the [model].
 Given the [fit] is an [adequate fit] then by the [interpretation] ofthe model we should be able to answer the [question].  The correct [assumptions] are related with an [adequate fit].

```{mermaid}
%%| fig-width: 5
%%| label: fig-model
%%| fig-cap: "The modelling process: the data, the model and the question are inter-related. The model needs assumptions before the fit and an adequate fit help to answer the question."
flowchart TB
  subgraph stats-modelling-process
   style stats-modelling-process fill:#e3f2fd,stroke:#1e88e5,stroke-width:2px
    direction LR
    D[data]
    M[model]
    Q[question]
  end
  A(assumptions) --> M
  D <--> M
  Q <--> M
  M --> F(adequate fit)
  D <--> Q
  F --> Q
  A <--> F
  G(goodness of fit) --> F
  M --> G
  D --> G
```
The [statistical modelling] is s process and therefore that there are a lot opportunities for that things to go wrong. In order to investigate this further let us look in more detail. 
We usually assume (explicitly or implicitly) that there exist a [population] of interest and t [data generating mechanism] which creates the observed [data]/[sample] is see @fig-mod_1.   
```{mermaid}
%%| fig-width: 5
%%| label: fig-mod_1
%%| fig-cap: "The relationship of the population and the data/sample."
flowchart LR
  P[population] --> G(data generating mechanism) 
  G --> D[data]

```
The [statistical model] is a filter which tries to extract [information] from the [data] either to go back to the [population], a process known as [statistical inference] or to gain knowledge from the [information] of  the data. More specifically this information is needed to answer the [question]s  ans it signify the [purpose] of the study, see @fig-mod_2.  
```{mermaid}
%%| fig-width: 5
%%| label: fig-mod_2
%%| fig-cap: "Showing how a model is behaving as afilter to extract information from the data"
flowchart LR
  D[data] --> M[model] 
  M --> I(information)
```
In order to create a [model] we  need to make  [assumptions]. The assumptions should reflect the [data generating mechanism] but also the [data] itself see @fig-mod_3. The [assumptions] should  account for the [natural variation] in the [data] but also able to capture the relationships between the variables in the [data]. 
```{mermaid}
%%| fig-width: 5
%%| label: fig-mod_3
%%| fig-cap: "A model need realistic assumptions to capture both the natural variable and ther interrealtioships in the model."
flowchart LR
  G(data generating mechanism)  --> D[data]
  D --> M[model] 
  A(assumptions) --> M
  G --> A
  M --> I(information)
  D --> A
```
The quality of the [information] to extracted in the [data] depends on the flexibility of the [model] and how realistic are those [assumptions]. Wrong assumption could lead to wrong conclusions. This fact was recognise at an early stage of  [statistical modelling] leading to the introduction of [diagnostics] tools to check for the appropriateness of the [assumptions]. A diagnostic is a  [statistic] or a [graphic]al plot which is a function of both the [data] and the [model] and therefore could check whether the assumptions of the [model] are adequate. A lot of the graphical [diagnostics] are based on the [residuals] of the [model] which if the assumptions are correct are expected to behave as [white noise]. @fig-mod_5 shows what we called  the classical statistical modelling process.  There is a loop going around  starting from the [assumptions] to the  [model] to the [fit] to the [diagnostics] and back into the [assumptions].  That is, assumptions are made the model is fitted and if the diagnostics intricate an inadequate fit different assumption are made and the process going into the loop until an [adequate  model] is achieved and the modelling proccess is finished. This process  of  the statistical modelling loop is going back to @BoxJenkins70 in their famous book on time series.      

```{mermaid}
%%| fig-width: 5
%%| label: fig-mod_5
%%| fig-cap: "The classical statistical modelling process which uses a single model and tries to improve it"
flowchart TB
  D[data] --> A(assumptions) 
   A --> M(model)
   M --> Y[adequate model]
   M --> W(fit)
   W --> V(diagnostics)
   V --> A
```

```{mermaid}
%%| fig-width: 5
%%| label: fig-mod_6
%%| fig-cap: "The classical statistical modelling process which uses a single model and tries to improve it"
flowchart TB
  D[data] --> A(assumptions) 
   A --> M(model)
   M --> Y[adequate model]
   M --> W(fit)
   W --> V(goodness of fit)
   V --> A
```
In a modern statistical modelling approach the data are partitioned to help detection of [over fitting] and prediction. @fig-modelling_modern show this new approach of statistical modelling. Instead of a single fitted model $B$ candidate [model]s are [fit]ted and subsequently aggregated (see [stacking]) before [model interpretation] and [model prediction].

```{mermaid}
%%| fig-width: 5
%%| label: fig-modelling_modern
%%| fig-cap: "The modern modelling process uses more models and aggregates them"
flowchart LR
  A[part. Data] --> B[Assumptions] 
  B --> C{fit K-Models}
   C -->|check| B
  C --> D[stacking]
  D --> E[interpretation & prediction]
```
The achieve this more  modern approach to statistical modelling the following technique could be used: [boosting] where a lot of week models are fitted to a single data set in such a way that the averaged models supplement each other. This is done by taking into the account the [residuals] of the previously [model fitting]; [bagging] when $B$ models are fitted to $B$ bootstrap [data partition]s and the models are averaged; [stacking] when when $B$ different [model]s are fitted to a single data set (no partition) and the models are averaged.

#### adequate model

#### natural variation

The [natural variation] refers to the variability in the [data]. The idea behind is that even if we were able to collect data more that ones from the same [population],  under similar conditions, the data will be different reflecting the [natural variation] of the data.  This makes necessary to use a [stochastic model].   In [regression analysis] the [natural variation] is reflected in the variation of the [response] variable. Even if we be able to collect data at the same values of the explanatory variables the response will be different. 

#### statistical model

A [statistical model] is a [stochastic model]. that is a [mathematical model] containing probabilistic statements to account for the [natural variation] of the data.  

#### stacking

The idea of [stacking] is to [fit] several [model]s to single [data] set but instead of performing a [model selection] to proceed by averaging the resulting models. The problem with [stacking] is that very rare the procedure leads to good [model interpretation].

#### stochastic

A [random] variable is [stochastic] if it has a probability distribution [pdf] associated with it. The term [stochastic] is equivalent with the rerm  [random]  

#### stochastic model

A [stochastic  model]  is a mathematical or an algorithmic model which incorporates randomness, interms of probabilistic statments.  The output of a [stochastic  model]is not completely predictable. Not all models need a stochastic component. A [mathematical model] or an [algorithmic model] donot have to be a stochastic model. A stochastic model contains [probability] assumtpions about one or more of his components. 

#### stochastic regression

A stochastic regression models contain [probability] assumptions on how the input-output model is generated. A typical assumption is the response is a function of the explanatory variables plus an [error], for example, $\textbf{y} = g(\textbf{x}) + \boldsymbol{\epsilon}$ where The minimal assumption for a regression model is about the behaviour of the  `response. Not all problems need a stochastic component.   Stochastic models are often used because many natural, social, and physical systems have an inherent variability.


#### smooth function

A [smooth function] $s()$ is a term in a [model] design to model non-linear relationships.  In a [regression] model those relationships are between the [explanatory variables] and the [response], in a [distributional regression model] is between the explanatory variables and a distribution parameter.  To simplify matters, consider the simple [regression] situation  when we have only one explanatory variable, $\textbf{x}$ and one response $\textbf{y}$. If we suspect that the relationship is not linear we couls try a [smooth] **non-parametric** [model] for the response:  $\textbf{y}= s(\textbf{x})+\boldsymbol{\epsilon}$. The function $s()$ is a general notation for a smoother. that is, a smooth function determined by tha [data]. It turns out that the **non-parametric** part is rather misleading because most of the smooth techniques are following this simple linear model  $\textbf{y} = \textbf{B}  \boldsymbol{\gamma} + \boldsymbol{\epsilon}$ where $\textbf{B}$ is a basis matrix constructed by the values of the explanatory vector $\textbf{x}$. The estimation of the linear parameter $\boldsymbol{\gamma}$ (the length of which could be as big as the length of the data) is done using a [penalised least squares] method with solution $\hat{\boldsymbol{\gamma}}=(\textbf{B}^{\top} \textbf{B}+\lambda \textbf{I} )^{-1} \textbf{B}\textbf{y}$. Note that if more that one smooth functions exist in model [formula] then the estimation of the smooth functions is achieved using a [backfitting] algorithm.     



#### smooth model

A smooth model is a model containing smooth functions.  For example the simpler smooth model is  $\textbf{y}= s(\textbf{x})+\boldsymbol{\epsilon}$.  

#### smoother 

A smoother is a [smooth] non-parametric function  design to model non-linear relationships see  [smooth function].

#### sufficient statistic

A [sufficient statistic] was a fundamental concept in statiscal inference in that tells you when a [statistic] captures all the information in the [data] about the  [parameters]. It was a  usefull  concepts before computers but its  usefulness is diminishing over yeaar  since in most real lile situations  [sufficient statistic] are not provided.  The [exponential family] of distribution is the only family allow [sufficient statistics]ans this only for the mean, $\mu$.    

#### summary statistic

A [summary statistic] is another wold for [statistic]



#### survival function

The [survival function] of a [random] variable  $Y$, denoted here as $S()$,  gives the probability $Y$ to exceed a specific value $Pr(Y>y_o)$.  It is defined as 1 minus the [cdf] of the random variable $Y$,  $S(Y)=1-F(Y)$. 


################################################################################

## T

#### target

The [target] variables is the [response] variable in a [regression analysis]

#### time series

A [time series] [data] set is a data set when observations were obtained over time and therefore that is a high probability that they are not independently distributed but there is autocorrelation between sequential observations. A [time series] analysis is a methodology to deal with a [time seriess] [data]. 


#### Tobit model 


#### training data

The training data are the data used in the [fit] stage of a modelling process. The contrast with the [test data] and the [validation data] sets.

#### test data

The test data are the data used to check the prediction power of the model.  


#### t distribution

see Student-t

#### t copula

A [t copula] is based on the  multivariate Student-t and 	allows symmetric log tails dependence and therefore is more robust than the [normal copula] 

#### tabular data

Tabular data are  spreadsheet type of data sets. Tabular data   are rectangular in shape with variables vertically  and observations horizontally. We refer to the number of observations as $n$ and the number of variables as $r$.  Over recent years we have seen an increasing in the size of data sets for both $n$ and $r$. Traditionally classical regression models  supported  the cases where $n\gg r$, where $\gg$ refers to “$n$ is much greater than $r$”. More  recent regression  techniques i.e [LASSO] ans [PCR] support situation where $n  \simeq r$, or even $n<r$, Note that the notation $\simeq$  refers to “$n$ is nearly equal to $r$".  A typical tabular data example is shown in @tbl-TheTableofData.


##### Tabular data example {.smaller}

| obs number | y      | x~1~    | x~2~    | x~3~    | ... | x~r-1~    | x~r~    |
|------------|--------|---------|---------|---------|-----|-----------|---------|
| 1          | y~1~   | x~11~   | x~12~   | x~13~   | ... | x~1r-1~   | x~1r~   |
| 2          | y~2~   | x~21~   | x~22~   | x~23~   | ... | x~2r-1~   | x~2r~   |
| 3          | y~3~   | x~31~   | x~32~   | x~33~   | ... | x~3r-1~   | x~3r~   |
| ...        | ...    | ...     | ...     | ...     | ... | ...       | ...     |
| n-1        | y~n-1~ | x~n-11~ | x~n-12~ | x~n-12~ | ... | x~n-1r-1~ | x~n-1r~ |
| n          | y~n~   | x~n1~   | x~n2~   | x~n3~   | ... | x~nr-1~   | x~nr~   |

: A tabular data example {#tbl-TheTableofData .striped .hover}


#### tail 

The word [tail] in [distributional regression]   framework is refer to the [tail]s of the distribution of the response.  The [tail] of a distribution is very important if the [purpose] of the study is extreme values or exceedance probabilities.


#### term

A [term] is one of the  [explanatory variables] in a [model],  after possible a [transformation] to make it suitable for [distributional regression] analysis. For example a [factor] is a [term].  A first order interaction is a [term]. 

#### test 

A [test] data set is the part of the [data] keep out of from fitting the [model] in order checking the predictive power of the model using a [measure of goodness of fit].  

#### training

A [training] data set is the [data] set in which the [model] is [fit]ted.


#### transformation

By [transformation] we mean usualy a [vector] transformation, that is, a transformation of one of the columns of the [data] in  something that it is more suitable for [statistical modelling]. In a [distributional regression] framework we rarely transforming  the [response] variable, since the variability of the response is part of its assumed distribution. For example if we beleive that the response is highly [skew] to the left we caassume the log-distribution for $Y$ i.e `LOGNO` or `logTF`.  Transformations in the $X$'s sometime do help the modelling process. Typically is if would suspect non-linearity in the relatioship and we would like ot use a [smoother] to model it ideally the  $X$s should  be equal spaced with no outliers. A [power transformation] could help this. In [centile estimation] when we are dealling with  a single $X$ variable (often `age`) a [transformation] of the  $X$ maybe is necessary to achieve a good [model]. [reference here $\ldots$ Rigby et al. 2026]. Note that the timing for creating a new [feature]  by  transforming $X$  could be achieved before or during the [fit]ting a [model]. For the formal case see the function `trans()` in the package `gamlss.prepdata` for the later case see Fasiolo at al. $\ldots$.           

################################################################################
################################################################################

## U


#### under fit

A [model]  under fits the [data] if it is not close enough to the data. The closeness is evaluated using a [goodness of fit] measure like [GAIC]. This happens if the model is not flexible enough and therefore misses important information in the [data].  See also [under fitting]


#### uniform distribution

#### univariate` distribution 

#### under fitting 

Under fitting occurs when the [fit] is very poor accoding to a [goodness of fit]  measure and it does not represent the [data] properly, see also [under fit]. 


#### unsupervised learning 

An [unsupervised learning] [model] also called a [classification model]  is a model which has [explanatory variables] but not [response]. The model tries to guess what class the response belong to given the [explanatory variables]. 

################################################################################
################################################################################
## V

#### validation data

The [validation data] set is used for tuning a model. That is useful  in estimating the [hyper parameter]s of a model.   The [validation data] set is an [out of bag] data set.

#### variable importance

A variable importance should show the important of a [term] in the [model]. That is, how much this term contributed in explaining the model. Note that for [distributional regression] a [terms] affect both the model of  [parameters] of the distribution but also the overall [measure of goodness of fit]. 
So, for example, one should be able to ask the question,  of how important say **age** is for $\sigma$  the variation the response  $Y$  but also how important is overall to the model compared with other terms that may influence other part of the model rather the scale parameter.  For [mathematical model] parametric [model]s the size of the estimated coefficients could provide an indication of how important a variable could be  but for other model could be more difficult. An [agnostic] method given in the literature (I think by Breimer) is to find the prediction power of the variables by scramble the variable of interest and compared it with the preditions ontain when the variable remain unscramble.This method couls produce indexes of importance both model parameter based and overall [measure of goodness of fit]     
     

#### vector 

In [statistical modelling] we refer to a [vector] as one of the columns in a [tabular data] set. In [regression analysis] both the [response] variable(s) and the [explanatory variables] are the [vector]s of interest for often we would like to explore their [pairwise relationship]s.

#### vine copula

The  basic idea of a [vine copula] is, for a given [data] set, to build a multivariate distribution for the [response]s  based their [pairwise relationship]s..    

################################################################################
################################################################################
## W

#### white noise

The term [white noise] is used to describe a pure [random] process in the sense that knowing the past values will tell tell you nothing about the future values. More precise is refers to a sequence $\{\varepsilon_t\}$ of a [random] variable with $\mathbb{E}[\varepsilon_t] = 0$ 
	$\operatorname{Var}(\varepsilon_t) = \sigma^2 < \infty$ and	$\operatorname{Cov}(\varepsilon_t, \varepsilon_{t-k}) = 0$ for all $k \neq 0$. If in addition we assumed that $\varepsilon_t \sim \mathcal{N}(0,\sigma^2)$ then we have a [Gaussian white noise]. Note that [independence] implies [white noise] but the oposite is not correct unless we have a [Gaussian white noise]. 
	

#### worm plot

A worm plot,  @vanBuurenFredriks01, is a [diagnostics] tool which could be applied to any standardised [residuals] of a [regression] model but more often to [z-scores] of a [distributional regression] model. A [worm plot] is a trended [QQ-plot]. Detrended to highlight departures from normality. 

```{r}
resid_wp(m2)
```



################################################################################
################################################################################
## X

#### $X$

$X$ is used in here as a generic term for [explanatory variables], [input variables] or [independent variables] in a [regression analysis] but also is used to indicate a [random] variable.  


################################################################################
################################################################################
## Y

#### $Y$

$Y$ is used as a generic terms for the [response],  the [target], or the [y-variable] for [regression analysis] and [distributional regression].  More specifically within a [distributional regression] framework one would like to match the [range] of the [response] [vector] in the [data] with the [range] of the assumed distribution $D()$ for the response.  This will ensure that in any [prediction] the right boundaries are observed.  


#### y-variable

We refer to the [y-variable] as the variable of interest in a [regression analysis]. Other names are the  [response], the  [target] or the **depended variable**.

################################################################################
################################################################################
## Z

#### z-scores 
In a  [distributional regression] [model] the [z-scores] are the normalised [residuals] of the [model]. 
If the model is an [adequate fit], we expect 95% of the [z-scores] values  to fall between -2 and 2. (More precisely between -1.96 and 1.96). Therefore [z-scores] for both [training] and [test] data can be used for detection of unusual observations.  The straightforward calculation of the [z-scores]   is  one  of the great advantages of the [distributional regression] compare with [quantile regression], @koenker2017quantile, where the [z-scores] could be calculate  but with a substantial computational cost see for example, Chapter 13 of @Stasinopoulosetal2017.    

################################################################################
################################################################################
